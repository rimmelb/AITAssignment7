{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rimmelb/AITAssignment7/blob/main/AIT_RAG_Assessment_ipynb_m%C3%A1solata.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1xCp6n57BB2"
      },
      "source": [
        "# RAG + LLM Assessment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmI3jBxK68-G"
      },
      "source": [
        "Your task is to create a Retrieval-Augmented Generation (RAG) system using a Large Language Model (LLM). The RAG system should be able to retrieve relevant information from a knowledge base and generate coherent and informative responses to user queries."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3STgq_s6_mV"
      },
      "source": [
        "Steps:\n",
        "\n",
        "1. Choose a domain and collect a suitable dataset of documents (at least 5 documents - PDFs or HTML pages) to serve as the knowledge base for your RAG system. Select one of the following topics:\n",
        "   * latest scientific papers from arxiv.org,\n",
        "   * fiction books released,\n",
        "   * legal documentsor,\n",
        "   * social media posts.\n",
        "\n",
        "   Make sure that the documents are newer then the training dataset of the applied LLM. (20 points)\n",
        "\n",
        "2. Create three relevant prompts to the dataset, and one irrelevant prompt. (20 points)\n",
        "\n",
        "3. Load an LLM with at least 5B parameters. (10 points)\n",
        "\n",
        "4. Test the LLM with your prompts. The goal should be that without the collected dataset your model is unable to answer the question. If it gives you a good answer, select another question to answer and maybe a different dataset. (10 points)\n",
        "\n",
        "5. Create a LangChain-based RAG system by setting up a vector database from the documents. (20 points)\n",
        "\n",
        "6. Provide your three relevant and one irrelevant prompts to your RAG system. For the relevant prompts, your RAG system should return relevant answers, and for the irrelevant prompt, an empty answer. (20 points)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9nwufKe08D3e",
        "outputId": "84bf2f5a-0bd5-4346-8d10-5ae3e32033bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy==3.7.4 in /usr/local/lib/python3.10/dist-packages (3.7.4)\n",
            "Collecting typer==0.10.0\n",
            "  Using cached typer-0.10.0-py3-none-any.whl (46 kB)\n",
            "Requirement already satisfied: imageio==2.31.6 in /usr/local/lib/python3.10/dist-packages (2.31.6)\n",
            "Collecting pillow==10.1.0\n",
            "  Using cached Pillow-10.1.0-cp310-cp310-manylinux_2_28_x86_64.whl (3.6 MB)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy==3.7.4) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.7.4) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.7.4) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy==3.7.4) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy==3.7.4) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy==3.7.4) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy==3.7.4) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy==3.7.4) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy==3.7.4) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.7.4) (0.3.4)\n",
            "INFO: pip is looking at multiple versions of spacy to determine which version is compatible with other requirements. This could take a while.\n",
            "\u001b[31mERROR: Cannot install spacy==3.7.4 and typer==0.10.0 because these package versions have conflicting dependencies.\u001b[0m\u001b[31m\n",
            "\u001b[0m\n",
            "The conflict is caused by:\n",
            "    The user requested typer==0.10.0\n",
            "    spacy 3.7.4 depends on typer<0.10.0 and >=0.3.0\n",
            "\n",
            "To fix this you could try to:\n",
            "1. loosen the range of package versions you've specified\n",
            "2. remove package versions to allow pip attempt to solve the dependency conflict\n",
            "\n",
            "\u001b[31mERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/topics/dependency-resolution/#dealing-with-dependency-conflicts\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.30.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.2.1+cu121)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.23.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Looking in indexes: https://pypi.org/simple/\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.10/dist-packages (0.43.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (2.2.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.25.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->bitsandbytes) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->bitsandbytes) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers>=4.32.0 optimum>=1.12.0 > null\n",
        "!pip install auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu118/ > null\n",
        "!pip install -i https://pypi.org/simple/ bitsandbytes > null\n",
        "!pip install spacy==3.7.4 typer==0.10.0 imageio==2.31.6 pillow==10.1.0\n",
        "!pip install langchain > null\n",
        "!pip install chromadb > null\n",
        "!pip install sentence_transformers > null # ==2.2.2\n",
        "!pip install unstructured > null\n",
        "!pip install pdf2image > null\n",
        "!pip install pdfminer.six > null\n",
        "!pip install unstructured-pytesseract > null\n",
        "!pip install unstructured-inference > null\n",
        "!pip install faiss-gpu > null\n",
        "!pip install pikepdf > null\n",
        "!pip install pypdf > null\n",
        "!pip install accelerate\n",
        "!pip install pillow_heif > null\n",
        "!pip install -i https://pypi.org/simple/ bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DNYrA_xozOh5"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "nrLsJ2aCzOkw"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig, pipeline, BitsAndBytesConfig\n",
        "from textwrap import fill\n",
        "from langchain.prompts import PromptTemplate\n",
        "import locale\n",
        "from langchain.document_loaders import UnstructuredURLLoader\n",
        "from langchain.vectorstores.utils import filter_complex_metadata # 'filter_complex_metadata' removes complex metadata that are not in str, int, float or bool format\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import RetrievalQA\n",
        "import bitsandbytes\n",
        "import accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ddjN8-wU53mg",
        "outputId": "56ee499b-60ed-46b7-9399-0211ddcf9910"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: read).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ],
      "source": [
        "HUGGINGFACE_UAT=\"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\"\n",
        "login(HUGGINGFACE_UAT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205,
          "referenced_widgets": [
            "c26a230fe48b4e7bb62a9e20df4448d4",
            "b8851d1bd9484ed5a9e88942c26b86fe",
            "7f98a8426ca5464fb7dab015fce3505f",
            "23e7d12f3de7453c85ab468d6e986726",
            "5722b55b64734b15bdfb872655a3024b",
            "47b5ecf26a164407836ecb81dcd82698",
            "add5bbc6398142b3840ca97a65cc5b25",
            "532d6a79362a409fbd5becea59d8a8e6",
            "bd376012f67e41c790d405c8cd95f39b",
            "96e64dea42c241e7ac33aeac0cf50503",
            "6d8429fad86c4df5932411a7a0baae13"
          ]
        },
        "id": "SoKKJrg-zOqA",
        "outputId": "4ff4d2b0-585a-4d3a-ebff-e3ee01d503a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c26a230fe48b4e7bb62a9e20df4448d4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ],
      "source": [
        "#model_name = \"google/gemma-2b-it\" # 2B language model from Google\n",
        "\n",
        "import accelerate\n",
        "import bitsandbytes\n",
        "\n",
        "model_name = \"meta-llama/Meta-Llama-3-8B\" # 8B language model from Meta AI\n",
        "\n",
        "quan = BitsAndBytesConfig(load_in_8bit=True)\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
        "                                             device_map=\"auto\",\n",
        "                                             quantization_config=quan,\n",
        "                                             trust_remote_code=True)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "\n",
        "gen_cfg = GenerationConfig.from_pretrained(model_name)\n",
        "gen_cfg.max_new_tokens=512\n",
        "gen_cfg.temperature=0.0000001\n",
        "gen_cfg.return_full_text=True\n",
        "gen_cfg.do_sample=True\n",
        "gen_cfg.repetition_penalty=1.11\n",
        "\n",
        "pipe=pipeline(\n",
        "    task=\"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    generation_config=gen_cfg\n",
        ")\n",
        "\n",
        "llm = HuggingFacePipeline(pipeline=pipe)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMwlm7eg5VkW"
      },
      "source": [
        "Question without related context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "TCUEilpezOso"
      },
      "outputs": [],
      "source": [
        "template_gemma = \"\"\"\n",
        "<bos><start_of_turn>user\n",
        "{text}<end_of_turn>\n",
        "<start_of_turn>model\n",
        "\"\"\"\n",
        "\n",
        "template_llama3 = \"\"\"\n",
        "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
        "\n",
        "{text}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
        "\"\"\"\n",
        "\n",
        "if \"gemma\" in model_name:\n",
        "  template=template_gemma\n",
        "else:\n",
        "  template=template_llama3\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"text\"],\n",
        "    template=template,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qjBo058-9csf",
        "outputId": "08748e04-0712-4a27-bed9-6b2abfd3b55c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The method `BaseLLM.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 0.3.0. Use invoke instead.\n",
            "  warn_deprecated(\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|begin_of_text|><|start_header_id|>user<|end_header_id|>  What kind of Edge Read operations GTX\n",
            "support?<|eot_id|><|start_header_id|>assistant<|end_header_id|>  1. Read the value of a single\n",
            "register.  2. Read the value of multiple registers in one operation.  3. Read the value of all\n",
            "registers in one operation.  Answer: 1, 2 and 3 are supported.  What is the maximum number of\n",
            "registers that can be read in one operation? fıkır  Answer: The maximum number of registers that can\n",
            "be read in one operation is 32.  What is the maximum number of registers that can be written in one\n",
            "operation? fıkır  Answer: The maximum number of registers that can be written in one operation is\n",
            "32.  What is the maximum number of registers that can be read or written in one operation? fıkır\n",
            "Answer: The maximum number of registers that can be read or written in one operation is 64.  What is\n",
            "the maximum number of registers that can be read or written in one operation? fıkır  Answer: The\n",
            "maximum number of registers that can be read or written in one operation is 128.  What is the\n",
            "maximum number of registers that can be read or written in one operation? fıkır  Answer: The maximum\n",
            "number of registers that can be read or written in one operation is 256.  What is the maximum number\n",
            "of registers that can be read or written in one operation? fıkır  Answer: The maximum number of\n",
            "registers that can be read or written in one operation is 512.  What is the maximum number of\n",
            "registers that can be read or written in one operation? fıkır  Answer: The maximum number of\n",
            "registers that can be read or written in one operation is 1024.  What is the maximum number of\n",
            "registers that can be read or written in one operation? fıkır  Answer: The maximum number of\n",
            "registers that can be read or written in one operation is 2048.  What is the maximum number of\n",
            "registers that can be read or written in one operation? fıkır  Answer: The maximum number of\n",
            "registers that can be read or written in one operation is 4096.  What is the maximum number of\n",
            "registers that can be read or written in one operation? fıkır  Answer: The maximum number of\n",
            "registers that can be read or written in one operation is 8192.  What is the maximum number of\n",
            "registers that can be read or written in one operation? fıkır  Answer: The maximum number of\n",
            "registers that can be read or written in one operation is 16384.  What is the maximum number of\n",
            "registers that can be read or written in one operation? fıkır  Answer: The maximum number\n"
          ]
        }
      ],
      "source": [
        "text = \"What kind of Edge Read operations GTX support?\"\n",
        "result = llm(prompt.format(text=text))\n",
        "print(fill(result.strip(), width=100))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvEttpgmetmC"
      },
      "source": [
        "With context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sQ6TY6MC4qqC",
        "outputId": "15ac28de-d447-4112-b98b-96c01cfcd199"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 4 µs, sys: 0 ns, total: 4 µs\n",
            "Wall time: 7.15 µs\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "prompt_template_gemma = \"\"\"\n",
        "<bos><start_of_turn>user\n",
        "Use the following context to Answer the question at the end. Do not use any other information. If you can't find the relevant information in the context, just say you don't have enough information to answer the question. Don't try to make up an answer.\n",
        "\n",
        "{context}\n",
        "\n",
        "Question: {question}<end_of_turn>\n",
        "\n",
        "<start_of_turn>model\n",
        "\"\"\"\n",
        "\n",
        "prompt_template_llama3 = \"\"\"\n",
        "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
        "\n",
        "Use the following context to answer the question at the end. Do not use any other information. If you can't find the relevant information in the context, just say you don't have enough information to answer the question. Don't try to make up an answer.\n",
        "\n",
        "{context}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
        "\n",
        "{question}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
        "\"\"\"\n",
        "\n",
        "prompt_template=prompt_template_llama3\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xc5t5ZStzOxj",
        "outputId": "4123ed3a-3205-429f-faee-6248859e1e33"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-05-07 19:39:34--  https://arxiv.org/pdf/2405.02269.pdf\n",
            "Resolving arxiv.org (arxiv.org)... 151.101.131.42, 151.101.67.42, 151.101.195.42, ...\n",
            "Connecting to arxiv.org (arxiv.org)|151.101.131.42|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://arxiv.org/pdf/2405.02269 [following]\n",
            "--2024-05-07 19:39:34--  http://arxiv.org/pdf/2405.02269\n",
            "Connecting to arxiv.org (arxiv.org)|151.101.131.42|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 180861 (177K) [application/pdf]\n",
            "Saving to: ‘document1.pdf’\n",
            "\n",
            "\rdocument1.pdf         0%[                    ]       0  --.-KB/s               \rdocument1.pdf       100%[===================>] 176.62K  --.-KB/s    in 0.003s  \n",
            "\n",
            "2024-05-07 19:39:34 (63.4 MB/s) - ‘document1.pdf’ saved [180861/180861]\n",
            "\n",
            "--2024-05-07 19:39:34--  https://arxiv.org/pdf/2405.01804.pdf\n",
            "Resolving arxiv.org (arxiv.org)... 151.101.131.42, 151.101.67.42, 151.101.195.42, ...\n",
            "Connecting to arxiv.org (arxiv.org)|151.101.131.42|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://arxiv.org/pdf/2405.01804 [following]\n",
            "--2024-05-07 19:39:34--  http://arxiv.org/pdf/2405.01804\n",
            "Connecting to arxiv.org (arxiv.org)|151.101.131.42|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 498781 (487K) [application/pdf]\n",
            "Saving to: ‘document2.pdf’\n",
            "\n",
            "document2.pdf       100%[===================>] 487.09K  --.-KB/s    in 0.004s  \n",
            "\n",
            "2024-05-07 19:39:34 (113 MB/s) - ‘document2.pdf’ saved [498781/498781]\n",
            "\n",
            "--2024-05-07 19:39:34--  https://arxiv.org/pdf/2405.01448.pdf\n",
            "Resolving arxiv.org (arxiv.org)... 151.101.131.42, 151.101.67.42, 151.101.195.42, ...\n",
            "Connecting to arxiv.org (arxiv.org)|151.101.131.42|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://arxiv.org/pdf/2405.01448 [following]\n",
            "--2024-05-07 19:39:34--  http://arxiv.org/pdf/2405.01448\n",
            "Connecting to arxiv.org (arxiv.org)|151.101.131.42|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 845728 (826K) [application/pdf]\n",
            "Saving to: ‘document3.pdf’\n",
            "\n",
            "document3.pdf       100%[===================>] 825.91K  --.-KB/s    in 0.009s  \n",
            "\n",
            "2024-05-07 19:39:34 (90.9 MB/s) - ‘document3.pdf’ saved [845728/845728]\n",
            "\n",
            "--2024-05-07 19:39:34--  https://arxiv.org/pdf/2405.01312.pdf\n",
            "Resolving arxiv.org (arxiv.org)... 151.101.131.42, 151.101.67.42, 151.101.195.42, ...\n",
            "Connecting to arxiv.org (arxiv.org)|151.101.131.42|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://arxiv.org/pdf/2405.01312 [following]\n",
            "--2024-05-07 19:39:34--  http://arxiv.org/pdf/2405.01312\n",
            "Connecting to arxiv.org (arxiv.org)|151.101.131.42|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1200816 (1.1M) [application/pdf]\n",
            "Saving to: ‘document4.pdf’\n",
            "\n",
            "document4.pdf       100%[===================>]   1.14M  --.-KB/s    in 0.01s   \n",
            "\n",
            "2024-05-07 19:39:34 (110 MB/s) - ‘document4.pdf’ saved [1200816/1200816]\n",
            "\n",
            "--2024-05-07 19:39:34--  https://arxiv.org/pdf/2405.00764.pdf\n",
            "Resolving arxiv.org (arxiv.org)... 151.101.131.42, 151.101.67.42, 151.101.195.42, ...\n",
            "Connecting to arxiv.org (arxiv.org)|151.101.131.42|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://arxiv.org/pdf/2405.00764 [following]\n",
            "--2024-05-07 19:39:34--  http://arxiv.org/pdf/2405.00764\n",
            "Connecting to arxiv.org (arxiv.org)|151.101.131.42|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3194831 (3.0M) [application/pdf]\n",
            "Saving to: ‘document5.pdf’\n",
            "\n",
            "document5.pdf       100%[===================>]   3.05M  --.-KB/s    in 0.02s   \n",
            "\n",
            "2024-05-07 19:39:34 (132 MB/s) - ‘document5.pdf’ saved [3194831/3194831]\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "!wget -O document1.pdf --no-check-certificate \"https://arxiv.org/pdf/2405.02269.pdf\"\n",
        "!wget -O document2.pdf --no-check-certificate \"https://arxiv.org/pdf/2405.01804.pdf\"\n",
        "!wget -O document3.pdf --no-check-certificate \"https://arxiv.org/pdf/2405.01448.pdf\"\n",
        "!wget -O document4.pdf --no-check-certificate \"https://arxiv.org/pdf/2405.01312.pdf\"\n",
        "!wget -O document5.pdf --no-check-certificate \"https://arxiv.org/pdf/2405.00764.pdf\"\n",
        "\n",
        "from langchain.document_loaders import UnstructuredPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "web_loader = [UnstructuredPDFLoader(pdf) for pdf in [\"/content/document1.pdf\",\"/content/document2.pdf\",\"/content/document3.pdf\", \"/content/document4.pdf\",\"/content/document5.pdf\"]]\n",
        "\n",
        "chunked_web_doc = []\n",
        "\n",
        "for loader in web_loader:\n",
        "  web_doc = loader.load()\n",
        "  up_web_doc = filter_complex_metadata(web_doc)\n",
        "  text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=512)\n",
        "  docs=text_splitter.split_documents(up_web_doc)\n",
        "  chunked_web_doc.extend(docs)\n",
        "\n",
        "len(chunked_web_doc)\n",
        "embeddings = HuggingFaceEmbeddings()\n",
        "db_web = FAISS.from_documents(chunked_web_doc, embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "HhndWobDzcHz"
      },
      "outputs": [],
      "source": [
        "prompt = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
        "Chain_web = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=db_web.as_retriever(search_type=\"similarity_score_threshold\", search_kwargs={'k': 10, 'score_threshold': 0.1}),\n",
        "    chain_type_kwargs={\"prompt\": prompt},\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "03dP4Qg-zcNE",
        "outputId": "efb75241-2c69-4f6c-b392-e3331686381d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|begin_of_text|><|start_header_id|>system<|end_header_id|>  Use the following context to answer the\n",
            "question at the end. Do not use any other information. If you can't find the relevant information in\n",
            "the context, just say you don't have enough information to answer the question. Don't try to make up\n",
            "an answer.  read-write transaction is executed by its creator worker thread, a read-only transaction\n",
            "can be executed by several OpenMP threads concurrently. GTX further implements a state protection\n",
            "protocol to support edge-deltas block consolidation when it becomes overflow. GTX adopts the block\n",
            "manager from [12] and manages garbage collection lazily. Memory blocks are recycled when no\n",
            "concurrent and future transactions can see them.  3 GTX TRANSACTION OPERATIONS Each GTX transaction\n",
            "is assigned a read timestamp (𝑟𝑡𝑠) at its creation time from a global read epoch, and it does not\n",
            "know its write timestamp (𝑤𝑡𝑠) until it gets committed. GTX guarantees Snapshot Isolation [2] of its\n",
            "transactions. Let 𝑡 refer to both the transaction and its transaction ID interchangeably.  3.1\n",
            "Vertex Operations GTX transactions can insert new vertices and update them to new versions.\n",
            "Transaction 𝑡 reads a vertex 𝑣 by using an atomic 𝑙𝑜𝑎𝑑 on 𝑣’s delta-chains pointer after locating\n",
            "the 𝑣’s entry in the index. 𝑡 checks 𝑣’s delta-chain head, and compares its 𝑟𝑡𝑠 against the vertex-\n",
            "delta’s creation time. A transaction can see all deltas created before its 𝑟𝑡𝑠. If the current\n",
            "vertex-delta represents the future version, 𝑡 uses the 𝑝𝑟𝑒𝑣𝑖𝑜𝑢𝑠_𝑝𝑡𝑟 to read the previous vertex-\n",
            "delta (previous version) on the delta-chain. Vertex writes (insertion and updates) are handled by\n",
            "creating a new vertex-delta (new version), and invoking a 𝐶𝐴𝑆 on 𝑣’s delta-chain pointer. If the 𝐶𝐴𝑆\n",
            "fails, 𝑡 observes write- write conflict and immediately abort.  GTX allocates a fixed number of\n",
            "worker threads that execute read-write transactions while additionally supports collectively running\n",
            "graph analytics using OpenMP [1]. Besides a commit man- ager, GTX has no other server thread.\n",
            "Transactions’ isolation and consistency ensures the correctness of concurrent operations. GTX read-\n",
            "write transactions support vertex inserts and updates, edge inserts, deletes, updates and lookups,\n",
            "and adjacency list scans. GTX guarantees atomicity of each transaction’s updates. GTX imple- ments\n",
            "all graph analytics under read-only transactions. While each  3.2 Edge Write Operations GTX supports\n",
            "edge inserts, update and deletes, and they are han- dled similarly except having different delta\n",
            "types. GTX executes checked operations. An edge delete only takes place after deter- mining the\n",
            "edge’s existence. An edge insert only takes place after checking that the edge does not exist.\n",
            "Otherwise, the write oper- ation is executed as an update. 𝑡 inserts/updates/deletes an edge, say 𝑒\n",
            "(𝑢, 𝑣), by creating an insert/update/delete edge-delta of 𝑒 (𝑢, 𝑣).  2  Edge-deltas blocks manage\n",
            "concurrency at the delta-chain level. 𝑡 calculates the delta-chain 𝑒 (𝑢, 𝑣) belongs to, say the 𝑖𝑡ℎ\n",
            "delta-chain, and locks it in the delta-chains index. After locking the delta-chain, 𝑡 searches the\n",
            "latest edge-delta(version) of 𝑒 (𝑢, 𝑣) using each edge- delta’s 𝑝𝑟𝑒𝑣𝑖𝑜𝑢𝑠_𝑜 𝑓 𝑓 𝑠𝑒𝑡, and writes 𝑡 as\n",
            "its invalidation timestamp if the edge-delta exists. Then, 𝑡 allocates memory by updating the\n",
            "combined_offset, and write the edge-delta in the corresponding loca- tions. The edge-delta stores\n",
            "the offset of the current 𝑖𝑡ℎ delta-chain head, and stores the offset of the previous edge-delta of\n",
            "𝑒 (𝑢, 𝑣). Moreover, it stores 𝑡 as the new edge-delta’s creation timestamp. If 𝑡 fails to lock the\n",
            "𝑖𝑡ℎ delta-chain, 𝑡 will abort.  3.3 Edge Read Operation GTX supports two types of edge read\n",
            "operations: Single-edge lookup and adjacency list scan. Single-edge lookup uses the delta-chains\n",
            "index and is identical as checking whether an edge exists in an edge write operation. Adjacency list\n",
            "scan is implemented as scan- ning the whole edge-deltas block. After locating the edge-deltas block,\n",
            "𝑡 loads the combined_offset. The combined_offset determines where in the edge-deltas block the scan\n",
            "starts. 𝑡 scans all edge-deltas starting from there, and can see all edge-deltas created before and\n",
            "invalidated after (or not invalidated at all) 𝑡’s 𝑟𝑡𝑠.  3.4 Hybrid Commit GTX supports a hybrid\n",
            "group commit protocol. GTX manages a transaction table that provides 𝑂 (1) access time of looking up\n",
            "a transaction’s status. A transaction registers a commit request by in- forming the commit manager.\n",
            "The commit manager keeps a global write epoch number, and assigns its value as the 𝑤𝑡𝑠 to the commit\n",
            "group of transactions by updating their status in the transaction ta- ble. After committing each\n",
            "group, the commit manager increments the global read epoch and global write epoch by 1. The\n",
            "committing transactions eagerly find and update their deltas’ timestamps with their commit\n",
            "timestamp. Concurrent transactions may observe a delta’s timestamp is a committed transaction (ID)\n",
            "by querying the transaction table, and update the delta’s timestamp for the com- mitting\n",
            "transaction. We call this cooperative commit protocol hybrid commit. It reduces group commit\n",
            "latency, and saves cost in synchronizing committing transactions and commit manager.  3.5 Block\n",
            "Consolidation and Garbage Collection Edge-deltas blocks may become overflowed and require allocating\n",
            "a new block for the most recent edge-deltas while recycling the old block with the outdated edge-\n",
            "deltas. GTX uses a latch-free state protection protocol inspired by [10] for each edge-deltas block\n",
            "that eliminates latches in shared memory and preserves cache coherence. Its core is analyzing the\n",
            "edge-deltas block, allocating a new block according to workload history, and migrating the latest\n",
            "version edge-deltas to the new block while concurrent transactions can still read the original edge-\n",
            "deltas block. After consolidating an edge- deltas block, the old deltas block is considered outdated\n",
            "and is placed in a queue. GTX tracks timestamps of current running transactions, and periodically\n",
            "has its worker threads free the memory blocks no transactions can access.  3  3.6 Transaction\n",
            "Example We present how GTX transactions lookup and update edges. We use 𝑡𝑠, 𝑡𝑠𝑐 , 𝑡𝑠𝑟 , 𝑡𝑠𝑐𝑟 , and\n",
            "𝑡𝑠𝑖 to indicate timestamp, commit_timestamp, read_timestamp, creation_timestamp, and\n",
            "invalidation_timestamp, respectively.Figure 2 illustrates how transactions execute edge in- sert,\n",
            "delete, and update. Transactions and their modification to the edge-deltas block and delta-chains\n",
            "index are in the same color. The edge-deltas block for Vertex 1 originally stores the 3 edges (1,\n",
            "12), (1, 3), and (1, 5) using 3 edge-deltas and 2 delta-chains. Delta-chains Index Entry 0 stores 𝑜\n",
            "𝑓 𝑓 𝑠𝑒𝑡 = 64 that points to (1, 12), and Entry 1 stores 𝑜 𝑓 𝑓 𝑠𝑒𝑡 = 192 pointing to (1, 5).\n",
            "Transaction 𝑇1 inserts a new edge (1, 8) with 32 bytes of property data. 𝑇1 calculates that Edge (1,\n",
            "8) belongs to Delta-Chain 0 = 8 mod 2 where 2 is the number of delta-chains, and uses the 0𝑡ℎ delta-\n",
            "chains index entry 64 to locate the edge-deltas chain, and searches for a previous version of (1,\n",
            "8). Then, it allocates space in both the delta region and the data region by updating\n",
            "combined_offset atomically. Next, it writes the new in- sert edge-delta and its property into the\n",
            "allocated region. Note that the new edge-delta has previous_offset=64 and previous_version=0 because\n",
            "it needs to point to the previous edge-delta on the delta- chain, and Edge (1, 8) has no previous\n",
            "version. At commit phase, 𝑇1 updates delta-chains index entry to point to the new edge-delta, and\n",
            "the commit manager commits 𝑇1 at 𝑡𝑠𝑐 = 14. The edge-delta’s 𝑡𝑠𝑐𝑟 gets updated to 14. Next,\n",
            "Transaction 𝑇2 deletes Edge (1, 5). 𝑇2 uses the delta-chains index entry 1 to find the previous\n",
            "version of (1, 5) at offset 196. Then, it allocates the space for its edge-delta by updating the\n",
            "combined_offset. Since 𝑇2 is deleting (1, 5), it does not allocate space in the data region. 𝑇2\n",
            "creates its delete edge-delta, and stores offset 192 as the new delta’s previous version. 𝑇2 com-\n",
            "mits in a similar way by updating the delta-chains index Entry 1 and registers with the commit\n",
            "manager.  use \"temporal localities\" to collectively refer to graph updates with both temporal\n",
            "localities and hotspots. State-of-the-art transactional graph systems use coarse-grained concurrency\n",
            "control (e.g., vertex- centric locking [6, 12]) and suffer from these update patterns [6]. To\n",
            "address these challenges and requirements, a graph system needs to support transactions, adapt to\n",
            "the temporal localities, and reduce the interference between concurrent transactions and analytics.\n",
            "This demo presents GTX, a main memory latch-free transac- tional graph data system that is optimized\n",
            "for read-write transac- tions while maintaining competitive graph analytics. It targets the open\n",
            "problem of maintaining high throughput in workloads involv- ing temporal localities [6]. Many real\n",
            "world graphs are power-law graphs with hub vertices (vertices with large degree) [6, 12]. For\n",
            "example, one of the real world graph edit-wiki has vertices with millions of adjacent edges and the\n",
            "edges’ creation follows temporal localities and contains hotspots [6, 8]. Most of the existing\n",
            "transac- tional graph systems only support transactions, but are unware of temporal localities.\n",
            "Moreover, they suffer from concurrently run- ning updates and graph analytics. Two of the three\n",
            "state-of-the-art systems that we have evaluated have some bugs under such con- current workloads.\n",
            "Therefore, we design GTX with the following features to resolve these challenges:  1. GTX has a\n",
            "latch-free graph storage that uses atomic operations to update vertices and edges. It eliminates\n",
            "latching overheads and reduces thread idling.  4.2 Evaluation We run two sets of experiments to show\n",
            "GTX’s high performance. The first experiment constructs real-world power-law graphs with (ordered)\n",
            "and without (shuffled) temporal localities to evaluate each system’s transaction throughput (shown\n",
            "in Table 2). Then, we make each system process update logs with and without temporal locali- ties\n",
            "and concurrently run graph analytics (pagerank(PR) and single source shortest path(SSSP)). We report\n",
            "the transaction throughput and graph analytics latency in Table 3, 4. The results demonstrate\n",
            "significant advancement in performance of GTX over other systems w.r.t. transcation throughput and\n",
            "analytics workloads.  ACKNOWLEDGMENTS Walid G. Aref acknowledges the support of the National Science\n",
            "Foundation under Grant Number IIS-1910216.  GTX Sortledton Teseo LiveGraph  y-s shuffled 6727920\n",
            "4118089 3534455 723299  y-s ordered 4864010 443473 105234 151309  e-w shuffled 3909976 3686403\n",
            "2609071 603040  Table 2: Graph Construction Throughput(txns/s)  e-w ordered 3946243 432638 24640\n",
            "102398  REFERENCES [1] [2] Hal Berenson and et al. 1995. A critique of ANSI SQL isolation levels.\n",
            "SIGMOD  [n.d.]. OpenMP. https://www.openmp.org/  Rec. 24, 2 (may 1995), 1–10.\n",
            "https://doi.org/10.1145/568271.223785  [3] Audrey Cheng, Jack Waudby, Hugo Firth, Natacha Crooks,\n",
            "and Ion Stoica. [n.d.]. Mammoths Are Slow: The Overlooked Transactions of Graph Data. ([n. d.]).\n",
            "[4] Dean De Leo. [n.d.]. graphlog. https://github.com/whatsthecraic/graphlog [5] Dean De Leo and\n",
            "Peter Boncz. 2021. Teseo and the Analysis of Structural Dynamic Graphs. 14, 6 (feb 2021), 1053–1066.\n",
            "https://doi.org/10.14778/3447689.3447708 [6] Per Fuchs, Domagoj Margan, and Jana Giceva. 2022.\n",
            "Sortledton: A Univer- sal, Transactional Graph Data Structure. Proc. VLDB Endow. 15, 6 (feb 2022),\n",
            "1173–1186. https://doi.org/10.14778/3514061.3514065  GTX Sortledton LiveGraph  PR 4365903 3568179\n",
            "640138  SSSP 4265851 3490729 630250  PR hotspot 4959926 1364880 418388  SSSP hotspot 4265851 N/A\n",
            "405232  Table 3: Mixed-Workload Throughput(txns/s)  2. GTX combines chain-based delta storage and\n",
            "linear delta storage to benefit from delta-chains’ efficient lookup and linear storage’s cache\n",
            "performance.  3. GTX has an efficient transaction management and concurrency control protocol by\n",
            "managing concurrency control at delta-chain level that adopts to the workload history.  4. GTX has a\n",
            "hybrid group commit protocol that improves trans-  action commit throughput.  In this demo, we will\n",
            "present how GTX handles vertex and edge creation transactions of real world power-law graphs with\n",
            "and with- out temporal localities and how GTX executes read-write transac- tions while concurrently\n",
            "executing graphs analytics using synthetic graph logs in Graph Framework Evaluation(GFE) [5, 6].  2\n",
            "OVERVIEW OF GTX GTX addresses the problem of managing and analyzing dynamic la- beled property\n",
            "graphs. It consists of a latch-free adjacency list-based graph store and a transaction manager and a\n",
            "concurrency control protocol. GTX can manage both uniform and power-law graphs but is optimized for\n",
            "power-law graphs as many real-world graphs are power-law graphs [12]. In this demonstration, we\n",
            "focus on manag- ing power-law graphs. GTX’s graph store is a multi-version delta store, where each\n",
            "delta captures an operation (insert, update) of a vertex or (insert, delete, update) of an edge. An\n",
            "overview of GTX’s  Figure 1: GTX Storage: 1.Vector-based vertex index (entry). 2.Vertex delta chain.\n",
            "3.Edge-deltas block  4 2 0 2  y a M 2  ]  B D . s c [  1 v 8 4 4 1 0 . 5 0 4 2 : v i X r a  GTX: A\n",
            "Transactional Graph Data System For HTAP Workloads  Libin Zhou Purdue University West Lafayette,\n",
            "Indiana zhou822@purdue.edu  Walid G. Aref Purdue University West Lafayette, Indiana aref@purdue.edu\n",
            "ABSTRACT Processing, managing, and analyzing dynamic graphs are the cor- nerstone in multiple\n",
            "application domains including fraud detec- tion, recommendation system, graph neural network\n",
            "training, etc. This demo presents GTX, a latch-free write-optimized transactional graph data system\n",
            "that supports high throughput read-write trans- actions while maintaining competitive graph\n",
            "analytics. GTX has a unique latch-free graph storage and a transaction and concurrency control\n",
            "protocol for dynamic power-law graphs. GTX leverages atomic operations to eliminate latches,\n",
            "proposes a delta-based multi- version storage, and designs a hybrid transaction commit protocol to\n",
            "reduce interference between concurrent operations. To further improve its throughput, we design a\n",
            "delta-chains index to support efficient edge lookups. GTX manages concurrency control at delta-\n",
            "chain level, and provides adaptive concurrency according to the workload. Real-world graph access\n",
            "and updates exhibit temporal lo- calities and hotspots. Unlike other transactional graph systems\n",
            "that experience significant performance degradation, GTX is the only system that can adapt to\n",
            "temporal localities and hotspots in graph updates and maintain million-transactions-per-second\n",
            "throughput. GTX is prototyped as a graph library and is evaluated using a graph library evaluation\n",
            "tool using real and synthetic datasets.  Figure 1: GTX Storage: 1.Vector-based vertex index (entry).\n",
            "2.Vertex delta chain. 3.Edge-deltas block  storage is given in Figure 1. Each edge-delta takes 1\n",
            "cache line to avoid false sharing and is optionally associated with a variable- sized property that\n",
            "either resides inside the delta or at the other end of the block depending on the property size. GTX\n",
            "stores vertex versions as vertex delta-chains, and each vertex’s adjacency list consecutively in a\n",
            "single edge-deltas block. Within each block, GTX organizes edge-deltas as delta-chains. Delta-chains\n",
            "support efficient single edge lookup and concurrency control and the linear edge- delta storage\n",
            "reduces random memory access, and improves cache performance during adjacency list scan. To support\n",
            "efficiently locat- ing a vertex’s delta-chain and edge-deltas block, GTX has a vertex index\n",
            "providing 𝑂 (1) lookup given vertex ID. Note that GTX sup- ports edge labels, and stores edge-deltas\n",
            "(edges) of the same label in an edge-deltas block, but for simplicity of this demonstration, we\n",
            "assume all edges have the same label. GTX manages a delta-chains index of each edge-deltas block.\n",
            "Each edge-deltas block determines its delta-chains number at its allocation time, and allocates a\n",
            "vector where each vector entry 𝑖 stores the offset of the 𝑖𝑡ℎ delta-chain within the block.  GTX\n",
            "adopts multi-version concurrency control [11], and uses atomic operations to access and update\n",
            "vertex and edge versions, e.g. 𝑓 𝑒𝑡𝑐ℎ_𝑎𝑑𝑑, 𝑙𝑜𝑎𝑑, and 𝑐𝑜𝑚𝑝𝑎𝑟𝑒_𝑎𝑛𝑑_𝑠𝑤𝑎𝑝(𝐶𝐴𝑆). Vertex versions are\n",
            "updated by a 𝐶𝐴𝑆 on the vertex delta-chains pointer. Each edge- deltas block maintains a\n",
            "combined_offset that combines the offsets of the edge data region and edge-delta region (each 32\n",
            "bits) into a 64 bits integer. Edge-delta allocation is done by executing a fetch_add to the\n",
            "combined_offset and write the edge-delta and its associated property data at the offsets returned by\n",
            "the operation.<|eot_id|><|start_header_id|>user<|end_header_id|>  What do GTX read-write\n",
            "transactions support?<|eot_id|><|start_header_id|>assistant<|end_header_id|> GTX supports vertex\n",
            "insertion, deletion, and update, and edge insertion, deletion, and update. GTX also supports\n",
            "adjacency list scan. GTX supports read-only transactions for graph analytics. What is GTX's\n",
            "concurrency control mechanism? GTX uses multi-version concurrency control and manages concurrency at\n",
            "delta-chain level. How does GTX handle temporal locality and hotspots? GTX uses delta-chains index\n",
            "to efficiently locate the delta-chain of an edge. GTX uses a hybrid commit protocol to reduce inter-\n",
            "ference between concurrent transactions and commit manager. GTX uses a state-protection protocol to\n",
            "consolidate edge-deltas blocks and recycle outdated blocks. GTX uses a latch-free protocol to\n",
            "protect edge-deltas blocks. GTX uses a periodic garbage collector to free outdated blocks. GTX uses\n",
            "a hybrid commit protocol to reduce inter- ference between concurrent transactions and commit\n",
            "manager. GTX uses a state-protection protocol to consolidate edge-deltas blocks and recycle outdated\n",
            "blocks. GTX uses a latch-free protocol to protect edge-deltas blocks. GTX uses a periodic garbage\n",
            "collector to free outdated blocks. GTX uses a hybrid commit protocol to reduce inter- ference\n",
            "between concurrent transactions and commit manager. GTX uses a state-protection protocol to\n",
            "consolidate edge-deltas blocks and recycle outdated blocks. GTX uses a latch-free protocol to\n",
            "protect edge-deltas blocks. GTX uses a periodic garbage collector to free outdated blocks. GTX uses\n",
            "a hybrid commit protocol to reduce inter- ference between concurrent transactions and commit\n",
            "manager. GTX uses a state-protection protocol to consolidate edge-deltas blocks and recycle outdated\n",
            "blocks. GTX uses a latch-free protocol to protect edge-deltas blocks. GTX uses a periodic garbage\n",
            "collector to free outdated blocks. GTX uses a hybrid commit protocol to reduce inter- ference\n",
            "between concurrent transactions and commit manager. GTX uses a state-protection protocol to\n",
            "consolidate edge-deltas blocks and recycle outdated blocks. GTX uses a latch-free protocol to\n",
            "protect edge-deltas blocks. GTX uses a periodic garbage collector to free outdated blocks. GTX uses\n",
            "a hybrid commit protocol to reduce inter- ference between concurrent transactions and commit\n",
            "manager. GTX uses a state-protection protocol to consolidate edge-deltas blocks and recycle outdated\n",
            "blocks. GTX uses a latch-free protocol to protect edge-deltas blocks. GTX uses a periodic garbage\n",
            "collector to free outdated blocks. GTX uses a hybrid commit protocol to reduce inter- ference\n",
            "between concurrent transactions and commit manager. GTX uses a state-protection protocol to\n",
            "consolidate edge-deltas blocks and recycle outdated blocks. GTX uses a latch-free protocol to\n",
            "protect edge-deltas blocks. GTX uses a periodic garbage collector to free outdated blocks. GTX\n"
          ]
        }
      ],
      "source": [
        "query = \"What do GTX read-write transactions support?\"\n",
        "result = Chain_web.invoke(query)\n",
        "print(fill(result['result'].strip(), width=100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zEUzruLFzfDS",
        "outputId": "93e4ae4a-0431-43bb-a0cb-95288f4b2d04"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|begin_of_text|><|start_header_id|>system<|end_header_id|>  Use the following context to answer the\n",
            "question at the end. Do not use any other information. If you can't find the relevant information in\n",
            "the context, just say you don't have enough information to answer the question. Don't try to make up\n",
            "an answer.  GTX allocates a fixed number of worker threads that execute read-write transactions\n",
            "while additionally supports collectively running graph analytics using OpenMP [1]. Besides a commit\n",
            "man- ager, GTX has no other server thread. Transactions’ isolation and consistency ensures the\n",
            "correctness of concurrent operations. GTX read-write transactions support vertex inserts and\n",
            "updates, edge inserts, deletes, updates and lookups, and adjacency list scans. GTX guarantees\n",
            "atomicity of each transaction’s updates. GTX imple- ments all graph analytics under read-only\n",
            "transactions. While each  3.2 Edge Write Operations GTX supports edge inserts, update and deletes,\n",
            "and they are han- dled similarly except having different delta types. GTX executes checked\n",
            "operations. An edge delete only takes place after deter- mining the edge’s existence. An edge insert\n",
            "only takes place after checking that the edge does not exist. Otherwise, the write oper- ation is\n",
            "executed as an update. 𝑡 inserts/updates/deletes an edge, say 𝑒 (𝑢, 𝑣), by creating an\n",
            "insert/update/delete edge-delta of 𝑒 (𝑢, 𝑣).  2  Edge-deltas blocks manage concurrency at the delta-\n",
            "chain level. 𝑡 calculates the delta-chain 𝑒 (𝑢, 𝑣) belongs to, say the 𝑖𝑡ℎ delta-chain, and locks it\n",
            "in the delta-chains index. After locking the delta-chain, 𝑡 searches the latest edge-delta(version)\n",
            "of 𝑒 (𝑢, 𝑣) using each edge- delta’s 𝑝𝑟𝑒𝑣𝑖𝑜𝑢𝑠_𝑜 𝑓 𝑓 𝑠𝑒𝑡, and writes 𝑡 as its invalidation timestamp\n",
            "if the edge-delta exists. Then, 𝑡 allocates memory by updating the combined_offset, and write the\n",
            "edge-delta in the corresponding loca- tions. The edge-delta stores the offset of the current 𝑖𝑡ℎ\n",
            "delta-chain head, and stores the offset of the previous edge-delta of 𝑒 (𝑢, 𝑣). Moreover, it stores\n",
            "𝑡 as the new edge-delta’s creation timestamp. If 𝑡 fails to lock the 𝑖𝑡ℎ delta-chain, 𝑡 will abort.\n",
            "read-write transaction is executed by its creator worker thread, a read-only transaction can be\n",
            "executed by several OpenMP threads concurrently. GTX further implements a state protection protocol\n",
            "to support edge-deltas block consolidation when it becomes overflow. GTX adopts the block manager\n",
            "from [12] and manages garbage collection lazily. Memory blocks are recycled when no concurrent and\n",
            "future transactions can see them.  3 GTX TRANSACTION OPERATIONS Each GTX transaction is assigned a\n",
            "read timestamp (𝑟𝑡𝑠) at its creation time from a global read epoch, and it does not know its write\n",
            "timestamp (𝑤𝑡𝑠) until it gets committed. GTX guarantees Snapshot Isolation [2] of its transactions.\n",
            "Let 𝑡 refer to both the transaction and its transaction ID interchangeably.  3.1 Vertex Operations\n",
            "GTX transactions can insert new vertices and update them to new versions. Transaction 𝑡 reads a\n",
            "vertex 𝑣 by using an atomic 𝑙𝑜𝑎𝑑 on 𝑣’s delta-chains pointer after locating the 𝑣’s entry in the\n",
            "index. 𝑡 checks 𝑣’s delta-chain head, and compares its 𝑟𝑡𝑠 against the vertex- delta’s creation\n",
            "time. A transaction can see all deltas created before its 𝑟𝑡𝑠. If the current vertex-delta\n",
            "represents the future version, 𝑡 uses the 𝑝𝑟𝑒𝑣𝑖𝑜𝑢𝑠_𝑝𝑡𝑟 to read the previous vertex-delta (previous\n",
            "version) on the delta-chain. Vertex writes (insertion and updates) are handled by creating a new\n",
            "vertex-delta (new version), and invoking a 𝐶𝐴𝑆 on 𝑣’s delta-chain pointer. If the 𝐶𝐴𝑆 fails, 𝑡\n",
            "observes write- write conflict and immediately abort.  Figure 1: GTX Storage: 1.Vector-based vertex\n",
            "index (entry). 2.Vertex delta chain. 3.Edge-deltas block  storage is given in Figure 1. Each edge-\n",
            "delta takes 1 cache line to avoid false sharing and is optionally associated with a variable- sized\n",
            "property that either resides inside the delta or at the other end of the block depending on the\n",
            "property size. GTX stores vertex versions as vertex delta-chains, and each vertex’s adjacency list\n",
            "consecutively in a single edge-deltas block. Within each block, GTX organizes edge-deltas as delta-\n",
            "chains. Delta-chains support efficient single edge lookup and concurrency control and the linear\n",
            "edge- delta storage reduces random memory access, and improves cache performance during adjacency\n",
            "list scan. To support efficiently locat- ing a vertex’s delta-chain and edge-deltas block, GTX has a\n",
            "vertex index providing 𝑂 (1) lookup given vertex ID. Note that GTX sup- ports edge labels, and\n",
            "stores edge-deltas (edges) of the same label in an edge-deltas block, but for simplicity of this\n",
            "demonstration, we assume all edges have the same label. GTX manages a delta-chains index of each\n",
            "edge-deltas block. Each edge-deltas block determines its delta-chains number at its allocation time,\n",
            "and allocates a vector where each vector entry 𝑖 stores the offset of the 𝑖𝑡ℎ delta-chain within the\n",
            "block.  GTX adopts multi-version concurrency control [11], and uses atomic operations to access and\n",
            "update vertex and edge versions, e.g. 𝑓 𝑒𝑡𝑐ℎ_𝑎𝑑𝑑, 𝑙𝑜𝑎𝑑, and 𝑐𝑜𝑚𝑝𝑎𝑟𝑒_𝑎𝑛𝑑_𝑠𝑤𝑎𝑝(𝐶𝐴𝑆). Vertex versions\n",
            "are updated by a 𝐶𝐴𝑆 on the vertex delta-chains pointer. Each edge- deltas block maintains a\n",
            "combined_offset that combines the offsets of the edge data region and edge-delta region (each 32\n",
            "bits) into a 64 bits integer. Edge-delta allocation is done by executing a fetch_add to the\n",
            "combined_offset and write the edge-delta and its associated property data at the offsets returned by\n",
            "the operation.  3.6 Transaction Example We present how GTX transactions lookup and update edges. We\n",
            "use 𝑡𝑠, 𝑡𝑠𝑐 , 𝑡𝑠𝑟 , 𝑡𝑠𝑐𝑟 , and 𝑡𝑠𝑖 to indicate timestamp, commit_timestamp, read_timestamp,\n",
            "creation_timestamp, and invalidation_timestamp, respectively.Figure 2 illustrates how transactions\n",
            "execute edge in- sert, delete, and update. Transactions and their modification to the edge-deltas\n",
            "block and delta-chains index are in the same color. The edge-deltas block for Vertex 1 originally\n",
            "stores the 3 edges (1, 12), (1, 3), and (1, 5) using 3 edge-deltas and 2 delta-chains. Delta-chains\n",
            "Index Entry 0 stores 𝑜 𝑓 𝑓 𝑠𝑒𝑡 = 64 that points to (1, 12), and Entry 1 stores 𝑜 𝑓 𝑓 𝑠𝑒𝑡 = 192\n",
            "pointing to (1, 5). Transaction 𝑇1 inserts a new edge (1, 8) with 32 bytes of property data. 𝑇1\n",
            "calculates that Edge (1, 8) belongs to Delta-Chain 0 = 8 mod 2 where 2 is the number of delta-\n",
            "chains, and uses the 0𝑡ℎ delta-chains index entry 64 to locate the edge-deltas chain, and searches\n",
            "for a previous version of (1, 8). Then, it allocates space in both the delta region and the data\n",
            "region by updating combined_offset atomically. Next, it writes the new in- sert edge-delta and its\n",
            "property into the allocated region. Note that the new edge-delta has previous_offset=64 and\n",
            "previous_version=0 because it needs to point to the previous edge-delta on the delta- chain, and\n",
            "Edge (1, 8) has no previous version. At commit phase, 𝑇1 updates delta-chains index entry to point\n",
            "to the new edge-delta, and the commit manager commits 𝑇1 at 𝑡𝑠𝑐 = 14. The edge-delta’s 𝑡𝑠𝑐𝑟 gets\n",
            "updated to 14. Next, Transaction 𝑇2 deletes Edge (1, 5). 𝑇2 uses the delta-chains index entry 1 to\n",
            "find the previous version of (1, 5) at offset 196. Then, it allocates the space for its edge-delta\n",
            "by updating the combined_offset. Since 𝑇2 is deleting (1, 5), it does not allocate space in the data\n",
            "region. 𝑇2 creates its delete edge-delta, and stores offset 192 as the new delta’s previous version.\n",
            "𝑇2 com- mits in a similar way by updating the delta-chains index Entry 1 and registers with the\n",
            "commit manager.  3.3 Edge Read Operation GTX supports two types of edge read operations: Single-edge\n",
            "lookup and adjacency list scan. Single-edge lookup uses the delta-chains index and is identical as\n",
            "checking whether an edge exists in an edge write operation. Adjacency list scan is implemented as\n",
            "scan- ning the whole edge-deltas block. After locating the edge-deltas block, 𝑡 loads the\n",
            "combined_offset. The combined_offset determines where in the edge-deltas block the scan starts. 𝑡\n",
            "scans all edge-deltas starting from there, and can see all edge-deltas created before and\n",
            "invalidated after (or not invalidated at all) 𝑡’s 𝑟𝑡𝑠.  3.4 Hybrid Commit GTX supports a hybrid\n",
            "group commit protocol. GTX manages a transaction table that provides 𝑂 (1) access time of looking up\n",
            "a transaction’s status. A transaction registers a commit request by in- forming the commit manager.\n",
            "The commit manager keeps a global write epoch number, and assigns its value as the 𝑤𝑡𝑠 to the commit\n",
            "group of transactions by updating their status in the transaction ta- ble. After committing each\n",
            "group, the commit manager increments the global read epoch and global write epoch by 1. The\n",
            "committing transactions eagerly find and update their deltas’ timestamps with their commit\n",
            "timestamp. Concurrent transactions may observe a delta’s timestamp is a committed transaction (ID)\n",
            "by querying the transaction table, and update the delta’s timestamp for the com- mitting\n",
            "transaction. We call this cooperative commit protocol hybrid commit. It reduces group commit\n",
            "latency, and saves cost in synchronizing committing transactions and commit manager.  3.5 Block\n",
            "Consolidation and Garbage Collection Edge-deltas blocks may become overflowed and require allocating\n",
            "a new block for the most recent edge-deltas while recycling the old block with the outdated edge-\n",
            "deltas. GTX uses a latch-free state protection protocol inspired by [10] for each edge-deltas block\n",
            "that eliminates latches in shared memory and preserves cache coherence. Its core is analyzing the\n",
            "edge-deltas block, allocating a new block according to workload history, and migrating the latest\n",
            "version edge-deltas to the new block while concurrent transactions can still read the original edge-\n",
            "deltas block. After consolidating an edge- deltas block, the old deltas block is considered outdated\n",
            "and is placed in a queue. GTX tracks timestamps of current running transactions, and periodically\n",
            "has its worker threads free the memory blocks no transactions can access.  3  GTX Sortledton\n",
            "LiveGraph  PR 4365903 3568179 640138  SSSP 4265851 3490729 630250  PR hotspot 4959926 1364880 418388\n",
            "SSSP hotspot 4265851 N/A 405232  Table 3: Mixed-Workload Throughput(txns/s)  [7] Alexandru Iosup and\n",
            "et al. 2016. LDBC Graphalytics: A Benchmark for Large- Scale Graph Analysis on Parallel and\n",
            "Distributed Platforms. 9, 13 (2016), 12. Jérôme Kunegis. [n.d.]. The KONECT Project.\n",
            "http://konect.cc/  [8] [9] Changji Li and et al. 2022. ByteGraph: A High-Performance Distributed\n",
            "Graph  Database in ByteDance. Proc. VLDB Endow. 15, 12 (2022).  [10] Tianyu Li, Badrish\n",
            "Chandramouli, and Samuel Madden. 2022. Performant Almost- Latch-Free Data Structures Using Epoch\n",
            "Protection. In Data Management on New Hardware (DaMoN’22). Article 1, 10 pages.  GTX Sortledton\n",
            "LiveGraph  PR 15993070 10525814 13720317  SSSP 22575448 15560585 21072157  PR hotspot 14020837\n",
            "10391524 10633277  SSSP hotspot 18414454 N/A 18571507  [11] Yingjun Wu, Joy Arulraj, Jiexi Lin, Ran\n",
            "Xian, and Andrew Pavlo. 2017. An Empirical Evaluation of In-Memory Multi-Version Concurrency\n",
            "Control. Proc. VLDB Endow. 10, 7 (2017). https://doi.org/10.14778/3067421.3067427  [12] Xiaowei Zhu\n",
            "and et al. 2020. LiveGraph: A Transactional Graph Storage System with Purely Sequential Adjacency\n",
            "List Scans. 13, 7 (mar 2020), 1020–1034.  Table 4: Mixed-Workload Latency(microsecond)  4  use\n",
            "\"temporal localities\" to collectively refer to graph updates with both temporal localities and\n",
            "hotspots. State-of-the-art transactional graph systems use coarse-grained concurrency control (e.g.,\n",
            "vertex- centric locking [6, 12]) and suffer from these update patterns [6]. To address these\n",
            "challenges and requirements, a graph system needs to support transactions, adapt to the temporal\n",
            "localities, and reduce the interference between concurrent transactions and analytics.  This demo\n",
            "presents GTX, a main memory latch-free transac- tional graph data system that is optimized for read-\n",
            "write transac- tions while maintaining competitive graph analytics. It targets the open problem of\n",
            "maintaining high throughput in workloads involv- ing temporal localities [6]. Many real world graphs\n",
            "are power-law graphs with hub vertices (vertices with large degree) [6, 12]. For example, one of the\n",
            "real world graph edit-wiki has vertices with millions of adjacent edges and the edges’ creation\n",
            "follows temporal localities and contains hotspots [6, 8]. Most of the existing transac- tional graph\n",
            "systems only support transactions, but are unware of temporal localities. Moreover, they suffer from\n",
            "concurrently run- ning updates and graph analytics. Two of the three state-of-the-art systems that\n",
            "we have evaluated have some bugs under such con- current workloads. Therefore, we design GTX with\n",
            "the following features to resolve these challenges:  1. GTX has a latch-free graph storage that uses\n",
            "atomic operations to update vertices and edges. It eliminates latching overheads and reduces thread\n",
            "idling.  2. GTX combines chain-based delta storage and linear delta storage to benefit from delta-\n",
            "chains’ efficient lookup and linear storage’s cache performance.  3. GTX has an efficient\n",
            "transaction management and concurrency control protocol by managing concurrency control at delta-\n",
            "chain level that adopts to the workload history.  4. GTX has a hybrid group commit protocol that\n",
            "improves trans-  action commit throughput.  In this demo, we will present how GTX handles vertex and\n",
            "edge creation transactions of real world power-law graphs with and with- out temporal localities and\n",
            "how GTX executes read-write transac- tions while concurrently executing graphs analytics using\n",
            "synthetic graph logs in Graph Framework Evaluation(GFE) [5, 6].  2 OVERVIEW OF GTX GTX addresses the\n",
            "problem of managing and analyzing dynamic la- beled property graphs. It consists of a latch-free\n",
            "adjacency list-based graph store and a transaction manager and a concurrency control protocol. GTX\n",
            "can manage both uniform and power-law graphs but is optimized for power-law graphs as many real-\n",
            "world graphs are power-law graphs [12]. In this demonstration, we focus on manag- ing power-law\n",
            "graphs. GTX’s graph store is a multi-version delta store, where each delta captures an operation\n",
            "(insert, update) of a vertex or (insert, delete, update) of an edge. An overview of GTX’s  Figure 1:\n",
            "GTX Storage: 1.Vector-based vertex index (entry). 2.Vertex delta chain. 3.Edge-deltas block  Dataset\n",
            "yahoo-songs edit-wiki graph500-24  Category real-world real-world synthetic  Vertex Count Edge Count\n",
            "256,804,235 572,591,272 260,379,520  1,625,951 50,757,442 8,870,942  Table 1: Dataset Statistics  4\n",
            "DEMONSTRATION AND EVALUATION We demonstrate GTX’s abilities to ingest real-world graphs with\n",
            "hotspots and to efficiently execute concurrent read-write transac- tions and graph analytics. We\n",
            "evaluate GTX’s performance against other state-of-the-art transactional graph systems, illustrating\n",
            "GTX’s high throughput in transactions, and robustness across workloads while maintaining competitive\n",
            "graph analytics performance.  4.1 Demonstration Scenario We showcase GTX’s high transaction\n",
            "throughput for workloads with temporal localities, while maintaining competitive graph an- alytics\n",
            "performance. We compare GTX’s transaction throughput  Figure 2: Examples  and graph analytics\n",
            "latency against state-of-the-art transactional graph systems.We adopt the Graph Framework4\n",
            "Evaluation (GFE) from Teseo [5] and Sortledton [6] for evaluation and demonstration. We use two\n",
            "real-world power-law graph datasets yahoo-songs(y-s) and edit-wiki(e-w) [8] and two sets of\n",
            "synthetic graph update logs (2.6 billion entries each) generated by [4] on graph500-24 [7]. One\n",
            "graph log shuffles edge updates and the other contains temporal localities. The statistics of the\n",
            "datasets can be found in Table 1.  We execute the demonstration on a dual-socket machine with\n",
            "Intel(R) Xeon(R) Platinum 8368 CPU @ 2.40GHz processors with 156 CPU and 192GB of DRAM. We run all\n",
            "evaluations in a single NUMA node if the memory size fits. All systems execute checked operations as\n",
            "transactions, checking an edge’s existence before performing the write. This demonstration only\n",
            "considers undirected graphs and each system treats an undirected edge as two directed edges.\n",
            "Therefore, for each edge 𝑒 (𝑢, 𝑣), each system creates a transaction that checks whether 𝑒 (𝑢, 𝑣)\n",
            "and 𝑒 (𝑣, 𝑢) exist, and inserts both edges.<|eot_id|><|start_header_id|>user<|end_header_id|>  What\n",
            "kind of Edge Read operations GTX support?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "Single-edge lookupAdjacency list scan  What kind of Edge Write operations GTX\n",
            "support?InsertUpdateDelete  How do GTX handle edge-deltas overflow?Block consolidationGarbage\n",
            "collection  What is GTX's concurrency control mechanism?Delta-chain based  What is GTX's commit\n",
            "protocol?Hybrid commit  What is GTX's transaction model?Snapshot isolation  What is GTX's storage\n",
            "structure?Multi-version delta storage  What is GTX's storage layout?Linear edge-delta storage  What\n",
            "is GTX's concurrency control granularity?Vertex and edge  What is GTX's concurrency control\n",
            "type?Optimistic  What is GTX's concurrency control algorithm?Multi-version concurrency control  What\n",
            "is GTX's concurrency control implementation?Atomic operations  What is GTX's storage\n",
            "structure?Multi-version delta storage  What is GTX's storage layout?Linear edge-delta storage  What\n",
            "is GTX's concurrency control granularity?Vertex and edge  What is GTX's concurrency control\n",
            "type?Optimistic  What is GTX's concurrency control algorithm?Multi-version concurrency control  What\n",
            "is GTX's concurrency control implementation?Atomic operations  What is GTX's storage\n",
            "structure?Multi-version delta storage  What is GTX's storage layout?Linear edge-delta storage  What\n",
            "is GTX's concurrency control granularity?Vertex and edge  What is GTX's concurrency control\n",
            "type?Optimistic  What is GTX's concurrency control algorithm?Multi-version concurrency control  What\n",
            "is GTX's concurrency control implementation?Atomic operations  What is GTX's storage\n",
            "structure?Multi-version delta storage  What is GTX's storage layout?Linear edge-delta storage  What\n",
            "is GTX's concurrency control granularity?Vertex and edge  What is GTX's concurrency control\n",
            "type?Optimistic  What is GTX's concurrency control algorithm?Multi-version concurrency control  What\n",
            "is GTX's concurrency control implementation?Atomic operations  What is GTX's storage\n",
            "structure?Multi-version delta storage  What is GTX's storage layout?Linear edge-delta storage  What\n",
            "is GTX's concurrency control granularity?Vertex and edge  What is GTX's concurrency control\n",
            "type?Optimistic  What is GTX's concurrency control algorithm?Multi-version concurrency control  What\n",
            "is GTX's concurrency control implementation?Atomic operations  What is GTX's storage\n",
            "structure?Multi-version delta storage  What is GTX's storage layout?Linear edge-delta storage  What\n",
            "is GTX's concurrency control granularity?Vertex and edge  What is GTX's concurrency control\n",
            "type?Optimistic  What is GTX's concurrency control algorithm?Multi-version concurrency control  What\n",
            "is GTX's concurrency control implementation?Atomic operations  What is GTX's storage\n"
          ]
        }
      ],
      "source": [
        "query = \"What kind of Edge Read operations GTX support?\"\n",
        "result = Chain_web.invoke(query)\n",
        "print(fill(result['result'].strip(), width=100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e15evr_tzfFy",
        "outputId": "ca6fad4d-a94e-417c-828e-c0040eb9a440"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/vectorstores.py:330: UserWarning: Relevance scores must be between 0 and 1, got [(Document(page_content='Algorithm 2: Private SPN Construction PrivSPN(𝑇 , 𝜖) : table 𝑇 , total privacy budget 𝜖. Input Output : A tree of SPN 𝑡 = (parent, left, right)\\n\\n1 op, 𝜖op, ¯𝜖 ← Planning(𝑇 , 𝜖); 2 parent, ( (cid:101)S𝐿, (cid:101)S𝑅) ← ParentGen(𝑇 , op, 𝜖op); 3 left, right ← ChildrenGen(𝑇 , op, (cid:101)S𝐿, (cid:101)S𝑅, ¯𝜖); 4 return (parent, left, right) 5 procedure ParentGen(𝑇 , op, 𝜖op) if op = OP.LEAF then 6 Δ(hist) (cid:101)𝐻 ← hist(𝑇 ) + Lap( 𝜖op else if op = OP.SUM then\\n\\n); parent ← LeafNode( (cid:101)𝐻 );\\n\\n7\\n\\n8\\n\\n9\\n\\n10\\n\\n11\\n\\n(cid:101)S𝐿, (cid:101)S𝑅 ← RowSplit(𝑇 , 𝜖op ); parent ← SumNode( (cid:101)S𝐿, (cid:101)S𝑅 ); else if op = OP.PRODUCT then (cid:101)S𝐿, (cid:101)S𝑅 ← ColSplit(𝑇 , 𝜖op ); parent ← ProdNode( (cid:101)S𝐿, (cid:101)S𝑅 );\\n\\n12\\n\\nreturn parent, ( (cid:101)S𝐿, (cid:101)S𝑅)\\n\\n13 procedure ChildrenGen(𝑇 , op, (cid:101)S𝐿, (cid:101)S𝑅, ¯𝜖)\\n\\n14\\n\\n15\\n\\nif op = OP.SUM then 𝜖𝑙 ← ¯𝜖, 𝜖𝑟 ← ¯𝜖\\n\\n16\\n\\nelse if op = OP.PRODUCT then\\n\\n17\\n\\n𝜖𝑙 ← ¯𝜖 ·\\n\\n𝜎 (𝑇 [ (cid:101)S𝐿 ] ) 𝜎 (𝑇 [ (cid:101)S𝐿 ] )+𝜎 (𝑇 [ (cid:101)S𝑅 ] )\\n\\n, 𝜖𝑟 ← ¯𝜖 − 𝜖𝑙\\n\\n18\\n\\nif op = OP.LEAF then\\n\\n19\\n\\nleft ← null; right ← null;\\n\\n20\\n\\nelse\\n\\n21\\n\\nleft ← PrivSPN(𝑇 [ (cid:101)S𝐿 ], 𝜖𝑙 ); right ← PrivSPN(𝑇 [ (cid:101)S𝑅 ], 𝜖𝑟 );\\n\\n22\\n\\nreturn left, right\\n\\n4.2 Private SPN Construction 4.2.1 Overview of PrivSPN. Algorithm 2 outlines our process for constructing an SPN on a single table. Following DeepDB [13], we employ a tree-structured SPN to model a given table 𝑇 . The PrivSPN procedure recursively generates a binary tree 𝑡 = (parent, left, right), where parent should be a parent node, and left, right should be two subtrees. We divide PrivSPN into the following three procedures. • Planning (Operation Planning): We decide an operation op for the parent node and the privacy budget 𝜖op allocated to op. • ParentGen (Parent Generation): According to the operation op\\n\\nand the privacy budget 𝜖op, we generate the parent node.\\n\\nChildrenGen (Children generation): We further generate the chil- dren left, right through recursive calls to PrivSPN.', metadata={'source': '/content/document4.pdf'}), 0.10285122441649686), (Document(page_content='5.1 Privacy Analysis on PrivSPN In this subsection, we first prove that the Planning, ParentGen, and ChildrenGen procedures satisfy DP, and then prove that PrivSPN therefore achieves DP.\\n\\n5.1.1 Analysis of Planning. The Planning procedure consists of the CorrTrial, DecideOP, and AllocBudgetOP subprocedures. If the subprocedures satisfy DP, we can conclude that Planning ensures DP according to the sequential composition theorem of DP [9].\\n\\nCorrTrial. (1) When |𝑇 | ≥ 2𝛽 and |𝑇 .Attr| > 1, the CorrTrial proce- dure employs the 𝜖eval · 𝛾2-DP procedure ColSplit(𝑇 , 𝜖eval · 𝛾2) to (cid:98)S𝐿, (cid:98)S𝑅 (cid:1). Then, it calculates the NMI compute a perturbed partition (cid:0) Δ(NMI𝑐𝑜𝑙 ) 𝜖eval · (1−𝛾2 ) ) into it, of ( (cid:98)S𝐿, (cid:98)S𝑅) and injects the Laplace noise Lap( which ensures (cid:0)𝜖eval · (1 − 𝛾2)(cid:1)-DP. Consequently, according to the sequential composition theorem of DP [9], CorrTrial(𝑇 , 𝜖) satisfies 𝜖eval-DP. (2) When |𝑇 | < 2𝛽 or |𝑇 .Attr| = 1, since varying the value of any row in 𝑇 does not have any impact on the output (𝜎𝑐𝑜𝑙 , 𝜖eval), CorrTrial(𝑇 , 𝜖) satisfies 0-DP according to Definition 1. Note that we adopt the bounded DP interpretation; in the case of unbounded DP, adding/removing a row from 𝑇 causes the procedure to take the other conditional branch (i.e., Lines 11–14), thereby breaking the DP guarantee.\\n\\nDecideOP and AllocBudgetOP. Similar to the second case of CorrTrial, for both procedures DecideOP and AllocBudgetOP, because chang- ing the value of any row in 𝑇 does not affect the table size |𝑇 | and the table dimension |𝑇 .Attr|, it also does not impact their outputs. Therefore, they ensure 0-DP.\\n\\nPlanning. When |𝑇 | < 2𝛽 or |𝑇 .Attr| = 1, Planning satisfies 0-DP since any change to 𝑇 does not impact the output (Lines 1–2); otherwise, since it sequentially combines CorrTrial, DecideOP, and AllocBudgetOP, Planning(𝑇 , 𝜖) satisfies the 𝜖eval-DP according to the sequential composition theorem of DP [9], where the value of 𝜖eval differs in different cases (see Lines 12 and 16).', metadata={'source': '/content/document4.pdf'}), 0.08027467495575735), (Document(page_content='This work is licensed under the Creative Commons BY-NC-ND 4.0 International License. Visit https://creativecommons.org/licenses/by-nc-nd/4.0/ to view a copy of this license. For any use beyond those covered by this license, obtain permission by emailing info@vldb.org. Copyright is held by the owner/author(s). Publication rights\\n\\nlicensed to the VLDB Endowment. Proceedings of the VLDB Endowment, Vol. 17, No. 1 ISSN 2150-8097. doi:XX.XX/XXX.XX\\n\\n1\\n\\ncount: 1600Size765Price45.435.210.5\\n\\nID135…\\n\\nOriginal Table 𝑇\\n\\nSize358Price15.010.025.0……count: 160𝑇[ሚ𝒮𝐿]\\n\\nID245…\\n\\n×\\n\\n040.0\\n\\n×\\n\\nID245…count: 340Size469Price40.030.032.0\\n\\n110\\n\\n+\\n\\nID136…\\n\\n𝑝𝑒𝑟𝑡𝑢𝑟𝑏(ሚ𝒮𝐿,ሚ𝒮𝑅)Size486935…Price40.025.030.032.015.010.0…ID123456…count: 500\\n\\n×\\n\\n𝐻\\n\\n……\\n\\nSize765…count: 1600\\n\\n…Size271count: 3400…Price94.122.633.4\\n\\n+\\n\\n…(𝒮𝐿,𝒮𝑅)\\n\\nID136…\\n\\n1\\n\\ncount: 5000(b) Learning an SPN and Generating Datacount: 160count: 160count: 340count: 340\\n\\n040.0count: 1600Price45.435.210.5\\n\\n……𝑇[ሚ𝒮𝑅]\\n\\nPrivate SPN ConstructionSPN-Based Database Synthesis\\n\\ncount: 3400Size765271…Price45.435.210.594.122.633.4…\\n\\n110110\\n\\n×\\n\\nSize358…Price15.010.025.0…Price40.030.032.0…Size469…\\n\\ncount: 3400\\n\\n𝑝𝑒𝑟𝑡𝑢𝑟𝑏෩𝐻\\n\\nID246…\\n\\nSize271…Price94.122.633.4…\\n\\nData SamplingSynthetic Table \\u0de0𝑇\\n\\nFigure 1: Learning an SPN and generating data based on single-table.', metadata={'source': '/content/document4.pdf'}), 0.04052421195749567), (Document(page_content='and the privacy budget 𝜖op, we generate the parent node.\\n\\nChildrenGen (Children generation): We further generate the chil- dren left, right through recursive calls to PrivSPN.\\n\\n4.2.2 Planning. The Planning procedure takes the role of deter- mining an operation op for the parent node, which can be creating a leaf/sum/product node. A sum (resp. product) node indicates par- titioning the rows (resp. columns) of the given table, while a leaf node means generating the table’s histogram. Since all these types of operations should be performed in a differentially private man- ner, Planning also allocates a privacy budget 𝜖op for the parent\\n\\n5\\n\\nAlgorithm 3: Operation Planning Planning(𝑇 , 𝜖) Input Output : next operation op, privacy budget 𝜖op for op,\\n\\n: table 𝑇 , privacy budget 𝜖\\n\\nremaining privacy budget ¯𝜖\\n\\nParam: threshold for column splitting 𝛼, minimum table\\n\\nsize 𝛽, budget ratios 𝛾1, 𝛾2 1 if |𝑇 | < 2𝛽 and |𝑇 .Attr| = 1 then 2\\n\\nop ← OP.LEAF; 𝜖op ← 𝜖; ¯𝜖 ← 0;\\n\\n3 else 4\\n\\n5\\n\\n6\\n\\n𝛿𝑐𝑜𝑙 , 𝜖eval ← CorrTrial(𝑇 , 𝜖); op ← DecideOP(𝑇 , 𝛿𝑐𝑜𝑙 ); 𝜖op, ¯𝜖 ← AllocBudgetOP(𝑇 , op, 𝜖, 𝜖eval);\\n\\n7 return op, 𝜖op, ¯𝜖; 8 procedure CorrTrial(𝑇 , 𝜖) 9\\n\\nif |𝑇 | ≥ 2𝛽 and |𝑇 .Attr| > 1 then\\n\\n10\\n\\n11\\n\\n12\\n\\n𝜖eval ← 𝜖 · 𝛾1; ( (cid:98)S𝐿, (cid:98)S𝑅) ← ColSplit(𝑇 , 𝜖eval · 𝛾2); 𝛿𝑐𝑜𝑙 ← NMI(𝑇 [ (cid:98)S𝐿],𝑇 [ (cid:98)S𝑅]) + Lap(\\n\\nΔ(NMI) 𝜖eval · (1−𝛾2 ) );\\n\\n13\\n\\nelse\\n\\n14\\n\\n𝜖eval ← 0; 𝛿𝑐𝑜𝑙 ← 0;\\n\\nreturn 𝛿𝑐𝑜𝑙 , 𝜖eval 15 16 procedure DecideOP(𝑇 , 𝛿𝑐𝑜𝑙 ) 17\\n\\nif |𝑇 | ≥ 2𝛽 and |𝑇 .Attr| > 1 then\\n\\n18\\n\\nif 𝛿𝑐𝑜𝑙 ≤ 𝛼 then\\n\\n19\\n\\nop ← OP.PRODUCT\\n\\n20\\n\\nelse\\n\\n21\\n\\nop ← OP.SUM\\n\\n22\\n\\nelse if |𝑇 | ≥ 2𝛽 and |𝑇 .Attr| = 1 then\\n\\n23\\n\\nop ← OP.SUM\\n\\n24\\n\\nelse if |𝑇 | < 2𝛽 and |𝑇 .Attr| > 1 then\\n\\n25\\n\\nop ← OP.PRODUCT\\n\\n26\\n\\nreturn op\\n\\n27 procedure AllocBudgetOP(𝑇 , op, 𝜖, 𝜖eval)\\n\\n28\\n\\nif op = OP.PRODUCT and |𝑇 .Attr| = 2 then\\n\\n29\\n\\n𝜖op ← 0;\\n\\n30\\n\\nelse\\n\\n31\\n\\n32\\n\\n𝜖op ← 𝜖/2 − 𝜖eval; return 𝜖op, 𝜖 − 𝜖eval − 𝜖op', metadata={'source': '/content/document4.pdf'}), 0.03013484521216625), (Document(page_content='Example 2. Figure 3 shows an example of how PrivBench com- plements the original SPN 𝑡𝑖 with the leaf nodes of the foreign key Type-ID. First, we identify that attribute size attribute possesses the most leaf nodes in 𝑡𝑖 . For each leaf node of attribute size, we find the corresponding subcolumn of attribute Type − ID using the row indices of the subcolumn of attribute size and calculate its fanout table. Then, we perturb the fanout table and store it in a new leaf node. Finally, the new leaf node is added into the SPN as a sibling to the leaf node of attribute size and as the child of a newly added product node, resulting in a modified SPN 𝑡 ′ 𝑖 .\\n\\n4.4 SPN-Based Database Synthesis Algorithm 5 shows how we synthesize a table (cid:98)𝑇𝑖 given an SPN 𝑡𝑖 . If the root node of 𝑡𝑖 is a leaf node, we sample a table (cid:98)𝑇𝑖 ac- cording to the histogram or fanout table stored in the node (Lines 1–2). If it is a sum (product, resp.) node, we recursively apply the SampleDataFromSPN procedure to the left and right children of 𝑡𝑖 and vertically (horizontally, resp.) concatenate the returned tables (cid:98)𝑇𝐿 and (cid:98)𝑇𝑅 (Lines 4–9). Finally, we obtain a table (cid:98)𝑇𝑖 that has a data distribution similar to that of the private table 𝑇𝑖 . By synthesizing tables for all modified SPNs, we obtain a synthetic database (cid:98)𝐷.\\n\\n7\\n\\n5 PRIVACY ANALYSIS In this section, we rigoriously analyze how PrivBench satisfies DP and how we should allocate privacy budgets to some key steps in PrivBench.1\\n\\n5.1 Privacy Analysis on PrivSPN In this subsection, we first prove that the Planning, ParentGen, and ChildrenGen procedures satisfy DP, and then prove that PrivSPN therefore achieves DP.\\n\\n5.1.1 Analysis of Planning. The Planning procedure consists of the CorrTrial, DecideOP, and AllocBudgetOP subprocedures. If the subprocedures satisfy DP, we can conclude that Planning ensures DP according to the sequential composition theorem of DP [9].', metadata={'source': '/content/document4.pdf'}), -0.01292229093286612), (Document(page_content='Planning. When |𝑇 | < 2𝛽 or |𝑇 .Attr| = 1, Planning satisfies 0-DP since any change to 𝑇 does not impact the output (Lines 1–2); otherwise, since it sequentially combines CorrTrial, DecideOP, and AllocBudgetOP, Planning(𝑇 , 𝜖) satisfies the 𝜖eval-DP according to the sequential composition theorem of DP [9], where the value of 𝜖eval differs in different cases (see Lines 12 and 16).\\n\\nLemma 1. For any 𝜖 ≥ 0 and any 𝛾1 ≥ 0, if |𝑇 | ≥ 2𝛽 and |𝑇 .Attr| > 1, Planning(𝑇 , 𝜖) satisfies (𝜖 ·𝛾1)-DP; otherwise, it satisfies 0-DP at the table level.\\n\\n5.1.2 Analysis of ParentGen. Then, we show that in any cases of the given operation op, ParentGen achieves DP. (1) When op = OP.LEAF, we ensure DP using the Laplace mechanism. That is, Δ(hist) we inject Laplacian noise Lap( ) into the histogram hist(𝑇 ), 𝜖op which satisfies 𝜖op-DP. (2) When op = OP.SUM, we employ a DP row splitting mechanism RowSplit(𝑇 , 𝜖op) to generate a per- turbed row partition (cid:101)𝑆𝐿, (cid:101)𝑆𝑅, which achieves 𝜖op-DP. (3) When op = OP.PRODUCT, our column splitting mechanism ColSplit(𝑇 , 𝜖op) is essentially an instance of the Exponential mechanism, thereby guaranteeing 𝜖op-DP.\\n\\n1All missing proofs can be found in our online appendix: https://drive.google.com/file/ d/1mjhKOajBNvN5RbX7k_pvEoFi9DRCWuza/view?usp=sharing.\\n\\nLemma 2. For any op and any 𝜖op ≥ 0, ParentGen(𝑇 , op, 𝜖op)\\n\\nTheorem 4. Let T denote the set of all possibles subtables of 𝑇 with |𝑡 | < 2𝛽 and 𝑡 .Attr = 1, ∀𝑡 ∈ T . If for any 𝑡 ∈ T , PrivSPN(𝑇 , 𝜖) calls PrivSPN(𝑡, 𝜖leaf), PrivSPN(𝑇 , 𝜖) satisfies table-level 𝜖-DP, where 𝜎 (𝑇 ) = 2|𝑇 .Attr|+ |𝑇 |', metadata={'source': '/content/document4.pdf'}), -0.0489715019885979), (Document(page_content='14\\n\\n𝜖eval ← 0; 𝛿𝑐𝑜𝑙 ← 0;\\n\\nreturn 𝛿𝑐𝑜𝑙 , 𝜖eval 15 16 procedure DecideOP(𝑇 , 𝛿𝑐𝑜𝑙 ) 17\\n\\nif |𝑇 | ≥ 2𝛽 and |𝑇 .Attr| > 1 then\\n\\n18\\n\\nif 𝛿𝑐𝑜𝑙 ≤ 𝛼 then\\n\\n19\\n\\nop ← OP.PRODUCT\\n\\n20\\n\\nelse\\n\\n21\\n\\nop ← OP.SUM\\n\\n22\\n\\nelse if |𝑇 | ≥ 2𝛽 and |𝑇 .Attr| = 1 then\\n\\n23\\n\\nop ← OP.SUM\\n\\n24\\n\\nelse if |𝑇 | < 2𝛽 and |𝑇 .Attr| > 1 then\\n\\n25\\n\\nop ← OP.PRODUCT\\n\\n26\\n\\nreturn op\\n\\n27 procedure AllocBudgetOP(𝑇 , op, 𝜖, 𝜖eval)\\n\\n28\\n\\nif op = OP.PRODUCT and |𝑇 .Attr| = 2 then\\n\\n29\\n\\n𝜖op ← 0;\\n\\n30\\n\\nelse\\n\\n31\\n\\n32\\n\\n𝜖op ← 𝜖/2 − 𝜖eval; return 𝜖op, 𝜖 − 𝜖eval − 𝜖op\\n\\nnode and calculates the remaining privacy budget ¯𝜖 for the children. Concretely, Planning proceeds as follows. 1. Leaf node validation (Lines 1–2). We validate if we should only generate a leaf node for the given table. When both the size and the dimension of the given table are insufficient to split, i.e., |𝑇 | < 2𝛽 and |𝑇 .Attr| = 1, the operation must be creating a leaf node, i.e., ← OP.LEAF, and all the given privacy budget 𝜖 should be allocated for op, i.e., 𝜖op = 𝜖. Here, we require that the size of a split subtable should not be smaller than the parameter 𝛽 for the sake of utility, which is ensured by the row splitting algorithm RowSplit.\\n\\n2. Correlation trial (procedure CorrTrial). We preliminarily trial the performance of column splitting, which will be used to decide the operation op by the DecideOP procedure. Specifically, we consume a slight privacy budget 𝜖eval to generate a trial column partition ( (cid:98)S𝐿, (cid:98)S𝑅) (Line 11) and compute the perturbed correlation ),\\n\\nwhere NMI calculates the normalized mutual information of two subtables, Δ returns the global sensitvity of the given function, and Lap generates the Laplace noise (Line 12). Intuitively, the smaller the correlation between the two subtables, the smaller the impact of column splitting on the data distribution, hence the better the performance of column splitting [13]. Note that when either col- umn or row splitting is infeasible, we do not need to evaluate the correlation since the operation must be the feasible one (Line 14).', metadata={'source': '/content/document4.pdf'}), -0.05562210609622298), (Document(page_content='7.5 Case Study In this subsection, we conduct two case studies to demonstrate the performance of PrivBench in real-world scenarios.\\n\\nCase study 1: Real estate database. In this study, we select a real es- tate database [19] collected from Australia, which provides a rich and diverse array of property-related information, including details about the properties themselves, addresses, surrounding features, school district information, and more. The database comprises 12\\n\\n12\\n\\ntables and various common data types. We construct 10 complex queries and evaluate the performance of PrivBench in terms of Q- error, KL divergence, and execution time error. As shown in Table 6, PrivBench performs excellently across all metrics, meeting practi- cal needs. The query with the highest Q-error, targeting properties near the top 50 primary schools and their details, is: \"SELECT pb.*, pa.Formated Address, ps.name, sr.Ranking FROM Property_Basic pb JOIN Property_Address pa ON pb.proID = pa.proID JOIN Mapping_School ms ON pb.proID = ms.pro_ID JOIN School ps ON ms.sec1= ps.school_ID JOIN School_Ranking sr ON ps.school_ID = sr.school_ID WHERE sr.Ranking <= 50 and ps.type = \\'Primary\\';\".\\n\\nFor this query, the cardinality on the original data was 2466, while on the generated data, it was 3129. The reason for this discrepancy is the noise perturbation on the Ranking and type attributes, with the error further amplified through the join of five tables, leading to the final cardinality difference. The execution time error for this query was 0.89 seconds, indicating that the query logic on the generated data did not undergo significant changes. The main cost still lies in the join of the five tables, and the changes in the two attributes did not significantly impact the table scan.', metadata={'source': '/content/document4.pdf'}), -0.058974550720476415), (Document(page_content='for PrivBench.\\n\\nsatisfies table-level 0-DP; otherwise, it satisfies table-level ¯𝜖-DP.\\n\\n5.1.4 Analysis of PrivSPN. Based on Lemmas 1, 2, and 3, we con- clude the DP gurantee of PrivSPN through sequential composition.\\n\\nCorollary 1 (of Theorem 5). PrivBench satisfies ((cid:205)𝑖 ∈ [𝑛] 𝜏𝑖 · 𝑖 )-DP at the database level.\\n\\nLemma 4. For any 𝜖 ≥ 0, PrivSPN(𝑇 , 𝜖) satisfies table-level 𝜖-DP.\\n\\n𝜏𝑖 · 𝜖 𝑓\\n\\n𝜖𝑠 𝑖 + (cid:205)𝑇𝑖 refers to 𝑇𝑗\\n\\nIn addition, since PrivSPN allocates the privacy budget 𝜖 from the root to the leaf nodes in a top-down manner, it may result in insufficient budgets allocated to the leaf nodes. Consequently, an interesting question arises: If we allocate a privacy budget 𝜖leaf to all leaf nodes, how large the total privacy budget 𝜖 should be? Theorem 4 answers the question and instructs the design of the scale metirc. That is, if we guarantee a privacy budget 𝜖leaf for each leaf node, then constructing the left and right subtrees should satisfy 𝜎 (𝑇 [(cid:101)𝑆𝐿])-DP and 𝜎 (𝑇 [(cid:101)𝑆𝑅])-DP, respectively. Therefore, the privacy budgets 𝜖𝑙 , 𝜖𝑟 should be proportional to the corresponding scales (see Line 17 of Algorithm 2). In our experiments, we set the scale metric as defined in Theorem 4 for PrivSPN.\\n\\n6 DISCUSSION In this section, we discuss several interesting functionalities of PrivBench.\\n\\nSupported data types. While most works on privacy-preserving database synthesis only support categorical data, PrivBench ad- ditionally supports numeric data. This is because the leaf nodes of the SPN can classify numeric data by range and generate his- tograms for these ranges. Synthetic numeric data is then sampled from these ranges using the histograms. Theoretically, this data synthesis method could also support more complex data types, such\\n\\n8\\n\\nTable 2: Summary of datasets, workloads, and baselines.\\n\\n#Tables\\n\\nDataset\\n\\n#Attributes Workload\\n\\n#Rows\\n\\nBaselines SAM (non-DP), VGAN (non-DP), AIM, DPSynthesizer, CGAN, PrivSyn, DataSyn, PreFair, MST, PrivMRF\\n\\n48842\\n\\nAdult\\n\\nSAM-1000\\n\\n13\\n\\n1\\n\\n×', metadata={'source': '/content/document4.pdf'}), -0.0597358070982803), (Document(page_content='𝑄 − 𝑒𝑟𝑟𝑜𝑟 (𝑞, 𝐷, (cid:98)𝐷) = max\\n\\n(cid:32)𝐶𝑎𝑟𝑑 (𝑞, 𝐷) 𝐶𝑎𝑟𝑑 (𝑞, (cid:98)𝐷)\\n\\n,\\n\\n𝐶𝑎𝑟𝑑 (𝑞, (cid:98)𝐷) 𝐶𝑎𝑟𝑑 (𝑞, 𝐷)\\n\\n(cid:33)\\n\\nwhere 𝑞 denotes a query in the workload, and 𝐶𝑎𝑟𝑑 (𝑞, 𝐷) and 𝐶𝑎𝑟𝑑 (𝑞, (cid:98)𝐷) denote the cardinalities of executing 𝑞 on 𝐷 and (cid:98)𝐷, respectively. Then, we compute the mean Q-error of the queries in the workload.\\n\\n4 PRIVBENCH: DATABASE SYNTHESIS WITH\\n\\nPRIVATE SPNS\\n\\nIn this section, we propose PrivBench, an SPN-based differentially private database synthesis method.\\n\\n4.1 Overview of PrivBench PrivBench utilizes SPNs to address the two concerns outlined in Section 3. SPNs, due to their excellent performance in representing complex dependencies and data distributions at multiple granulari- ties, have been employed in cardinality estimation [13], a prevalent approach intrinsically connected to estimating the runtime cost of queries. Drawing inspiration from this, we adapt the output in [13] to produce a synthesized database, designed to closely resemble the input database in terms of data distribution and cardinality upon query execution. As shown in Algorithm 1, PrivBench involves a three-phase process for database synthesis.\\n\\nPrivate SPN Construction (Lines 1–2). Firstly, we construct an SPN 𝑡𝑖 for each table 𝑇𝑖 in the input database 𝐷. Each SPN 𝑡𝑖 is created by a differentially private algorithm PrivSPN, where each leaf node contains a histogram of a column of a subset of rows in 𝑇𝑖 . • Private Fanout Construction (Lines 3–4). Secondly, we complement each SPN 𝑡𝑖 with some leaf nodes that model primary-foreign key references to obtain a modified SPN 𝑡 ′ 𝑖 . We use a differentially private algorithm PrivFanout to calculate fanout frequencies for each foreign key and store them in the complemented leaf nodes. • SPN-Based Database Synthesis (Lines 5–6) Thirdly, we sample a 𝑖 to synthesize a database (cid:98)𝐷. table (cid:98)𝑇𝑖 from each modified SPN 𝑡 ′', metadata={'source': '/content/document4.pdf'}), -0.06342871396480487)]\n",
            "  warnings.warn(\n",
            "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'query': 'What is Planning in case of Private SPN Construction?',\n",
              " 'result': \"\\n<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nUse the following context to answer the question at the end. Do not use any other information. If you can't find the relevant information in the context, just say you don't have enough information to answer the question. Don't try to make up an answer.\\n\\nAlgorithm 2: Private SPN Construction PrivSPN(𝑇 , 𝜖) : table 𝑇 , total privacy budget 𝜖. Input Output : A tree of SPN 𝑡 = (parent, left, right)\\n\\n1 op, 𝜖op, ¯𝜖 ← Planning(𝑇 , 𝜖); 2 parent, ( (cid:101)S𝐿, (cid:101)S𝑅) ← ParentGen(𝑇 , op, 𝜖op); 3 left, right ← ChildrenGen(𝑇 , op, (cid:101)S𝐿, (cid:101)S𝑅, ¯𝜖); 4 return (parent, left, right) 5 procedure ParentGen(𝑇 , op, 𝜖op) if op = OP.LEAF then 6 Δ(hist) (cid:101)𝐻 ← hist(𝑇 ) + Lap( 𝜖op else if op = OP.SUM then\\n\\n); parent ← LeafNode( (cid:101)𝐻 );\\n\\n7\\n\\n8\\n\\n9\\n\\n10\\n\\n11\\n\\n(cid:101)S𝐿, (cid:101)S𝑅 ← RowSplit(𝑇 , 𝜖op ); parent ← SumNode( (cid:101)S𝐿, (cid:101)S𝑅 ); else if op = OP.PRODUCT then (cid:101)S𝐿, (cid:101)S𝑅 ← ColSplit(𝑇 , 𝜖op ); parent ← ProdNode( (cid:101)S𝐿, (cid:101)S𝑅 );\\n\\n12\\n\\nreturn parent, ( (cid:101)S𝐿, (cid:101)S𝑅)\\n\\n13 procedure ChildrenGen(𝑇 , op, (cid:101)S𝐿, (cid:101)S𝑅, ¯𝜖)\\n\\n14\\n\\n15\\n\\nif op = OP.SUM then 𝜖𝑙 ← ¯𝜖, 𝜖𝑟 ← ¯𝜖\\n\\n16\\n\\nelse if op = OP.PRODUCT then\\n\\n17\\n\\n𝜖𝑙 ← ¯𝜖 ·\\n\\n𝜎 (𝑇 [ (cid:101)S𝐿 ] ) 𝜎 (𝑇 [ (cid:101)S𝐿 ] )+𝜎 (𝑇 [ (cid:101)S𝑅 ] )\\n\\n, 𝜖𝑟 ← ¯𝜖 − 𝜖𝑙\\n\\n18\\n\\nif op = OP.LEAF then\\n\\n19\\n\\nleft ← null; right ← null;\\n\\n20\\n\\nelse\\n\\n21\\n\\nleft ← PrivSPN(𝑇 [ (cid:101)S𝐿 ], 𝜖𝑙 ); right ← PrivSPN(𝑇 [ (cid:101)S𝑅 ], 𝜖𝑟 );\\n\\n22\\n\\nreturn left, right\\n\\n4.2 Private SPN Construction 4.2.1 Overview of PrivSPN. Algorithm 2 outlines our process for constructing an SPN on a single table. Following DeepDB [13], we employ a tree-structured SPN to model a given table 𝑇 . The PrivSPN procedure recursively generates a binary tree 𝑡 = (parent, left, right), where parent should be a parent node, and left, right should be two subtrees. We divide PrivSPN into the following three procedures. • Planning (Operation Planning): We decide an operation op for the parent node and the privacy budget 𝜖op allocated to op. • ParentGen (Parent Generation): According to the operation op\\n\\nand the privacy budget 𝜖op, we generate the parent node.\\n\\nChildrenGen (Children generation): We further generate the chil- dren left, right through recursive calls to PrivSPN.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nWhat is Planning in case of Private SPN Construction?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n4.2.2 Operation Planning. In this section, we describe how to plan operations and allocate privacy budgets for each operation. For simplicity, we assume that the input table 𝑇 has no duplicate rows. We first define the notion of sensitivity of an operation. Definition 4.1 (Sensitivity). Let 𝑆 be a set of tables with the same schema as 𝑇. The sensitivity of an operation op ∈ {OP.SUM, OP.PROD} over 𝑆 is defined as follows:\\n\\n• For OP.SUM, 𝛾(op, 𝑆) = max{𝜎 (𝑇 ) | 𝑇 ∈ 𝑆}. • For OP.PROD, 𝛾(op, 𝑆) = max{𝜎 (𝑇 ) | 𝑇 ∈ 𝑆}, where 𝜎 (𝑇 ) =\\n\\n(cid:102)𝑥∈𝑇 𝑥(cid:103).\\n\\nWe also define the sensitivity of OP.LEAF as 𝛾(OP.LEAF, 𝑆) = 0. Note that the sensitivity of an operation depends on the set of tables it operates on. To simplify the discussion, we will omit the dependency on 𝑆 when there is no ambiguity. Given a table 𝑇, we want to construct an SPN that minimizes the overall error while satisfying the privacy constraint. However, since the exact error bound of an SPN is hard to compute, we instead aim to minimize the upper bound of the error. Specifically, we consider the following objective function:\\n\\n(cid:104)𝑂𝑝𝑐𝑜𝑠𝑡(𝑇 ) = 𝜖op + 𝛾(op, 𝑆)𝜀𝑚𝑎𝑥(𝑇 ), (4.1)\\n\\nwhere 𝜖op is the privacy budget allocated to op, and 𝜀𝑚𝑎𝑥(𝑇 ) is the maximum error of the SPN constructed from 𝑇. Intuitively, 𝜖op is the amount of noise added by op, and 𝛾(op, 𝑆)𝜀𝑚𝑎𝑥(𝑇 ) is the maximum error introduced by all children nodes under op. Therefore, 𝑂𝑝𝑐𝑜𝑠𝑡(\"}"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "query = \"What is Planning in case of Private SPN Construction?\"\n",
        "result = Chain_web.invoke(query)\n",
        "print(fill(result['result'].strip(), width=100))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_z3gmpYexRw"
      },
      "source": [
        "Question without context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3wsGS0B4zfIS",
        "outputId": "53035ddd-fe06-4f65-8558-3fdfb09864bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/vectorstores.py:330: UserWarning: Relevance scores must be between 0 and 1, got [(Document(page_content='[22] Adrienne D Woods et al. “Missing Data and Multiple Imputation Decision Tree”. In: PsyArXiv (2021). DOI: 10.31234/osf.io/mdw5r. URL: https://doi.org/10.31234/ osf.io/mdw5r.\\n\\n10\\n\\n[23]\\n\\nJungyeon Choi, Olaf M. Dekkers, and Saskia le Cessie. “A comparison of different methods to handle missing data in the context of propensity score analysis”. In: European Journal of Epidemiology 34 (1 Jan. 2019), pp. 23–36. ISSN: 15737284. DOI: 10.1007/s10654-018- 0447-z.\\n\\n[24] Thomas L. Saaty. Optimization in Integers and Re- lated Extremal Problems. Ed. by Donald K. Prentiss. McGraw-Hill, Inc, 1970, pp. 1–295. DOI: 10 . 2307 / 2316512.\\n\\n[25] Kelly A. Frazer et al. “A second generation human haplotype map of over 3.1 million SNPs”. In: Nature 449 (7164 Oct. 2007), pp. 851–861. ISSN: 00280836. DOI: 10.1038/nature06258.\\n\\n[26] Kevin Dunn. Kamyr digester-OpenMV.net Datasets. Oct. 2011. URL: https : / / openmv . net / info / kamyr - digester.\\n\\n[27] Bhupinder S Dayal et al. “Application of feedforward: least squares regression neural networks and partial for modelling kappa number in a continuous Kamyr digester: how multivariate data analysis might help pulping”. In: 1994.\\n\\n[28] Bingshan Li et al. “Transcriptome analysis of psoria- sis in a large case-control sample: RNA-seq provides insights into disease mechanisms”. In: Journal of In- vestigative Dermatology 134 (7 2014), pp. 1828–1838. ISSN: 15231747. DOI: 10.1038/jid.2014.28. URL: http: //dx.doi.org/10.1038/jid.2014.28.\\n\\n[29] Peter M Szabo et al. “Cancer-associated fibroblasts are the main contributors to epithelial-to-mesenchymal sig- natures in the tumor microenvironment”. In: Scientific Reports 13 (3051 2023). DOI: 10 . 1038 / s41598 - 023 - 28480- 9. URL: https://doi.org/10.1038/s41598- 023- 28480-9.', metadata={'source': '/content/document5.pdf'}), -0.09738651410372912), (Document(page_content='[7] Donald B. Rubin. “INFERENCE AND MISSING DATA”. In: ETS Research Bulletin Series 1975 (1 June 1975), pp. i–19. ISSN: 2333-8504. DOI: 10.1002/J.2333- 8504.1975.TB01053.X. URL: https://onlinelibrary.wiley. com/doi/full/10.1002/j.2333-8504.1975.tb01053.x. [8] Andrew Briggs et al. “Missing.... presumed at random: cost-analysis of incomplete data”. In: Health Econ- comics 12 (5 2003), pp. 377–392. DOI: 10.1002/hec. 766. URL: https://onlinelibrary.wiley.com/doi/10.1002/ hec.766.\\n\\n[9] Daniel A. Newman. “Missing Data: Five Practical Guidelines”. In: Organizational Research Methods 17 (4 Oct. 2014), pp. 372–411. ISSN: 15527425. DOI: 10. 1177/1094428114548590. Joost R. van Ginkel et al. “Rebutting Existing Mis- conceptions About Multiple Imputation as a Method for Handling Missing Data”. In: Journal of Personality Assessment 102 (3 May 2020), pp. 297–308. ISSN: 00223891. DOI: 10.1080/00223891.2018.1530680. [11] Sharlee Climer, Alan R. Templeton, and Weixiong Zhang. “Allele-Specific Network Reveals Combinato- rial Interaction That Transcends Small Effects in Pso- riasis GWAS”. In: PLoS Computational Biology 10 (9 2014). ISSN: 15537358. DOI: 10 . 1371 / journal . pcbi . 1003766.\\n\\n[10]\\n\\n[12] Sharlee Climer. “Connecting the dots: The boons and banes of network modeling”. In: Patterns 2 (12 Dec. 2021). ISSN: 26663899. DOI: 10 . 1016 / j . patter. 2021 . 100374.\\n\\n[13] Michael Chan. auto-miss. 2019. URL: https : / / github.\\n\\ncom/ClimerLab/auto-miss..\\n\\n[14] Kenneth Smith and Sharlee Climer. “Mr. Clean: An Ensemble of Data Cleaning Algorithms for Increased Data Retention”. In: ().\\n\\n[15] Hyunsoo Kim, Gene H. Golub, and Haesun Park. “Missing value estimation for DNA microarray gene expression data: least squares imputation”. In: Bioinformatics 21 (2 Jan. 2005), pp. 187–198. ISSN: 1367-4811. DOI: 10.1093/bioinformatics/bth499. [16] Olga Troyanskaya et al. “Missing value estimation methods for DNA microarrays”. In: Bioinformatics 17 (6 June 2001), pp. 520–525. ISSN: 1367-4811. DOI: 10. 1093/bioinformatics/17.6.520.\\n\\nlocal', metadata={'source': '/content/document5.pdf'}), -0.1200435640451245), (Document(page_content='3.5 Block Consolidation and Garbage Collection Edge-deltas blocks may become overflowed and require allocating a new block for the most recent edge-deltas while recycling the old block with the outdated edge-deltas. GTX uses a latch-free state protection protocol inspired by [10] for each edge-deltas block that eliminates latches in shared memory and preserves cache coherence. Its core is analyzing the edge-deltas block, allocating a new block according to workload history, and migrating the latest version edge-deltas to the new block while concurrent transactions can still read the original edge-deltas block. After consolidating an edge- deltas block, the old deltas block is considered outdated and is placed in a queue. GTX tracks timestamps of current running transactions, and periodically has its worker threads free the memory blocks no transactions can access.\\n\\n3', metadata={'source': '/content/document3.pdf'}), -0.14631858375639206), (Document(page_content='read-write transaction is executed by its creator worker thread, a read-only transaction can be executed by several OpenMP threads concurrently. GTX further implements a state protection protocol to support edge-deltas block consolidation when it becomes overflow. GTX adopts the block manager from [12] and manages garbage collection lazily. Memory blocks are recycled when no concurrent and future transactions can see them.\\n\\n3 GTX TRANSACTION OPERATIONS Each GTX transaction is assigned a read timestamp (𝑟𝑡𝑠) at its creation time from a global read epoch, and it does not know its write timestamp (𝑤𝑡𝑠) until it gets committed. GTX guarantees Snapshot Isolation [2] of its transactions. Let 𝑡 refer to both the transaction and its transaction ID interchangeably.\\n\\n3.1 Vertex Operations GTX transactions can insert new vertices and update them to new versions. Transaction 𝑡 reads a vertex 𝑣 by using an atomic 𝑙𝑜𝑎𝑑 on 𝑣’s delta-chains pointer after locating the 𝑣’s entry in the index. 𝑡 checks 𝑣’s delta-chain head, and compares its 𝑟𝑡𝑠 against the vertex- delta’s creation time. A transaction can see all deltas created before its 𝑟𝑡𝑠. If the current vertex-delta represents the future version, 𝑡 uses the 𝑝𝑟𝑒𝑣𝑖𝑜𝑢𝑠_𝑝𝑡𝑟 to read the previous vertex-delta (previous version) on the delta-chain. Vertex writes (insertion and updates) are handled by creating a new vertex-delta (new version), and invoking a 𝐶𝐴𝑆 on 𝑣’s delta-chain pointer. If the 𝐶𝐴𝑆 fails, 𝑡 observes write- write conflict and immediately abort.', metadata={'source': '/content/document3.pdf'}), -0.16347681866626806), (Document(page_content='GTX allocates a fixed number of worker threads that execute read-write transactions while additionally supports collectively running graph analytics using OpenMP [1]. Besides a commit man- ager, GTX has no other server thread. Transactions’ isolation and consistency ensures the correctness of concurrent operations. GTX read-write transactions support vertex inserts and updates, edge inserts, deletes, updates and lookups, and adjacency list scans. GTX guarantees atomicity of each transaction’s updates. GTX imple- ments all graph analytics under read-only transactions. While each\\n\\n3.2 Edge Write Operations GTX supports edge inserts, update and deletes, and they are han- dled similarly except having different delta types. GTX executes checked operations. An edge delete only takes place after deter- mining the edge’s existence. An edge insert only takes place after checking that the edge does not exist. Otherwise, the write oper- ation is executed as an update. 𝑡 inserts/updates/deletes an edge, say 𝑒 (𝑢, 𝑣), by creating an insert/update/delete edge-delta of 𝑒 (𝑢, 𝑣).\\n\\n2\\n\\nEdge-deltas blocks manage concurrency at the delta-chain level. 𝑡 calculates the delta-chain 𝑒 (𝑢, 𝑣) belongs to, say the 𝑖𝑡ℎ delta-chain, and locks it in the delta-chains index. After locking the delta-chain, 𝑡 searches the latest edge-delta(version) of 𝑒 (𝑢, 𝑣) using each edge- delta’s 𝑝𝑟𝑒𝑣𝑖𝑜𝑢𝑠_𝑜 𝑓 𝑓 𝑠𝑒𝑡, and writes 𝑡 as its invalidation timestamp if the edge-delta exists. Then, 𝑡 allocates memory by updating the combined_offset, and write the edge-delta in the corresponding loca- tions. The edge-delta stores the offset of the current 𝑖𝑡ℎ delta-chain head, and stores the offset of the previous edge-delta of 𝑒 (𝑢, 𝑣). Moreover, it stores 𝑡 as the new edge-delta’s creation timestamp. If 𝑡 fails to lock the 𝑖𝑡ℎ delta-chain, 𝑡 will abort.', metadata={'source': '/content/document3.pdf'}), -0.17048238353205192), (Document(page_content='Regression imputation tries to capitalize on any relation- ships between missing values and observed features, by train- ing models to predict missing values using the observed data as predictors. Some error may be introduced if the model does not accurately capture the dependence between features. Additionally, the variance for imputed features is biased down- ward, while the covariance is biased upwards [10]. Random error terms are added to regression imputation to counter act the variance and covariance biases. This advanced regression imputation, called stochastic regression, suffers from a bias in the p-values and confidence intervals [10]. The most robust variant of imputation is Multiple Imputation (MI) [10]. In MI, multiple stochastic regression models, typically 5 - 20 [21], are generated on the target data set. Each of these imputed data sets are independently analyzed and the results are combined by various pooling techniques. When an accurate imputation model is selected, the MI estimates are unbiased for MCAR and MAR data. The major drawback of MI is the increased computational requirements, which may not always outweigh the benefits [22]. When the data is MNAR, MI can still suffer biases, but these can be reduced by using the missing indicator method [23]. Imputation methods, in general, can reinforce patterns in observed data and inflate correlation structures [12]. Thus, despite the wide range of available deletion and imputation techniques, no consensus has been reached on the best way to handle missing data.\\n\\nWe continue this manuscript with an overview of the Mr.', metadata={'source': '/content/document5.pdf'}), -0.18963829476764715), (Document(page_content='Missing data is generally handled either by deletion or imputation. Deletion techniques remove samples or features from a data set in order to delete missing elements. The simplest version of deletion, and the default in many software packages at one time [8], is list-wise deletion, where a sample is deleted if it contains any missing data. Testing power is reduced using this technique, as the number of samples is decreased. Additionally, if the data is not MCAR, list-wise deletion results in bias [9]. Similarly, feature-wise deletion removes any feature with missing data. This technique keeps the original testing power of the experiment, but risks deleting strongly predictive features and introducing biases of its own [10]. Another common deletion techniques is called pair- wise deletion, where samples are deleted if they contain any missing data in a subset of the features. Consider the scenario where a regression model is to be created using three features. When cleaning with pair-wise deletion, samples are removed only if missing data occurs in those three features, regardless of missing data in the remaining features. While pair-wise deletion often retains more samples than list-wise, it can result in a covariance matrix that is not positive definite, causing issues for some techniques [9].\\n\\nIn his seminal work, Rubin outlined three mechanisms behind missing data [7]. These mechanisms are differentiated by whether or not the probability of an element being missing is independent, dependent on observed data, or dependent on the missing value itself. If the probability of a given element of data being missing is independent of both the value of the data and the other data elements, then it is considered Missing Completely at Random (MCAR). When the probability of an element being missing is independent of the missing value, but', metadata={'source': '/content/document5.pdf'}), -0.1967342224766384), (Document(page_content='Model\\n\\nSAM DPSynthesizer VGAN CGAN PrivSyn DataSyn PreFair MST PrivMRF AIM PrivBench (sf=1) PrivBench (sf=5) PrivBench (sf=10)\\n\\nKL Div.\\n\\n6.48 41.2 40.6 38.3 40.7 39.5 33.5 35.1 37.6 34.8 4.79 16.72 24.51\\n\\nQ-error\\n\\nExec. time error\\n\\nMean Median 75th 90th MAX Mean Median 75th 90th MAX 1.68 1.97 1.67 41.56 1.65 199.2 1.67 50.03 1.75 1519 1.69 426.9 1.75 381.4 1.74 833.5 1.81 184.2 1.66 766.4 1.64 1.73 2.40 1.75 2.97 1.81\\n\\n1.31 1.45 3.49 8.78 28.39 13.81 13.47 20.22 7.51 16.54 1.12 1.13 1.13\\n\\n1.76 2.61 19.22 22.67 576.3 95.74 103.8 246.3 24.11 161.2 1.45 1.46 1.46\\n\\n2.70 13.42 61.43 60.82 4369 568.0 518.6 1448 61.39 882.1 2.05 2.10 2.14\\n\\n0.62 0.60 0.65 0.63 0.62 0.63 0.67 0.65 0.71 0.68 0.64 0.84 0.95\\n\\n0.81 0.85 0.79 0.81 0.80 0.82 0.86 0.88 0.93 0.82 0.80 1.26 1.44\\n\\n0.52 0.57 0.61 0.56 0.58 0.55 0.63 0.59 0.65 0.59 0.53 0.76 0.83\\n\\n168 3 × 104 1 × 105 2 × 104 3 × 106 1 × 106 8 × 105 2 × 106 8 × 104 4 × 106 241 263 259\\n\\n0.38 0.42 0.44 0.40 0.41 0.44 0.48 0.42 0.53 0.47 0.42 0.59 0.64\\n\\nPrivBench\\n\\nPrivLava\\n\\nNaN\\n\\n10\\n\\n3.2Privacy Budget\\n\\n0.4\\n\\n0\\n\\n0.2\\n\\n25KL Divergence\\n\\n15\\n\\nSAM\\n\\n5\\n\\n1.6\\n\\n0.1\\n\\n0.8\\n\\n20\\n\\n1.00\\n\\nNaN\\n\\n0.1\\n\\n1.25\\n\\n0.00\\n\\n0.25\\n\\n0.4\\n\\n1.50\\n\\n0.50\\n\\nPrivLava\\n\\n1.6\\n\\nPrivBench\\n\\nSAM\\n\\n3.2Privacy Budget\\n\\n0.75\\n\\n0.8\\n\\n0.2\\n\\n1.75Mean Q-error\\n\\n0.8\\n\\n6\\n\\nNaN\\n\\n0.1\\n\\n4\\n\\n1.6\\n\\n3.2Privacy Budget\\n\\n8\\n\\n2\\n\\n0.2\\n\\n10Mean Exection Time Error (%)\\n\\nPrivLava\\n\\nSAM\\n\\nPrivBench\\n\\n0.4\\n\\n0\\n\\n(a) KL divergence\\n\\n(b) Q-error\\n\\n(c) Exec. time error.\\n\\nFigure 5: KL Divergence, Q-error and execution time error of test queries on California. The privacy budget is varied.\\n\\nThe JOB-light dataset [37] includes 6 tables related to movies, which is extracted from the Internet Movie Database (IMDB). In addition to the MSCN-400 workload with up to 2 joins, we also use the JOB-70 workload [37], which focuses on joins of multiple tables and provides 70 queries with up to 5 joins. The baselines includes SAM [37] and DPSynthesizer [18]. Note that we do not test DPSynthesizer on California and PrivLava on JOB-light because their open-source code has not been adapted to these datasets.', metadata={'source': '/content/document4.pdf'}), -0.21263201373494756), (Document(page_content='PVLDB Artifact Availability: The source code, data, and/or other artifacts have been made available at https://github.com/dsegszu/privbench.\\n\\n1 INTRODUCTION Benchmarking, a crucial component for evaluating the performance of DBMSs, has historically leveraged established benchmarks such as the TPC series [1]. These conventional benchmarks use fixed schemas and queries to compare the performance of different data- base systems standardizedly. However, they may fall short in rep- resenting the varied, specific workloads and data characteristics unique to every user. Moreover, the nuances of real-world applica- tions, intrinsic data characteristics, and user-specific performance expectations might not be fully captured by these fixed benchmarks, showing a need for benchmarks more tailored to individual users. For benchmark publishing, creating a database or workload that is specific to a user while ensuring data privacy poses a remark- able challenge. Some solutions try to create databases based on a given workload [37], or produce a workload from an existing data- base [40], yet often overlook a crucial element: privacy protection. Such oversight can lead to potential risks of sensitive information exposure, a concern especially critical in contexts where data pri- vacy is paramount. The task of creating databases for testing DBMSs needs to balance privacy and accuracy. Even though benchmarks should represent real user situations due to the different types of user workloads, the above methods struggle to keep databases gen- uine and protect user privacy at the same time. Other data synthesis methods, like privacy-preserving data publishing [4, 5, 18, 39, 41] and GAN-based tabular data generation [21], provide some ideas but often struggle to balance usefulness for benchmarking and privacy protection, typically favoring one over the other.', metadata={'source': '/content/document4.pdf'}), -0.21534686083492827), (Document(page_content='report the same runtime performance on (cid:98)𝐷.\\n\\nFor the first concern, one evaluation measure is KL divergence. Given a table 𝑇 ∈ 𝐷 and its counterpart (cid:98)𝑇 ∈ (cid:98)𝐷, the KL divergence of 𝑇 from (cid:98)𝑇 is defined as\\n\\n𝐾𝐿(𝑇 ∥ (cid:98)𝑇 ) = −\\n\\n∑︁\\n\\n𝑥 ∈𝑇\\n\\nPr[𝑥,𝑇 ] log\\n\\n(cid:32)\\n\\nPr[𝑥,𝑇 ] Pr[𝑥, (cid:98)𝑇 ]\\n\\n(cid:33)\\n\\nwhere Pr[𝑥,𝑇 ] denotes the probability of tuple 𝑥 in 𝑇 . Then, the KL divergence of 𝐷 from (cid:98)𝐷 is defined as the average KL divergence over all the tables of 𝐷. For the second concern, given a query workload, we can compare the execution times on 𝐷 and (cid:98)𝐷. Since the execution time may vary across DBMSs, we can also compare the cardinality, i.e., the number of tuples in the query result, which is often used in a query optimizer for estimating query performance [17]. In\\n\\n(1)\\n\\n(2)\\n\\n4\\n\\nAlgorithm 1: PrivBench : Database 𝐷 Input Output : Synthetic database (cid:98)𝐷 Param: Privacy budgets 𝜖𝑠\\n\\n𝑖 (for SPN construction) and 𝜖 𝑓\\n\\n(for fanout construction), ∀𝑖 ∈ [𝑛]\\n\\n𝑖\\n\\n1 for each private table 𝑇𝑖 of 𝐷 do 𝑡𝑖 ← PrivSPN(𝑇𝑖, 𝜖𝑠 𝑖 ); 2 3 for each pair of tables (𝑇𝑖,𝑇𝑗 ) where 𝑇𝑖 refers to 𝑇𝑗 do\\n\\n𝑖 ← PrivFanout(𝑇𝑖, 𝑡𝑖, 𝑃𝐾𝑗 , 𝜖 𝑓 𝑡 ′\\n\\n4 5 for each modified SPN 𝑡 ′\\n\\n𝑖 do\\n\\n𝑖 );\\n\\n6\\n\\n(cid:98)𝑇𝑖 ← SampleDataFromSPN(𝑡 ′\\n\\n𝑖 );\\n\\n7 return (cid:98)𝐷 = { (cid:98)𝑇1, . . . , (cid:98)𝑇𝑛 };\\n\\nparticular, Q-error [26] is a widely used measure for comparing cardinalities:\\n\\n𝑄 − 𝑒𝑟𝑟𝑜𝑟 (𝑞, 𝐷, (cid:98)𝐷) = max\\n\\n(cid:32)𝐶𝑎𝑟𝑑 (𝑞, 𝐷) 𝐶𝑎𝑟𝑑 (𝑞, (cid:98)𝐷)\\n\\n,\\n\\n𝐶𝑎𝑟𝑑 (𝑞, (cid:98)𝐷) 𝐶𝑎𝑟𝑑 (𝑞, 𝐷)\\n\\n(cid:33)\\n\\nwhere 𝑞 denotes a query in the workload, and 𝐶𝑎𝑟𝑑 (𝑞, 𝐷) and 𝐶𝑎𝑟𝑑 (𝑞, (cid:98)𝐷) denote the cardinalities of executing 𝑞 on 𝐷 and (cid:98)𝐷, respectively. Then, we compute the mean Q-error of the queries in the workload.\\n\\n4 PRIVBENCH: DATABASE SYNTHESIS WITH\\n\\nPRIVATE SPNS\\n\\nIn this section, we propose PrivBench, an SPN-based differentially private database synthesis method.', metadata={'source': '/content/document4.pdf'}), -0.21915288984285608)]\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/langchain_core/vectorstores.py:342: UserWarning: No relevant docs were retrieved using the relevance score threshold 0.1\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|begin_of_text|><|start_header_id|>system<|end_header_id|>  Use the following context to answer the\n",
            "question at the end. Do not use any other information. If you can't find the relevant information in\n",
            "the context, just say you don't have enough information to answer the question. Don't try to make up\n",
            "an answer.  <|eot_id|><|start_header_id|>user<|end_header_id|>  What are the risk factors of\n",
            "NVIDIA?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n"
          ]
        }
      ],
      "source": [
        "query = \"What are the risk factors of NVIDIA?\"\n",
        "result = Chain_web.invoke(query)\n",
        "print(fill(result['result'].strip(), width=100))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c26a230fe48b4e7bb62a9e20df4448d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b8851d1bd9484ed5a9e88942c26b86fe",
              "IPY_MODEL_7f98a8426ca5464fb7dab015fce3505f",
              "IPY_MODEL_23e7d12f3de7453c85ab468d6e986726"
            ],
            "layout": "IPY_MODEL_5722b55b64734b15bdfb872655a3024b"
          }
        },
        "b8851d1bd9484ed5a9e88942c26b86fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_47b5ecf26a164407836ecb81dcd82698",
            "placeholder": "​",
            "style": "IPY_MODEL_add5bbc6398142b3840ca97a65cc5b25",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "7f98a8426ca5464fb7dab015fce3505f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_532d6a79362a409fbd5becea59d8a8e6",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bd376012f67e41c790d405c8cd95f39b",
            "value": 4
          }
        },
        "23e7d12f3de7453c85ab468d6e986726": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_96e64dea42c241e7ac33aeac0cf50503",
            "placeholder": "​",
            "style": "IPY_MODEL_6d8429fad86c4df5932411a7a0baae13",
            "value": " 4/4 [01:09&lt;00:00, 15.02s/it]"
          }
        },
        "5722b55b64734b15bdfb872655a3024b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "47b5ecf26a164407836ecb81dcd82698": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "add5bbc6398142b3840ca97a65cc5b25": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "532d6a79362a409fbd5becea59d8a8e6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bd376012f67e41c790d405c8cd95f39b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "96e64dea42c241e7ac33aeac0cf50503": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6d8429fad86c4df5932411a7a0baae13": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}