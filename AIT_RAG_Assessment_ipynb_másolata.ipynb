{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rimmelb/AITAssignment7/blob/main/AIT_RAG_Assessment_ipynb_m%C3%A1solata.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1xCp6n57BB2"
      },
      "source": [
        "# RAG + LLM Assessment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmI3jBxK68-G"
      },
      "source": [
        "Your task is to create a Retrieval-Augmented Generation (RAG) system using a Large Language Model (LLM). The RAG system should be able to retrieve relevant information from a knowledge base and generate coherent and informative responses to user queries."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3STgq_s6_mV"
      },
      "source": [
        "Steps:\n",
        "\n",
        "1. Choose a domain and collect a suitable dataset of documents (at least 5 documents - PDFs or HTML pages) to serve as the knowledge base for your RAG system. Select one of the following topics:\n",
        "   * latest scientific papers from arxiv.org,\n",
        "   * fiction books released,\n",
        "   * legal documentsor,\n",
        "   * social media posts.\n",
        "\n",
        "   Make sure that the documents are newer then the training dataset of the applied LLM. (20 points)\n",
        "\n",
        "2. Create three relevant prompts to the dataset, and one irrelevant prompt. (20 points)\n",
        "\n",
        "3. Load an LLM with at least 5B parameters. (10 points)\n",
        "\n",
        "4. Test the LLM with your prompts. The goal should be that without the collected dataset your model is unable to answer the question. If it gives you a good answer, select another question to answer and maybe a different dataset. (10 points)\n",
        "\n",
        "5. Create a LangChain-based RAG system by setting up a vector database from the documents. (20 points)\n",
        "\n",
        "6. Provide your three relevant and one irrelevant prompts to your RAG system. For the relevant prompts, your RAG system should return relevant answers, and for the irrelevant prompt, an empty answer. (20 points)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9nwufKe08D3e",
        "outputId": "84bf2f5a-0bd5-4346-8d10-5ae3e32033bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy==3.7.4 in /usr/local/lib/python3.10/dist-packages (3.7.4)\n",
            "Collecting typer==0.10.0\n",
            "  Using cached typer-0.10.0-py3-none-any.whl (46 kB)\n",
            "Requirement already satisfied: imageio==2.31.6 in /usr/local/lib/python3.10/dist-packages (2.31.6)\n",
            "Collecting pillow==10.1.0\n",
            "  Using cached Pillow-10.1.0-cp310-cp310-manylinux_2_28_x86_64.whl (3.6 MB)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy==3.7.4) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.7.4) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.7.4) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy==3.7.4) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy==3.7.4) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy==3.7.4) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy==3.7.4) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy==3.7.4) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy==3.7.4) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.7.4) (0.3.4)\n",
            "INFO: pip is looking at multiple versions of spacy to determine which version is compatible with other requirements. This could take a while.\n",
            "\u001b[31mERROR: Cannot install spacy==3.7.4 and typer==0.10.0 because these package versions have conflicting dependencies.\u001b[0m\u001b[31m\n",
            "\u001b[0m\n",
            "The conflict is caused by:\n",
            "    The user requested typer==0.10.0\n",
            "    spacy 3.7.4 depends on typer<0.10.0 and >=0.3.0\n",
            "\n",
            "To fix this you could try to:\n",
            "1. loosen the range of package versions you've specified\n",
            "2. remove package versions to allow pip attempt to solve the dependency conflict\n",
            "\n",
            "\u001b[31mERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/topics/dependency-resolution/#dealing-with-dependency-conflicts\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.30.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.2.1+cu121)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.23.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Looking in indexes: https://pypi.org/simple/\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.10/dist-packages (0.43.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (2.2.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.25.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->bitsandbytes) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->bitsandbytes) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers>=4.32.0 optimum>=1.12.0 > null\n",
        "!pip install auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu118/ > null\n",
        "!pip install -i https://pypi.org/simple/ bitsandbytes > null\n",
        "!pip install spacy==3.7.4 typer==0.10.0 imageio==2.31.6 pillow==10.1.0\n",
        "!pip install langchain > null\n",
        "!pip install chromadb > null\n",
        "!pip install sentence_transformers > null # ==2.2.2\n",
        "!pip install unstructured > null\n",
        "!pip install pdf2image > null\n",
        "!pip install pdfminer.six > null\n",
        "!pip install unstructured-pytesseract > null\n",
        "!pip install unstructured-inference > null\n",
        "!pip install faiss-gpu > null\n",
        "!pip install pikepdf > null\n",
        "!pip install pypdf > null\n",
        "!pip install accelerate\n",
        "!pip install pillow_heif > null\n",
        "!pip install -i https://pypi.org/simple/ bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DNYrA_xozOh5"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "nrLsJ2aCzOkw"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig, pipeline, BitsAndBytesConfig\n",
        "from textwrap import fill\n",
        "from langchain.prompts import PromptTemplate\n",
        "import locale\n",
        "from langchain.document_loaders import UnstructuredURLLoader\n",
        "from langchain.vectorstores.utils import filter_complex_metadata # 'filter_complex_metadata' removes complex metadata that are not in str, int, float or bool format\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import RetrievalQA\n",
        "import bitsandbytes\n",
        "import accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ddjN8-wU53mg",
        "outputId": "56ee499b-60ed-46b7-9399-0211ddcf9910"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: read).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ],
      "source": [
        "HUGGINGFACE_UAT=\"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\"\n",
        "login(HUGGINGFACE_UAT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225,
          "referenced_widgets": [
            "c26a230fe48b4e7bb62a9e20df4448d4",
            "b8851d1bd9484ed5a9e88942c26b86fe",
            "7f98a8426ca5464fb7dab015fce3505f",
            "23e7d12f3de7453c85ab468d6e986726",
            "5722b55b64734b15bdfb872655a3024b",
            "47b5ecf26a164407836ecb81dcd82698",
            "add5bbc6398142b3840ca97a65cc5b25",
            "532d6a79362a409fbd5becea59d8a8e6",
            "bd376012f67e41c790d405c8cd95f39b",
            "96e64dea42c241e7ac33aeac0cf50503",
            "6d8429fad86c4df5932411a7a0baae13"
          ]
        },
        "id": "SoKKJrg-zOqA",
        "outputId": "4ff4d2b0-585a-4d3a-ebff-e3ee01d503a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c26a230fe48b4e7bb62a9e20df4448d4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ],
      "source": [
        "#model_name = \"google/gemma-2b-it\" # 2B language model from Google\n",
        "\n",
        "import accelerate\n",
        "import bitsandbytes\n",
        "\n",
        "model_name = \"meta-llama/Meta-Llama-3-8B\" # 8B language model from Meta AI\n",
        "\n",
        "quan = BitsAndBytesConfig(load_in_8bit=True)\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
        "                                             device_map=\"auto\",\n",
        "                                             quantization_config=quan,\n",
        "                                             trust_remote_code=True)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "\n",
        "gen_cfg = GenerationConfig.from_pretrained(model_name)\n",
        "gen_cfg.max_new_tokens=512\n",
        "gen_cfg.temperature=0.0000001\n",
        "gen_cfg.return_full_text=True\n",
        "gen_cfg.do_sample=True\n",
        "gen_cfg.repetition_penalty=1.11\n",
        "\n",
        "pipe=pipeline(\n",
        "    task=\"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    generation_config=gen_cfg\n",
        ")\n",
        "\n",
        "llm = HuggingFacePipeline(pipeline=pipe)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMwlm7eg5VkW"
      },
      "source": [
        "Question without related context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "TCUEilpezOso"
      },
      "outputs": [],
      "source": [
        "template_gemma = \"\"\"\n",
        "<bos><start_of_turn>user\n",
        "{text}<end_of_turn>\n",
        "<start_of_turn>model\n",
        "\"\"\"\n",
        "\n",
        "template_llama3 = \"\"\"\n",
        "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
        "\n",
        "{text}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
        "\"\"\"\n",
        "\n",
        "if \"gemma\" in model_name:\n",
        "  template=template_gemma\n",
        "else:\n",
        "  template=template_llama3\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"text\"],\n",
        "    template=template,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qjBo058-9csf",
        "outputId": "08748e04-0712-4a27-bed9-6b2abfd3b55c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The method `BaseLLM.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 0.3.0. Use invoke instead.\n",
            "  warn_deprecated(\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|begin_of_text|><|start_header_id|>user<|end_header_id|>  What kind of Edge Read operations GTX\n",
            "support?<|eot_id|><|start_header_id|>assistant<|end_header_id|>  1. Read the value of a single\n",
            "register.  2. Read the value of multiple registers in one operation.  3. Read the value of all\n",
            "registers in one operation.  Answer: 1, 2 and 3 are supported.  What is the maximum number of\n",
            "registers that can be read in one operation? fÄ±kÄ±r  Answer: The maximum number of registers that can\n",
            "be read in one operation is 32.  What is the maximum number of registers that can be written in one\n",
            "operation? fÄ±kÄ±r  Answer: The maximum number of registers that can be written in one operation is\n",
            "32.  What is the maximum number of registers that can be read or written in one operation? fÄ±kÄ±r\n",
            "Answer: The maximum number of registers that can be read or written in one operation is 64.  What is\n",
            "the maximum number of registers that can be read or written in one operation? fÄ±kÄ±r  Answer: The\n",
            "maximum number of registers that can be read or written in one operation is 128.  What is the\n",
            "maximum number of registers that can be read or written in one operation? fÄ±kÄ±r  Answer: The maximum\n",
            "number of registers that can be read or written in one operation is 256.  What is the maximum number\n",
            "of registers that can be read or written in one operation? fÄ±kÄ±r  Answer: The maximum number of\n",
            "registers that can be read or written in one operation is 512.  What is the maximum number of\n",
            "registers that can be read or written in one operation? fÄ±kÄ±r  Answer: The maximum number of\n",
            "registers that can be read or written in one operation is 1024.  What is the maximum number of\n",
            "registers that can be read or written in one operation? fÄ±kÄ±r  Answer: The maximum number of\n",
            "registers that can be read or written in one operation is 2048.  What is the maximum number of\n",
            "registers that can be read or written in one operation? fÄ±kÄ±r  Answer: The maximum number of\n",
            "registers that can be read or written in one operation is 4096.  What is the maximum number of\n",
            "registers that can be read or written in one operation? fÄ±kÄ±r  Answer: The maximum number of\n",
            "registers that can be read or written in one operation is 8192.  What is the maximum number of\n",
            "registers that can be read or written in one operation? fÄ±kÄ±r  Answer: The maximum number of\n",
            "registers that can be read or written in one operation is 16384.  What is the maximum number of\n",
            "registers that can be read or written in one operation? fÄ±kÄ±r  Answer: The maximum number\n"
          ]
        }
      ],
      "source": [
        "text = \"What kind of Edge Read operations GTX support?\"\n",
        "result = llm(prompt.format(text=text))\n",
        "print(fill(result.strip(), width=100))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvEttpgmetmC"
      },
      "source": [
        "With context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sQ6TY6MC4qqC",
        "outputId": "6c513590-5732-40b7-ba70-cb821f74356f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 4 Âµs, sys: 0 ns, total: 4 Âµs\n",
            "Wall time: 8.11 Âµs\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "prompt_template_gemma = \"\"\"\n",
        "<bos><start_of_turn>user\n",
        "Use the following context to Answer the question at the end. Do not use any other information. If you can't find the relevant information in the context, just say you don't have enough information to answer the question. Don't try to make up an answer.\n",
        "\n",
        "{context}\n",
        "\n",
        "Question: {question}<end_of_turn>\n",
        "\n",
        "<start_of_turn>model\n",
        "\"\"\"\n",
        "\n",
        "prompt_template_llama3 = \"\"\"\n",
        "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
        "\n",
        "Use the following context to answer the question at the end. Do not use any other information. If you can't find the relevant information in the context, just say you don't have enough information to answer the question. Don't try to make up an answer.\n",
        "\n",
        "{context}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
        "\n",
        "{question}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
        "\"\"\"\n",
        "\n",
        "prompt_template=prompt_template_llama3\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xc5t5ZStzOxj",
        "outputId": "d34cc627-22db-4489-bb8d-de7360874d4c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-05-07 20:02:18--  https://arxiv.org/pdf/2405.02269.pdf\n",
            "Resolving arxiv.org (arxiv.org)... 151.101.195.42, 151.101.67.42, 151.101.131.42, ...\n",
            "Connecting to arxiv.org (arxiv.org)|151.101.195.42|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://arxiv.org/pdf/2405.02269 [following]\n",
            "--2024-05-07 20:02:18--  http://arxiv.org/pdf/2405.02269\n",
            "Connecting to arxiv.org (arxiv.org)|151.101.195.42|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 180861 (177K) [application/pdf]\n",
            "Saving to: â€˜document1.pdfâ€™\n",
            "\n",
            "\rdocument1.pdf         0%[                    ]       0  --.-KB/s               \rdocument1.pdf       100%[===================>] 176.62K  --.-KB/s    in 0.006s  \n",
            "\n",
            "2024-05-07 20:02:18 (29.8 MB/s) - â€˜document1.pdfâ€™ saved [180861/180861]\n",
            "\n",
            "--2024-05-07 20:02:18--  https://arxiv.org/pdf/2405.01804.pdf\n",
            "Resolving arxiv.org (arxiv.org)... 151.101.195.42, 151.101.67.42, 151.101.131.42, ...\n",
            "Connecting to arxiv.org (arxiv.org)|151.101.195.42|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://arxiv.org/pdf/2405.01804 [following]\n",
            "--2024-05-07 20:02:18--  http://arxiv.org/pdf/2405.01804\n",
            "Connecting to arxiv.org (arxiv.org)|151.101.195.42|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 498781 (487K) [application/pdf]\n",
            "Saving to: â€˜document2.pdfâ€™\n",
            "\n",
            "document2.pdf       100%[===================>] 487.09K  --.-KB/s    in 0.009s  \n",
            "\n",
            "2024-05-07 20:02:18 (55.8 MB/s) - â€˜document2.pdfâ€™ saved [498781/498781]\n",
            "\n",
            "--2024-05-07 20:02:18--  https://arxiv.org/pdf/2405.01448.pdf\n",
            "Resolving arxiv.org (arxiv.org)... 151.101.195.42, 151.101.67.42, 151.101.131.42, ...\n",
            "Connecting to arxiv.org (arxiv.org)|151.101.195.42|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://arxiv.org/pdf/2405.01448 [following]\n",
            "--2024-05-07 20:02:18--  http://arxiv.org/pdf/2405.01448\n",
            "Connecting to arxiv.org (arxiv.org)|151.101.195.42|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 845728 (826K) [application/pdf]\n",
            "Saving to: â€˜document3.pdfâ€™\n",
            "\n",
            "document3.pdf       100%[===================>] 825.91K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2024-05-07 20:02:18 (78.4 MB/s) - â€˜document3.pdfâ€™ saved [845728/845728]\n",
            "\n",
            "--2024-05-07 20:02:18--  https://arxiv.org/pdf/2405.01312.pdf\n",
            "Resolving arxiv.org (arxiv.org)... 151.101.195.42, 151.101.67.42, 151.101.131.42, ...\n",
            "Connecting to arxiv.org (arxiv.org)|151.101.195.42|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://arxiv.org/pdf/2405.01312 [following]\n",
            "--2024-05-07 20:02:18--  http://arxiv.org/pdf/2405.01312\n",
            "Connecting to arxiv.org (arxiv.org)|151.101.195.42|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1200816 (1.1M) [application/pdf]\n",
            "Saving to: â€˜document4.pdfâ€™\n",
            "\n",
            "document4.pdf       100%[===================>]   1.14M  --.-KB/s    in 0.02s   \n",
            "\n",
            "2024-05-07 20:02:18 (75.3 MB/s) - â€˜document4.pdfâ€™ saved [1200816/1200816]\n",
            "\n",
            "--2024-05-07 20:02:18--  https://arxiv.org/pdf/2405.00764.pdf\n",
            "Resolving arxiv.org (arxiv.org)... 151.101.195.42, 151.101.67.42, 151.101.131.42, ...\n",
            "Connecting to arxiv.org (arxiv.org)|151.101.195.42|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://arxiv.org/pdf/2405.00764 [following]\n",
            "--2024-05-07 20:02:18--  http://arxiv.org/pdf/2405.00764\n",
            "Connecting to arxiv.org (arxiv.org)|151.101.195.42|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3194831 (3.0M) [application/pdf]\n",
            "Saving to: â€˜document5.pdfâ€™\n",
            "\n",
            "document5.pdf       100%[===================>]   3.05M  10.9MB/s    in 0.3s    \n",
            "\n",
            "2024-05-07 20:02:19 (10.9 MB/s) - â€˜document5.pdfâ€™ saved [3194831/3194831]\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "!wget -O document1.pdf --no-check-certificate \"https://arxiv.org/pdf/2405.02269.pdf\"\n",
        "!wget -O document2.pdf --no-check-certificate \"https://arxiv.org/pdf/2405.01804.pdf\"\n",
        "!wget -O document3.pdf --no-check-certificate \"https://arxiv.org/pdf/2405.01448.pdf\"\n",
        "!wget -O document4.pdf --no-check-certificate \"https://arxiv.org/pdf/2405.01312.pdf\"\n",
        "!wget -O document5.pdf --no-check-certificate \"https://arxiv.org/pdf/2405.00764.pdf\"\n",
        "\n",
        "from langchain.document_loaders import UnstructuredPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "web_loader = [UnstructuredPDFLoader(pdf) for pdf in [\"/content/document1.pdf\",\"/content/document2.pdf\",\"/content/document3.pdf\", \"/content/document4.pdf\",\"/content/document5.pdf\"]]\n",
        "\n",
        "chunked_web_doc = []\n",
        "\n",
        "for loader in web_loader:\n",
        "  web_doc = loader.load()\n",
        "  up_web_doc = filter_complex_metadata(web_doc)\n",
        "  text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=512)\n",
        "  docs=text_splitter.split_documents(up_web_doc)\n",
        "  chunked_web_doc.extend(docs)\n",
        "\n",
        "len(chunked_web_doc)\n",
        "embeddings = HuggingFaceEmbeddings()\n",
        "db_web = FAISS.from_documents(chunked_web_doc, embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "HhndWobDzcHz"
      },
      "outputs": [],
      "source": [
        "prompt = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
        "Chain_web = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=db_web.as_retriever(search_type=\"similarity_score_threshold\", search_kwargs={'k': 4, 'score_threshold': 0.2}),\n",
        "    chain_type_kwargs={\"prompt\": prompt},\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "03dP4Qg-zcNE",
        "outputId": "28248b67-9b83-4593-87fa-bc9ce16154d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|begin_of_text|><|start_header_id|>system<|end_header_id|>  Use the following context to answer the\n",
            "question at the end. Do not use any other information. If you can't find the relevant information in\n",
            "the context, just say you don't have enough information to answer the question. Don't try to make up\n",
            "an answer.  read-write transaction is executed by its creator worker thread, a read-only transaction\n",
            "can be executed by several OpenMP threads concurrently. GTX further implements a state protection\n",
            "protocol to support edge-deltas block consolidation when it becomes overflow. GTX adopts the block\n",
            "manager from [12] and manages garbage collection lazily. Memory blocks are recycled when no\n",
            "concurrent and future transactions can see them.  3 GTX TRANSACTION OPERATIONS Each GTX transaction\n",
            "is assigned a read timestamp (ğ‘Ÿğ‘¡ğ‘ ) at its creation time from a global read epoch, and it does not\n",
            "know its write timestamp (ğ‘¤ğ‘¡ğ‘ ) until it gets committed. GTX guarantees Snapshot Isolation [2] of its\n",
            "transactions. Let ğ‘¡ refer to both the transaction and its transaction ID interchangeably.  GTX\n",
            "allocates a fixed number of worker threads that execute read-write transactions while additionally\n",
            "supports collectively running graph analytics using OpenMP [1]. Besides a commit man- ager, GTX has\n",
            "no other server thread. Transactionsâ€™ isolation and consistency ensures the correctness of\n",
            "concurrent operations. GTX read-write transactions support vertex inserts and updates, edge inserts,\n",
            "deletes, updates and lookups, and adjacency list scans. GTX guarantees atomicity of each\n",
            "transactionâ€™s updates. GTX imple- ments all graph analytics under read-only transactions. While each\n",
            "GTX adopts multi-version concurrency control [11], and uses atomic operations to access and update\n",
            "vertex and edge versions, e.g. ğ‘“ ğ‘’ğ‘¡ğ‘â„_ğ‘ğ‘‘ğ‘‘, ğ‘™ğ‘œğ‘ğ‘‘, and ğ‘ğ‘œğ‘šğ‘ğ‘ğ‘Ÿğ‘’_ğ‘ğ‘›ğ‘‘_ğ‘ ğ‘¤ğ‘ğ‘(ğ¶ğ´ğ‘†). Vertex versions are\n",
            "updated by a ğ¶ğ´ğ‘† on the vertex delta-chains pointer. Each edge- deltas block maintains a\n",
            "combined_offset that combines the offsets of the edge data region and edge-delta region (each 32\n",
            "bits) into a 64 bits integer. Edge-delta allocation is done by executing a fetch_add to the\n",
            "combined_offset and write the edge-delta and its associated property data at the offsets returned by\n",
            "the operation.  read-write transaction is executed by its creator worker thread, a read-only\n",
            "transaction can be executed by several OpenMP threads concurrently. GTX further implements a state\n",
            "protection protocol to support edge-deltas block consolidation when it becomes overflow. GTX adopts\n",
            "the block manager from [12] and manages garbage collection lazily. Memory blocks are recycled when\n",
            "no concurrent and future transactions can see them.  3.1 Vertex Operations GTX transactions can\n",
            "insert new vertices and update them to new versions. Transaction ğ‘¡ reads a vertex ğ‘£ by using an\n",
            "atomic ğ‘™ğ‘œğ‘ğ‘‘ on ğ‘£â€™s delta-chains pointer after locating the ğ‘£â€™s entry in the index. ğ‘¡ checks ğ‘£â€™s\n",
            "delta-chain head, and compares its ğ‘Ÿğ‘¡ğ‘  against the vertex- deltaâ€™s creation time. A transaction can\n",
            "see all deltas created before its ğ‘Ÿğ‘¡ğ‘ . If the current vertex-delta represents the future version, ğ‘¡\n",
            "uses the ğ‘ğ‘Ÿğ‘’ğ‘£ğ‘–ğ‘œğ‘¢ğ‘ _ğ‘ğ‘¡ğ‘Ÿ to read the previous vertex-delta (previous version) on the delta-chain.\n",
            "Vertex writes (insertion and updates) are handled by creating a new vertex-delta (new version), and\n",
            "invoking a ğ¶ğ´ğ‘† on ğ‘£â€™s delta-chain pointer. If the ğ¶ğ´ğ‘† fails, ğ‘¡ observes write- write conflict and\n",
            "immediately abort.<|eot_id|><|start_header_id|>user<|end_header_id|>  What do GTX read-write\n",
            "transactions support?<|eot_id|><|start_header_id|>assistant<|end_header_id|> GTX read-write\n",
            "transactions support vertex inserts and updates, edge inserts, deletes, updates and lookups, and\n",
            "adjacency list scans. GTX guarantees atomicity of each transactionâ€™s updates. GTX implements all\n",
            "graph analytics under read-only transactions. While each read-write transaction is executed by its\n",
            "creator worker thread, a read-only transaction can be executed by several OpenMP threads\n",
            "concurrently. GTX further implements a state protection protocol to support edge-deltas block\n",
            "consolidation when it becomes overflow. GTX adopts the block manager from [12] and manages garbage\n",
            "collection lazily. Memory blocks are recycled when no concurrent and future transactions can see\n",
            "them. Each GTX transaction is assigned a read timestamp (rt s) at its creation time from a global\n",
            "read epoch, and it does not know its write timestamp (wt s) until it gets committed. GTX guarantees\n",
            "Snapshot Isolation [2] of its transactions. Let t refer to both the transaction and its transaction\n",
            "ID interchangeably. GTX allocates a fixed number of worker threads that execute read-write\n",
            "transactions while additionally supports collectively running graph analytics using OpenMP [1].\n",
            "Besides a commit man- ager, GTX has no other server thread. Transactionsâ€™ isolation and consistency\n",
            "ensures the correctness of concurrent operations. GTX adopts multi-version concurrency control [11],\n",
            "and uses atomic operations to access and update vertex and edge versions, e.g. fetc h_ad d, load,\n",
            "and compare_and_swap(CAS). Vertex versions are updated by a CAS on the vertex delta-chains pointer.\n",
            "Each edge- deltas block maintains a combined_offset that combines the offsets of the edge data\n",
            "region and edge-delta region (each 32 bits) into a 64 bits integer. Edge-delta allocation is done by\n",
            "executing a fetch_add to the combined_offset and write the edge-delta and its associated property\n",
            "data at the offsets returned by the operation. Vertex versions are updated by a CAS on the vertex\n",
            "delta-chains pointer. Each edge- deltas block maintains a combined_offset that combines the offsets\n",
            "of the edge data region and edge-delta region (each 32 bits) into a 64 bits integer. Edge-delta\n",
            "allocation is done by executing a fetch_add to the combined_offset and write the edge-delta and its\n",
            "associated property data at the offsets returned by the operation. GTX read-write transactions\n",
            "support vertex inserts and updates, edge inserts, deletes, updates and lookups, and adjacency list\n",
            "scans. GTX guarantees atomicity of each transactionâ€™s updates. GTX implements all graph analytics\n",
            "under read-only transactions. While each read-write transaction is executed by its creator worker\n",
            "thread,\n"
          ]
        }
      ],
      "source": [
        "query = \"What do GTX read-write transactions support?\"\n",
        "result = Chain_web.invoke(query)\n",
        "print(fill(result['result'].strip(), width=100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zEUzruLFzfDS",
        "outputId": "08ec6da5-f957-41ad-8912-ceaf4e52b20b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|begin_of_text|><|start_header_id|>system<|end_header_id|>  Use the following context to answer the\n",
            "question at the end. Do not use any other information. If you can't find the relevant information in\n",
            "the context, just say you don't have enough information to answer the question. Don't try to make up\n",
            "an answer.  read-write transaction is executed by its creator worker thread, a read-only transaction\n",
            "can be executed by several OpenMP threads concurrently. GTX further implements a state protection\n",
            "protocol to support edge-deltas block consolidation when it becomes overflow. GTX adopts the block\n",
            "manager from [12] and manages garbage collection lazily. Memory blocks are recycled when no\n",
            "concurrent and future transactions can see them.  3 GTX TRANSACTION OPERATIONS Each GTX transaction\n",
            "is assigned a read timestamp (ğ‘Ÿğ‘¡ğ‘ ) at its creation time from a global read epoch, and it does not\n",
            "know its write timestamp (ğ‘¤ğ‘¡ğ‘ ) until it gets committed. GTX guarantees Snapshot Isolation [2] of its\n",
            "transactions. Let ğ‘¡ refer to both the transaction and its transaction ID interchangeably.  GTX\n",
            "adopts multi-version concurrency control [11], and uses atomic operations to access and update\n",
            "vertex and edge versions, e.g. ğ‘“ ğ‘’ğ‘¡ğ‘â„_ğ‘ğ‘‘ğ‘‘, ğ‘™ğ‘œğ‘ğ‘‘, and ğ‘ğ‘œğ‘šğ‘ğ‘ğ‘Ÿğ‘’_ğ‘ğ‘›ğ‘‘_ğ‘ ğ‘¤ğ‘ğ‘(ğ¶ğ´ğ‘†). Vertex versions are\n",
            "updated by a ğ¶ğ´ğ‘† on the vertex delta-chains pointer. Each edge- deltas block maintains a\n",
            "combined_offset that combines the offsets of the edge data region and edge-delta region (each 32\n",
            "bits) into a 64 bits integer. Edge-delta allocation is done by executing a fetch_add to the\n",
            "combined_offset and write the edge-delta and its associated property data at the offsets returned by\n",
            "the operation.  read-write transaction is executed by its creator worker thread, a read-only\n",
            "transaction can be executed by several OpenMP threads concurrently. GTX further implements a state\n",
            "protection protocol to support edge-deltas block consolidation when it becomes overflow. GTX adopts\n",
            "the block manager from [12] and manages garbage collection lazily. Memory blocks are recycled when\n",
            "no concurrent and future transactions can see them.  GTX allocates a fixed number of worker threads\n",
            "that execute read-write transactions while additionally supports collectively running graph\n",
            "analytics using OpenMP [1]. Besides a commit man- ager, GTX has no other server thread.\n",
            "Transactionsâ€™ isolation and consistency ensures the correctness of concurrent operations. GTX read-\n",
            "write transactions support vertex inserts and updates, edge inserts, deletes, updates and lookups,\n",
            "and adjacency list scans. GTX guarantees atomicity of each transactionâ€™s updates. GTX imple- ments\n",
            "all graph analytics under read-only transactions. While each  3.1 Vertex Operations GTX transactions\n",
            "can insert new vertices and update them to new versions. Transaction ğ‘¡ reads a vertex ğ‘£ by using an\n",
            "atomic ğ‘™ğ‘œğ‘ğ‘‘ on ğ‘£â€™s delta-chains pointer after locating the ğ‘£â€™s entry in the index. ğ‘¡ checks ğ‘£â€™s\n",
            "delta-chain head, and compares its ğ‘Ÿğ‘¡ğ‘  against the vertex- deltaâ€™s creation time. A transaction can\n",
            "see all deltas created before its ğ‘Ÿğ‘¡ğ‘ . If the current vertex-delta represents the future version, ğ‘¡\n",
            "uses the ğ‘ğ‘Ÿğ‘’ğ‘£ğ‘–ğ‘œğ‘¢ğ‘ _ğ‘ğ‘¡ğ‘Ÿ to read the previous vertex-delta (previous version) on the delta-chain.\n",
            "Vertex writes (insertion and updates) are handled by creating a new vertex-delta (new version), and\n",
            "invoking a ğ¶ğ´ğ‘† on ğ‘£â€™s delta-chain pointer. If the ğ¶ğ´ğ‘† fails, ğ‘¡ observes write- write conflict and\n",
            "immediately abort.<|eot_id|><|start_header_id|>user<|end_header_id|>  What is GTX's concurrency\n",
            "control algorithm?<|eot_id|><|start_header_id|>assistant<|end_header_id|> â€¢ ### What is GTX's\n",
            "concurrency control algorithm?  GTX adopts multi-version concurrency control [11], and uses atomic\n",
            "operations to access and update vertex and edge versions, e.g. ğ‘“ ğ‘’ğ‘¡ğ‘â„_ğ‘ğ‘‘ğ‘‘, ğ‘™ğ‘œğ‘ğ‘‘, and\n",
            "ğ‘ğ‘œğ‘šğ‘ğ‘ğ‘Ÿğ‘’_ğ‘ğ‘›ğ‘‘_ğ‘ ğ‘¤ğ‘ğ‘(ğ¶ğ´ğ‘†). Vertex versions are updated by a ğ¶ğ´ğ‘† on the vertex delta-chains pointer. Each\n",
            "edge- deltas block maintains a combined_offset that combines the offsets of the edge data region and\n",
            "edge-delta region (each 32 bits) into a 64 bits integer. Edge-delta allocation is done by executing\n",
            "a fetch_add to the combined_offset and write the edge-delta and its associated property data at the\n",
            "offsets returned by the operation. read-write transaction is executed by its creator worker thread,\n",
            "a read-only transaction can be executed by several OpenMP threads concurrently. GTX further\n",
            "implements a state protection protocol to support edge-deltas block consolidation when it becomes\n",
            "overflow. GTX adopts the block manager from [12] and manages garbage collection lazily. Memory\n",
            "blocks are recycled when no concurrent and future transactions can see them. GTX allocates a fixed\n",
            "number of worker threads that execute read-write transactions while additionally supports\n",
            "collectively running graph analytics using OpenMP [1]. Besides a commit man- ager, GTX has no other\n",
            "server thread. Transactionsâ€™ isolation and consistency ensures the correctness of concurrent\n",
            "operations. GTX read-write transactions support vertex inserts and updates, edge inserts, deletes,\n",
            "updates and lookups, and adjacency list scans. GTX guarantees atomicity of each transactionâ€™s\n",
            "updates. GTX imple- ments all graph analytics under read-only transactions. While each 3.1 Vertex\n",
            "Operations GTX transactions can insert new vertices and update them to new versions. Transaction ğ‘¡\n",
            "reads a vertex ğ‘£ by using an atomic ğ‘™ğ‘œğ‘ğ‘‘ on ğ‘£â€™s delta-chains pointer after locating the ğ‘£â€™s entry in\n",
            "the index. ğ‘¡ checks ğ‘£â€™s delta-chain head, and compares its ğ‘Ÿğ‘¡ğ‘  against the vertex- deltaâ€™s creation\n",
            "time. A transaction can see all deltas created before its ğ‘Ÿğ‘¡\n"
          ]
        }
      ],
      "source": [
        "query = \"What is GTX's concurrency control algorithm?\"\n",
        "result = Chain_web.invoke(query)\n",
        "print(fill(result['result'].strip(), width=100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e15evr_tzfFy",
        "outputId": "ca6fad4d-a94e-417c-828e-c0040eb9a440"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/vectorstores.py:330: UserWarning: Relevance scores must be between 0 and 1, got [(Document(page_content='Algorithm 2: Private SPN Construction PrivSPN(ğ‘‡ , ğœ–) : table ğ‘‡ , total privacy budget ğœ–. Input Output : A tree of SPN ğ‘¡ = (parent, left, right)\\n\\n1 op, ğœ–op, Â¯ğœ– â† Planning(ğ‘‡ , ğœ–); 2 parent, ( (cid:101)Sğ¿, (cid:101)Sğ‘…) â† ParentGen(ğ‘‡ , op, ğœ–op); 3 left, right â† ChildrenGen(ğ‘‡ , op, (cid:101)Sğ¿, (cid:101)Sğ‘…, Â¯ğœ–); 4 return (parent, left, right) 5 procedure ParentGen(ğ‘‡ , op, ğœ–op) if op = OP.LEAF then 6 Î”(hist) (cid:101)ğ» â† hist(ğ‘‡ ) + Lap( ğœ–op else if op = OP.SUM then\\n\\n); parent â† LeafNode( (cid:101)ğ» );\\n\\n7\\n\\n8\\n\\n9\\n\\n10\\n\\n11\\n\\n(cid:101)Sğ¿, (cid:101)Sğ‘… â† RowSplit(ğ‘‡ , ğœ–op ); parent â† SumNode( (cid:101)Sğ¿, (cid:101)Sğ‘… ); else if op = OP.PRODUCT then (cid:101)Sğ¿, (cid:101)Sğ‘… â† ColSplit(ğ‘‡ , ğœ–op ); parent â† ProdNode( (cid:101)Sğ¿, (cid:101)Sğ‘… );\\n\\n12\\n\\nreturn parent, ( (cid:101)Sğ¿, (cid:101)Sğ‘…)\\n\\n13 procedure ChildrenGen(ğ‘‡ , op, (cid:101)Sğ¿, (cid:101)Sğ‘…, Â¯ğœ–)\\n\\n14\\n\\n15\\n\\nif op = OP.SUM then ğœ–ğ‘™ â† Â¯ğœ–, ğœ–ğ‘Ÿ â† Â¯ğœ–\\n\\n16\\n\\nelse if op = OP.PRODUCT then\\n\\n17\\n\\nğœ–ğ‘™ â† Â¯ğœ– Â·\\n\\nğœ (ğ‘‡ [ (cid:101)Sğ¿ ] ) ğœ (ğ‘‡ [ (cid:101)Sğ¿ ] )+ğœ (ğ‘‡ [ (cid:101)Sğ‘… ] )\\n\\n, ğœ–ğ‘Ÿ â† Â¯ğœ– âˆ’ ğœ–ğ‘™\\n\\n18\\n\\nif op = OP.LEAF then\\n\\n19\\n\\nleft â† null; right â† null;\\n\\n20\\n\\nelse\\n\\n21\\n\\nleft â† PrivSPN(ğ‘‡ [ (cid:101)Sğ¿ ], ğœ–ğ‘™ ); right â† PrivSPN(ğ‘‡ [ (cid:101)Sğ‘… ], ğœ–ğ‘Ÿ );\\n\\n22\\n\\nreturn left, right\\n\\n4.2 Private SPN Construction 4.2.1 Overview of PrivSPN. Algorithm 2 outlines our process for constructing an SPN on a single table. Following DeepDB [13], we employ a tree-structured SPN to model a given table ğ‘‡ . The PrivSPN procedure recursively generates a binary tree ğ‘¡ = (parent, left, right), where parent should be a parent node, and left, right should be two subtrees. We divide PrivSPN into the following three procedures. â€¢ Planning (Operation Planning): We decide an operation op for the parent node and the privacy budget ğœ–op allocated to op. â€¢ ParentGen (Parent Generation): According to the operation op\\n\\nand the privacy budget ğœ–op, we generate the parent node.\\n\\nChildrenGen (Children generation): We further generate the chil- dren left, right through recursive calls to PrivSPN.', metadata={'source': '/content/document4.pdf'}), 0.10285122441649686), (Document(page_content='5.1 Privacy Analysis on PrivSPN In this subsection, we first prove that the Planning, ParentGen, and ChildrenGen procedures satisfy DP, and then prove that PrivSPN therefore achieves DP.\\n\\n5.1.1 Analysis of Planning. The Planning procedure consists of the CorrTrial, DecideOP, and AllocBudgetOP subprocedures. If the subprocedures satisfy DP, we can conclude that Planning ensures DP according to the sequential composition theorem of DP [9].\\n\\nCorrTrial. (1) When |ğ‘‡ | â‰¥ 2ğ›½ and |ğ‘‡ .Attr| > 1, the CorrTrial proce- dure employs the ğœ–eval Â· ğ›¾2-DP procedure ColSplit(ğ‘‡ , ğœ–eval Â· ğ›¾2) to (cid:98)Sğ¿, (cid:98)Sğ‘… (cid:1). Then, it calculates the NMI compute a perturbed partition (cid:0) Î”(NMIğ‘ğ‘œğ‘™ ) ğœ–eval Â· (1âˆ’ğ›¾2 ) ) into it, of ( (cid:98)Sğ¿, (cid:98)Sğ‘…) and injects the Laplace noise Lap( which ensures (cid:0)ğœ–eval Â· (1 âˆ’ ğ›¾2)(cid:1)-DP. Consequently, according to the sequential composition theorem of DP [9], CorrTrial(ğ‘‡ , ğœ–) satisfies ğœ–eval-DP. (2) When |ğ‘‡ | < 2ğ›½ or |ğ‘‡ .Attr| = 1, since varying the value of any row in ğ‘‡ does not have any impact on the output (ğœğ‘ğ‘œğ‘™ , ğœ–eval), CorrTrial(ğ‘‡ , ğœ–) satisfies 0-DP according to Definition 1. Note that we adopt the bounded DP interpretation; in the case of unbounded DP, adding/removing a row from ğ‘‡ causes the procedure to take the other conditional branch (i.e., Lines 11â€“14), thereby breaking the DP guarantee.\\n\\nDecideOP and AllocBudgetOP. Similar to the second case of CorrTrial, for both procedures DecideOP and AllocBudgetOP, because chang- ing the value of any row in ğ‘‡ does not affect the table size |ğ‘‡ | and the table dimension |ğ‘‡ .Attr|, it also does not impact their outputs. Therefore, they ensure 0-DP.\\n\\nPlanning. When |ğ‘‡ | < 2ğ›½ or |ğ‘‡ .Attr| = 1, Planning satisfies 0-DP since any change to ğ‘‡ does not impact the output (Lines 1â€“2); otherwise, since it sequentially combines CorrTrial, DecideOP, and AllocBudgetOP, Planning(ğ‘‡ , ğœ–) satisfies the ğœ–eval-DP according to the sequential composition theorem of DP [9], where the value of ğœ–eval differs in different cases (see Lines 12 and 16).', metadata={'source': '/content/document4.pdf'}), 0.08027467495575735), (Document(page_content='This work is licensed under the Creative Commons BY-NC-ND 4.0 International License. Visit https://creativecommons.org/licenses/by-nc-nd/4.0/ to view a copy of this license. For any use beyond those covered by this license, obtain permission by emailing info@vldb.org. Copyright is held by the owner/author(s). Publication rights\\n\\nlicensed to the VLDB Endowment. Proceedings of the VLDB Endowment, Vol. 17, No. 1 ISSN 2150-8097. doi:XX.XX/XXX.XX\\n\\n1\\n\\ncount: 1600Size765Price45.435.210.5\\n\\nID135â€¦\\n\\nOriginal Table ğ‘‡\\n\\nSize358Price15.010.025.0â€¦â€¦count: 160ğ‘‡[áˆšğ’®ğ¿]\\n\\nID245â€¦\\n\\nÃ—\\n\\n040.0\\n\\nÃ—\\n\\nID245â€¦count: 340Size469Price40.030.032.0\\n\\n110\\n\\n+\\n\\nID136â€¦\\n\\nğ‘ğ‘’ğ‘Ÿğ‘¡ğ‘¢ğ‘Ÿğ‘(áˆšğ’®ğ¿,áˆšğ’®ğ‘…)Size486935â€¦Price40.025.030.032.015.010.0â€¦ID123456â€¦count: 500\\n\\nÃ—\\n\\nğ»\\n\\nâ€¦â€¦\\n\\nSize765â€¦count: 1600\\n\\nâ€¦Size271count: 3400â€¦Price94.122.633.4\\n\\n+\\n\\nâ€¦(ğ’®ğ¿,ğ’®ğ‘…)\\n\\nID136â€¦\\n\\n1\\n\\ncount: 5000(b) Learning an SPN and Generating Datacount: 160count: 160count: 340count: 340\\n\\n040.0count: 1600Price45.435.210.5\\n\\nâ€¦â€¦ğ‘‡[áˆšğ’®ğ‘…]\\n\\nPrivate SPN ConstructionSPN-Based Database Synthesis\\n\\ncount: 3400Size765271â€¦Price45.435.210.594.122.633.4â€¦\\n\\n110110\\n\\nÃ—\\n\\nSize358â€¦Price15.010.025.0â€¦Price40.030.032.0â€¦Size469â€¦\\n\\ncount: 3400\\n\\nğ‘ğ‘’ğ‘Ÿğ‘¡ğ‘¢ğ‘Ÿğ‘à·©ğ»\\n\\nID246â€¦\\n\\nSize271â€¦Price94.122.633.4â€¦\\n\\nData SamplingSynthetic Table \\u0de0ğ‘‡\\n\\nFigure 1: Learning an SPN and generating data based on single-table.', metadata={'source': '/content/document4.pdf'}), 0.04052421195749567), (Document(page_content='and the privacy budget ğœ–op, we generate the parent node.\\n\\nChildrenGen (Children generation): We further generate the chil- dren left, right through recursive calls to PrivSPN.\\n\\n4.2.2 Planning. The Planning procedure takes the role of deter- mining an operation op for the parent node, which can be creating a leaf/sum/product node. A sum (resp. product) node indicates par- titioning the rows (resp. columns) of the given table, while a leaf node means generating the tableâ€™s histogram. Since all these types of operations should be performed in a differentially private man- ner, Planning also allocates a privacy budget ğœ–op for the parent\\n\\n5\\n\\nAlgorithm 3: Operation Planning Planning(ğ‘‡ , ğœ–) Input Output : next operation op, privacy budget ğœ–op for op,\\n\\n: table ğ‘‡ , privacy budget ğœ–\\n\\nremaining privacy budget Â¯ğœ–\\n\\nParam: threshold for column splitting ğ›¼, minimum table\\n\\nsize ğ›½, budget ratios ğ›¾1, ğ›¾2 1 if |ğ‘‡ | < 2ğ›½ and |ğ‘‡ .Attr| = 1 then 2\\n\\nop â† OP.LEAF; ğœ–op â† ğœ–; Â¯ğœ– â† 0;\\n\\n3 else 4\\n\\n5\\n\\n6\\n\\nğ›¿ğ‘ğ‘œğ‘™ , ğœ–eval â† CorrTrial(ğ‘‡ , ğœ–); op â† DecideOP(ğ‘‡ , ğ›¿ğ‘ğ‘œğ‘™ ); ğœ–op, Â¯ğœ– â† AllocBudgetOP(ğ‘‡ , op, ğœ–, ğœ–eval);\\n\\n7 return op, ğœ–op, Â¯ğœ–; 8 procedure CorrTrial(ğ‘‡ , ğœ–) 9\\n\\nif |ğ‘‡ | â‰¥ 2ğ›½ and |ğ‘‡ .Attr| > 1 then\\n\\n10\\n\\n11\\n\\n12\\n\\nğœ–eval â† ğœ– Â· ğ›¾1; ( (cid:98)Sğ¿, (cid:98)Sğ‘…) â† ColSplit(ğ‘‡ , ğœ–eval Â· ğ›¾2); ğ›¿ğ‘ğ‘œğ‘™ â† NMI(ğ‘‡ [ (cid:98)Sğ¿],ğ‘‡ [ (cid:98)Sğ‘…]) + Lap(\\n\\nÎ”(NMI) ğœ–eval Â· (1âˆ’ğ›¾2 ) );\\n\\n13\\n\\nelse\\n\\n14\\n\\nğœ–eval â† 0; ğ›¿ğ‘ğ‘œğ‘™ â† 0;\\n\\nreturn ğ›¿ğ‘ğ‘œğ‘™ , ğœ–eval 15 16 procedure DecideOP(ğ‘‡ , ğ›¿ğ‘ğ‘œğ‘™ ) 17\\n\\nif |ğ‘‡ | â‰¥ 2ğ›½ and |ğ‘‡ .Attr| > 1 then\\n\\n18\\n\\nif ğ›¿ğ‘ğ‘œğ‘™ â‰¤ ğ›¼ then\\n\\n19\\n\\nop â† OP.PRODUCT\\n\\n20\\n\\nelse\\n\\n21\\n\\nop â† OP.SUM\\n\\n22\\n\\nelse if |ğ‘‡ | â‰¥ 2ğ›½ and |ğ‘‡ .Attr| = 1 then\\n\\n23\\n\\nop â† OP.SUM\\n\\n24\\n\\nelse if |ğ‘‡ | < 2ğ›½ and |ğ‘‡ .Attr| > 1 then\\n\\n25\\n\\nop â† OP.PRODUCT\\n\\n26\\n\\nreturn op\\n\\n27 procedure AllocBudgetOP(ğ‘‡ , op, ğœ–, ğœ–eval)\\n\\n28\\n\\nif op = OP.PRODUCT and |ğ‘‡ .Attr| = 2 then\\n\\n29\\n\\nğœ–op â† 0;\\n\\n30\\n\\nelse\\n\\n31\\n\\n32\\n\\nğœ–op â† ğœ–/2 âˆ’ ğœ–eval; return ğœ–op, ğœ– âˆ’ ğœ–eval âˆ’ ğœ–op', metadata={'source': '/content/document4.pdf'}), 0.03013484521216625), (Document(page_content='Example 2. Figure 3 shows an example of how PrivBench com- plements the original SPN ğ‘¡ğ‘– with the leaf nodes of the foreign key Type-ID. First, we identify that attribute size attribute possesses the most leaf nodes in ğ‘¡ğ‘– . For each leaf node of attribute size, we find the corresponding subcolumn of attribute Type âˆ’ ID using the row indices of the subcolumn of attribute size and calculate its fanout table. Then, we perturb the fanout table and store it in a new leaf node. Finally, the new leaf node is added into the SPN as a sibling to the leaf node of attribute size and as the child of a newly added product node, resulting in a modified SPN ğ‘¡ â€² ğ‘– .\\n\\n4.4 SPN-Based Database Synthesis Algorithm 5 shows how we synthesize a table (cid:98)ğ‘‡ğ‘– given an SPN ğ‘¡ğ‘– . If the root node of ğ‘¡ğ‘– is a leaf node, we sample a table (cid:98)ğ‘‡ğ‘– ac- cording to the histogram or fanout table stored in the node (Lines 1â€“2). If it is a sum (product, resp.) node, we recursively apply the SampleDataFromSPN procedure to the left and right children of ğ‘¡ğ‘– and vertically (horizontally, resp.) concatenate the returned tables (cid:98)ğ‘‡ğ¿ and (cid:98)ğ‘‡ğ‘… (Lines 4â€“9). Finally, we obtain a table (cid:98)ğ‘‡ğ‘– that has a data distribution similar to that of the private table ğ‘‡ğ‘– . By synthesizing tables for all modified SPNs, we obtain a synthetic database (cid:98)ğ·.\\n\\n7\\n\\n5 PRIVACY ANALYSIS In this section, we rigoriously analyze how PrivBench satisfies DP and how we should allocate privacy budgets to some key steps in PrivBench.1\\n\\n5.1 Privacy Analysis on PrivSPN In this subsection, we first prove that the Planning, ParentGen, and ChildrenGen procedures satisfy DP, and then prove that PrivSPN therefore achieves DP.\\n\\n5.1.1 Analysis of Planning. The Planning procedure consists of the CorrTrial, DecideOP, and AllocBudgetOP subprocedures. If the subprocedures satisfy DP, we can conclude that Planning ensures DP according to the sequential composition theorem of DP [9].', metadata={'source': '/content/document4.pdf'}), -0.01292229093286612), (Document(page_content='Planning. When |ğ‘‡ | < 2ğ›½ or |ğ‘‡ .Attr| = 1, Planning satisfies 0-DP since any change to ğ‘‡ does not impact the output (Lines 1â€“2); otherwise, since it sequentially combines CorrTrial, DecideOP, and AllocBudgetOP, Planning(ğ‘‡ , ğœ–) satisfies the ğœ–eval-DP according to the sequential composition theorem of DP [9], where the value of ğœ–eval differs in different cases (see Lines 12 and 16).\\n\\nLemma 1. For any ğœ– â‰¥ 0 and any ğ›¾1 â‰¥ 0, if |ğ‘‡ | â‰¥ 2ğ›½ and |ğ‘‡ .Attr| > 1, Planning(ğ‘‡ , ğœ–) satisfies (ğœ– Â·ğ›¾1)-DP; otherwise, it satisfies 0-DP at the table level.\\n\\n5.1.2 Analysis of ParentGen. Then, we show that in any cases of the given operation op, ParentGen achieves DP. (1) When op = OP.LEAF, we ensure DP using the Laplace mechanism. That is, Î”(hist) we inject Laplacian noise Lap( ) into the histogram hist(ğ‘‡ ), ğœ–op which satisfies ğœ–op-DP. (2) When op = OP.SUM, we employ a DP row splitting mechanism RowSplit(ğ‘‡ , ğœ–op) to generate a per- turbed row partition (cid:101)ğ‘†ğ¿, (cid:101)ğ‘†ğ‘…, which achieves ğœ–op-DP. (3) When op = OP.PRODUCT, our column splitting mechanism ColSplit(ğ‘‡ , ğœ–op) is essentially an instance of the Exponential mechanism, thereby guaranteeing ğœ–op-DP.\\n\\n1All missing proofs can be found in our online appendix: https://drive.google.com/file/ d/1mjhKOajBNvN5RbX7k_pvEoFi9DRCWuza/view?usp=sharing.\\n\\nLemma 2. For any op and any ğœ–op â‰¥ 0, ParentGen(ğ‘‡ , op, ğœ–op)\\n\\nTheorem 4. Let T denote the set of all possibles subtables of ğ‘‡ with |ğ‘¡ | < 2ğ›½ and ğ‘¡ .Attr = 1, âˆ€ğ‘¡ âˆˆ T . If for any ğ‘¡ âˆˆ T , PrivSPN(ğ‘‡ , ğœ–) calls PrivSPN(ğ‘¡, ğœ–leaf), PrivSPN(ğ‘‡ , ğœ–) satisfies table-level ğœ–-DP, where ğœ (ğ‘‡ ) = 2|ğ‘‡ .Attr|+ |ğ‘‡ |', metadata={'source': '/content/document4.pdf'}), -0.0489715019885979), (Document(page_content='14\\n\\nğœ–eval â† 0; ğ›¿ğ‘ğ‘œğ‘™ â† 0;\\n\\nreturn ğ›¿ğ‘ğ‘œğ‘™ , ğœ–eval 15 16 procedure DecideOP(ğ‘‡ , ğ›¿ğ‘ğ‘œğ‘™ ) 17\\n\\nif |ğ‘‡ | â‰¥ 2ğ›½ and |ğ‘‡ .Attr| > 1 then\\n\\n18\\n\\nif ğ›¿ğ‘ğ‘œğ‘™ â‰¤ ğ›¼ then\\n\\n19\\n\\nop â† OP.PRODUCT\\n\\n20\\n\\nelse\\n\\n21\\n\\nop â† OP.SUM\\n\\n22\\n\\nelse if |ğ‘‡ | â‰¥ 2ğ›½ and |ğ‘‡ .Attr| = 1 then\\n\\n23\\n\\nop â† OP.SUM\\n\\n24\\n\\nelse if |ğ‘‡ | < 2ğ›½ and |ğ‘‡ .Attr| > 1 then\\n\\n25\\n\\nop â† OP.PRODUCT\\n\\n26\\n\\nreturn op\\n\\n27 procedure AllocBudgetOP(ğ‘‡ , op, ğœ–, ğœ–eval)\\n\\n28\\n\\nif op = OP.PRODUCT and |ğ‘‡ .Attr| = 2 then\\n\\n29\\n\\nğœ–op â† 0;\\n\\n30\\n\\nelse\\n\\n31\\n\\n32\\n\\nğœ–op â† ğœ–/2 âˆ’ ğœ–eval; return ğœ–op, ğœ– âˆ’ ğœ–eval âˆ’ ğœ–op\\n\\nnode and calculates the remaining privacy budget Â¯ğœ– for the children. Concretely, Planning proceeds as follows. 1. Leaf node validation (Lines 1â€“2). We validate if we should only generate a leaf node for the given table. When both the size and the dimension of the given table are insufficient to split, i.e., |ğ‘‡ | < 2ğ›½ and |ğ‘‡ .Attr| = 1, the operation must be creating a leaf node, i.e., â† OP.LEAF, and all the given privacy budget ğœ– should be allocated for op, i.e., ğœ–op = ğœ–. Here, we require that the size of a split subtable should not be smaller than the parameter ğ›½ for the sake of utility, which is ensured by the row splitting algorithm RowSplit.\\n\\n2. Correlation trial (procedure CorrTrial). We preliminarily trial the performance of column splitting, which will be used to decide the operation op by the DecideOP procedure. Specifically, we consume a slight privacy budget ğœ–eval to generate a trial column partition ( (cid:98)Sğ¿, (cid:98)Sğ‘…) (Line 11) and compute the perturbed correlation ),\\n\\nwhere NMI calculates the normalized mutual information of two subtables, Î” returns the global sensitvity of the given function, and Lap generates the Laplace noise (Line 12). Intuitively, the smaller the correlation between the two subtables, the smaller the impact of column splitting on the data distribution, hence the better the performance of column splitting [13]. Note that when either col- umn or row splitting is infeasible, we do not need to evaluate the correlation since the operation must be the feasible one (Line 14).', metadata={'source': '/content/document4.pdf'}), -0.05562210609622298), (Document(page_content='7.5 Case Study In this subsection, we conduct two case studies to demonstrate the performance of PrivBench in real-world scenarios.\\n\\nCase study 1: Real estate database. In this study, we select a real es- tate database [19] collected from Australia, which provides a rich and diverse array of property-related information, including details about the properties themselves, addresses, surrounding features, school district information, and more. The database comprises 12\\n\\n12\\n\\ntables and various common data types. We construct 10 complex queries and evaluate the performance of PrivBench in terms of Q- error, KL divergence, and execution time error. As shown in Table 6, PrivBench performs excellently across all metrics, meeting practi- cal needs. The query with the highest Q-error, targeting properties near the top 50 primary schools and their details, is: \"SELECT pb.*, pa.Formated Address, ps.name, sr.Ranking FROM Property_Basic pb JOIN Property_Address pa ON pb.proID = pa.proID JOIN Mapping_School ms ON pb.proID = ms.pro_ID JOIN School ps ON ms.sec1= ps.school_ID JOIN School_Ranking sr ON ps.school_ID = sr.school_ID WHERE sr.Ranking <= 50 and ps.type = \\'Primary\\';\".\\n\\nFor this query, the cardinality on the original data was 2466, while on the generated data, it was 3129. The reason for this discrepancy is the noise perturbation on the Ranking and type attributes, with the error further amplified through the join of five tables, leading to the final cardinality difference. The execution time error for this query was 0.89 seconds, indicating that the query logic on the generated data did not undergo significant changes. The main cost still lies in the join of the five tables, and the changes in the two attributes did not significantly impact the table scan.', metadata={'source': '/content/document4.pdf'}), -0.058974550720476415), (Document(page_content='for PrivBench.\\n\\nsatisfies table-level 0-DP; otherwise, it satisfies table-level Â¯ğœ–-DP.\\n\\n5.1.4 Analysis of PrivSPN. Based on Lemmas 1, 2, and 3, we con- clude the DP gurantee of PrivSPN through sequential composition.\\n\\nCorollary 1 (of Theorem 5). PrivBench satisfies ((cid:205)ğ‘– âˆˆ [ğ‘›] ğœğ‘– Â· ğ‘– )-DP at the database level.\\n\\nLemma 4. For any ğœ– â‰¥ 0, PrivSPN(ğ‘‡ , ğœ–) satisfies table-level ğœ–-DP.\\n\\nğœğ‘– Â· ğœ– ğ‘“\\n\\nğœ–ğ‘  ğ‘– + (cid:205)ğ‘‡ğ‘– refers to ğ‘‡ğ‘—\\n\\nIn addition, since PrivSPN allocates the privacy budget ğœ– from the root to the leaf nodes in a top-down manner, it may result in insufficient budgets allocated to the leaf nodes. Consequently, an interesting question arises: If we allocate a privacy budget ğœ–leaf to all leaf nodes, how large the total privacy budget ğœ– should be? Theorem 4 answers the question and instructs the design of the scale metirc. That is, if we guarantee a privacy budget ğœ–leaf for each leaf node, then constructing the left and right subtrees should satisfy ğœ (ğ‘‡ [(cid:101)ğ‘†ğ¿])-DP and ğœ (ğ‘‡ [(cid:101)ğ‘†ğ‘…])-DP, respectively. Therefore, the privacy budgets ğœ–ğ‘™ , ğœ–ğ‘Ÿ should be proportional to the corresponding scales (see Line 17 of Algorithm 2). In our experiments, we set the scale metric as defined in Theorem 4 for PrivSPN.\\n\\n6 DISCUSSION In this section, we discuss several interesting functionalities of PrivBench.\\n\\nSupported data types. While most works on privacy-preserving database synthesis only support categorical data, PrivBench ad- ditionally supports numeric data. This is because the leaf nodes of the SPN can classify numeric data by range and generate his- tograms for these ranges. Synthetic numeric data is then sampled from these ranges using the histograms. Theoretically, this data synthesis method could also support more complex data types, such\\n\\n8\\n\\nTable 2: Summary of datasets, workloads, and baselines.\\n\\n#Tables\\n\\nDataset\\n\\n#Attributes Workload\\n\\n#Rows\\n\\nBaselines SAM (non-DP), VGAN (non-DP), AIM, DPSynthesizer, CGAN, PrivSyn, DataSyn, PreFair, MST, PrivMRF\\n\\n48842\\n\\nAdult\\n\\nSAM-1000\\n\\n13\\n\\n1\\n\\nÃ—', metadata={'source': '/content/document4.pdf'}), -0.0597358070982803), (Document(page_content='ğ‘„ âˆ’ ğ‘’ğ‘Ÿğ‘Ÿğ‘œğ‘Ÿ (ğ‘, ğ·, (cid:98)ğ·) = max\\n\\n(cid:32)ğ¶ğ‘ğ‘Ÿğ‘‘ (ğ‘, ğ·) ğ¶ğ‘ğ‘Ÿğ‘‘ (ğ‘, (cid:98)ğ·)\\n\\n,\\n\\nğ¶ğ‘ğ‘Ÿğ‘‘ (ğ‘, (cid:98)ğ·) ğ¶ğ‘ğ‘Ÿğ‘‘ (ğ‘, ğ·)\\n\\n(cid:33)\\n\\nwhere ğ‘ denotes a query in the workload, and ğ¶ğ‘ğ‘Ÿğ‘‘ (ğ‘, ğ·) and ğ¶ğ‘ğ‘Ÿğ‘‘ (ğ‘, (cid:98)ğ·) denote the cardinalities of executing ğ‘ on ğ· and (cid:98)ğ·, respectively. Then, we compute the mean Q-error of the queries in the workload.\\n\\n4 PRIVBENCH: DATABASE SYNTHESIS WITH\\n\\nPRIVATE SPNS\\n\\nIn this section, we propose PrivBench, an SPN-based differentially private database synthesis method.\\n\\n4.1 Overview of PrivBench PrivBench utilizes SPNs to address the two concerns outlined in Section 3. SPNs, due to their excellent performance in representing complex dependencies and data distributions at multiple granulari- ties, have been employed in cardinality estimation [13], a prevalent approach intrinsically connected to estimating the runtime cost of queries. Drawing inspiration from this, we adapt the output in [13] to produce a synthesized database, designed to closely resemble the input database in terms of data distribution and cardinality upon query execution. As shown in Algorithm 1, PrivBench involves a three-phase process for database synthesis.\\n\\nPrivate SPN Construction (Lines 1â€“2). Firstly, we construct an SPN ğ‘¡ğ‘– for each table ğ‘‡ğ‘– in the input database ğ·. Each SPN ğ‘¡ğ‘– is created by a differentially private algorithm PrivSPN, where each leaf node contains a histogram of a column of a subset of rows in ğ‘‡ğ‘– . â€¢ Private Fanout Construction (Lines 3â€“4). Secondly, we complement each SPN ğ‘¡ğ‘– with some leaf nodes that model primary-foreign key references to obtain a modified SPN ğ‘¡ â€² ğ‘– . We use a differentially private algorithm PrivFanout to calculate fanout frequencies for each foreign key and store them in the complemented leaf nodes. â€¢ SPN-Based Database Synthesis (Lines 5â€“6) Thirdly, we sample a ğ‘– to synthesize a database (cid:98)ğ·. table (cid:98)ğ‘‡ğ‘– from each modified SPN ğ‘¡ â€²', metadata={'source': '/content/document4.pdf'}), -0.06342871396480487)]\n",
            "  warnings.warn(\n",
            "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'query': 'What is Planning in case of Private SPN Construction?',\n",
              " 'result': \"\\n<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nUse the following context to answer the question at the end. Do not use any other information. If you can't find the relevant information in the context, just say you don't have enough information to answer the question. Don't try to make up an answer.\\n\\nAlgorithm 2: Private SPN Construction PrivSPN(ğ‘‡ , ğœ–) : table ğ‘‡ , total privacy budget ğœ–. Input Output : A tree of SPN ğ‘¡ = (parent, left, right)\\n\\n1 op, ğœ–op, Â¯ğœ– â† Planning(ğ‘‡ , ğœ–); 2 parent, ( (cid:101)Sğ¿, (cid:101)Sğ‘…) â† ParentGen(ğ‘‡ , op, ğœ–op); 3 left, right â† ChildrenGen(ğ‘‡ , op, (cid:101)Sğ¿, (cid:101)Sğ‘…, Â¯ğœ–); 4 return (parent, left, right) 5 procedure ParentGen(ğ‘‡ , op, ğœ–op) if op = OP.LEAF then 6 Î”(hist) (cid:101)ğ» â† hist(ğ‘‡ ) + Lap( ğœ–op else if op = OP.SUM then\\n\\n); parent â† LeafNode( (cid:101)ğ» );\\n\\n7\\n\\n8\\n\\n9\\n\\n10\\n\\n11\\n\\n(cid:101)Sğ¿, (cid:101)Sğ‘… â† RowSplit(ğ‘‡ , ğœ–op ); parent â† SumNode( (cid:101)Sğ¿, (cid:101)Sğ‘… ); else if op = OP.PRODUCT then (cid:101)Sğ¿, (cid:101)Sğ‘… â† ColSplit(ğ‘‡ , ğœ–op ); parent â† ProdNode( (cid:101)Sğ¿, (cid:101)Sğ‘… );\\n\\n12\\n\\nreturn parent, ( (cid:101)Sğ¿, (cid:101)Sğ‘…)\\n\\n13 procedure ChildrenGen(ğ‘‡ , op, (cid:101)Sğ¿, (cid:101)Sğ‘…, Â¯ğœ–)\\n\\n14\\n\\n15\\n\\nif op = OP.SUM then ğœ–ğ‘™ â† Â¯ğœ–, ğœ–ğ‘Ÿ â† Â¯ğœ–\\n\\n16\\n\\nelse if op = OP.PRODUCT then\\n\\n17\\n\\nğœ–ğ‘™ â† Â¯ğœ– Â·\\n\\nğœ (ğ‘‡ [ (cid:101)Sğ¿ ] ) ğœ (ğ‘‡ [ (cid:101)Sğ¿ ] )+ğœ (ğ‘‡ [ (cid:101)Sğ‘… ] )\\n\\n, ğœ–ğ‘Ÿ â† Â¯ğœ– âˆ’ ğœ–ğ‘™\\n\\n18\\n\\nif op = OP.LEAF then\\n\\n19\\n\\nleft â† null; right â† null;\\n\\n20\\n\\nelse\\n\\n21\\n\\nleft â† PrivSPN(ğ‘‡ [ (cid:101)Sğ¿ ], ğœ–ğ‘™ ); right â† PrivSPN(ğ‘‡ [ (cid:101)Sğ‘… ], ğœ–ğ‘Ÿ );\\n\\n22\\n\\nreturn left, right\\n\\n4.2 Private SPN Construction 4.2.1 Overview of PrivSPN. Algorithm 2 outlines our process for constructing an SPN on a single table. Following DeepDB [13], we employ a tree-structured SPN to model a given table ğ‘‡ . The PrivSPN procedure recursively generates a binary tree ğ‘¡ = (parent, left, right), where parent should be a parent node, and left, right should be two subtrees. We divide PrivSPN into the following three procedures. â€¢ Planning (Operation Planning): We decide an operation op for the parent node and the privacy budget ğœ–op allocated to op. â€¢ ParentGen (Parent Generation): According to the operation op\\n\\nand the privacy budget ğœ–op, we generate the parent node.\\n\\nChildrenGen (Children generation): We further generate the chil- dren left, right through recursive calls to PrivSPN.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nWhat is Planning in case of Private SPN Construction?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n4.2.2 Operation Planning. In this section, we describe how to plan operations and allocate privacy budgets for each operation. For simplicity, we assume that the input table ğ‘‡ has no duplicate rows. We first define the notion of sensitivity of an operation. Definition 4.1 (Sensitivity). Let ğ‘† be a set of tables with the same schema as ğ‘‡. The sensitivity of an operation op âˆˆ {OP.SUM, OP.PROD} over ğ‘† is defined as follows:\\n\\nâ€¢ For OP.SUM, ğ›¾(op, ğ‘†) = max{ğœ (ğ‘‡ ) | ğ‘‡ âˆˆ ğ‘†}. â€¢ For OP.PROD, ğ›¾(op, ğ‘†) = max{ğœ (ğ‘‡ ) | ğ‘‡ âˆˆ ğ‘†}, where ğœ (ğ‘‡ ) =\\n\\n(cid:102)ğ‘¥âˆˆğ‘‡ ğ‘¥(cid:103).\\n\\nWe also define the sensitivity of OP.LEAF as ğ›¾(OP.LEAF, ğ‘†) = 0. Note that the sensitivity of an operation depends on the set of tables it operates on. To simplify the discussion, we will omit the dependency on ğ‘† when there is no ambiguity. Given a table ğ‘‡, we want to construct an SPN that minimizes the overall error while satisfying the privacy constraint. However, since the exact error bound of an SPN is hard to compute, we instead aim to minimize the upper bound of the error. Specifically, we consider the following objective function:\\n\\n(cid:104)ğ‘‚ğ‘ğ‘ğ‘œğ‘ ğ‘¡(ğ‘‡ ) = ğœ–op + ğ›¾(op, ğ‘†)ğœ€ğ‘šğ‘ğ‘¥(ğ‘‡ ), (4.1)\\n\\nwhere ğœ–op is the privacy budget allocated to op, and ğœ€ğ‘šğ‘ğ‘¥(ğ‘‡ ) is the maximum error of the SPN constructed from ğ‘‡. Intuitively, ğœ–op is the amount of noise added by op, and ğ›¾(op, ğ‘†)ğœ€ğ‘šğ‘ğ‘¥(ğ‘‡ ) is the maximum error introduced by all children nodes under op. Therefore, ğ‘‚ğ‘ğ‘ğ‘œğ‘ ğ‘¡(\"}"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "query = \"What is Planning in case of Private SPN Construction?\"\n",
        "result = Chain_web.invoke(query)\n",
        "print(fill(result['result'].strip(), width=100))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_z3gmpYexRw"
      },
      "source": [
        "Question without context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3wsGS0B4zfIS",
        "outputId": "92373d5a-1f7f-4da8-ea11-f12166923b87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/vectorstores.py:330: UserWarning: Relevance scores must be between 0 and 1, got [(Document(page_content='1. GTX has a latch-free graph storage that uses atomic operations to update vertices and edges. It eliminates latching overheads and reduces thread idling.', metadata={'source': '/content/document3.pdf'}), -0.0015943138711989047), (Document(page_content='10\\n\\n[23]\\n\\nJungyeon Choi, Olaf M. Dekkers, and Saskia le Cessie. â€œA comparison of different methods to handle missing data in the context of propensity score analysisâ€. In: European Journal of Epidemiology 34 (1 Jan. 2019), pp. 23â€“36. ISSN: 15737284. DOI: 10.1007/s10654-018- 0447-z.\\n\\n[24] Thomas L. Saaty. Optimization in Integers and Re- lated Extremal Problems. Ed. by Donald K. Prentiss. McGraw-Hill, Inc, 1970, pp. 1â€“295. DOI: 10 . 2307 / 2316512.\\n\\n[25] Kelly A. Frazer et al. â€œA second generation human haplotype map of over 3.1 million SNPsâ€. In: Nature 449 (7164 Oct. 2007), pp. 851â€“861. ISSN: 00280836. DOI: 10.1038/nature06258.\\n\\n[26] Kevin Dunn. Kamyr digester-OpenMV.net Datasets. Oct. 2011. URL: https : / / openmv . net / info / kamyr - digester.\\n\\n[27] Bhupinder S Dayal et al. â€œApplication of feedforward: least squares regression neural networks and partial for modelling kappa number in a continuous Kamyr digester: how multivariate data analysis might help pulpingâ€. In: 1994.', metadata={'source': '/content/document5.pdf'}), -0.1059909618146222), (Document(page_content='GTX allocates a fixed number of worker threads that execute read-write transactions while additionally supports collectively running graph analytics using OpenMP [1]. Besides a commit man- ager, GTX has no other server thread. Transactionsâ€™ isolation and consistency ensures the correctness of concurrent operations. GTX read-write transactions support vertex inserts and updates, edge inserts, deletes, updates and lookups, and adjacency list scans. GTX guarantees atomicity of each transactionâ€™s updates. GTX imple- ments all graph analytics under read-only transactions. While each', metadata={'source': '/content/document3.pdf'}), -0.11603270135343369), (Document(page_content='[5] Paul Madley-Dowd et al. â€œThe proportion of missing data should not be used to guide decisions on multiple imputationâ€. In: Journal of Clinical Epidemiology 110 (June 2019), pp. 63â€“73. ISSN: 18785921. DOI: 10.1016/ j.jclinepi.2019.02.016.\\n\\n[6] Ben Omega Petrazzini et al. â€œEvaluation of different approaches for missing data imputation on features associated to genomic dataâ€. In: BioData Mining 14 (1 Dec. 2021). ISSN: 17560381. DOI: 10.1186/s13040- 021-00274-7.\\n\\n[7] Donald B. Rubin. â€œINFERENCE AND MISSING DATAâ€. In: ETS Research Bulletin Series 1975 (1 June 1975), pp. iâ€“19. ISSN: 2333-8504. DOI: 10.1002/J.2333- 8504.1975.TB01053.X. URL: https://onlinelibrary.wiley. com/doi/full/10.1002/j.2333-8504.1975.tb01053.x. [8] Andrew Briggs et al. â€œMissing.... presumed at random: cost-analysis of incomplete dataâ€. In: Health Econ- comics 12 (5 2003), pp. 377â€“392. DOI: 10.1002/hec. 766. URL: https://onlinelibrary.wiley.com/doi/10.1002/ hec.766.', metadata={'source': '/content/document5.pdf'}), -0.11860155177017284)]\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/langchain_core/vectorstores.py:342: UserWarning: No relevant docs were retrieved using the relevance score threshold 0.2\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|begin_of_text|><|start_header_id|>system<|end_header_id|>  Use the following context to answer the\n",
            "question at the end. Do not use any other information. If you can't find the relevant information in\n",
            "the context, just say you don't have enough information to answer the question. Don't try to make up\n",
            "an answer.  <|eot_id|><|start_header_id|>user<|end_header_id|>  What are the risk factors of\n",
            "NVIDIA?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n"
          ]
        }
      ],
      "source": [
        "query = \"What are the risk factors of NVIDIA?\"\n",
        "result = Chain_web.invoke(query)\n",
        "print(fill(result['result'].strip(), width=100))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c26a230fe48b4e7bb62a9e20df4448d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b8851d1bd9484ed5a9e88942c26b86fe",
              "IPY_MODEL_7f98a8426ca5464fb7dab015fce3505f",
              "IPY_MODEL_23e7d12f3de7453c85ab468d6e986726"
            ],
            "layout": "IPY_MODEL_5722b55b64734b15bdfb872655a3024b"
          }
        },
        "b8851d1bd9484ed5a9e88942c26b86fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_47b5ecf26a164407836ecb81dcd82698",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_add5bbc6398142b3840ca97a65cc5b25",
            "value": "Loadingâ€‡checkpointâ€‡shards:â€‡100%"
          }
        },
        "7f98a8426ca5464fb7dab015fce3505f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_532d6a79362a409fbd5becea59d8a8e6",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bd376012f67e41c790d405c8cd95f39b",
            "value": 4
          }
        },
        "23e7d12f3de7453c85ab468d6e986726": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_96e64dea42c241e7ac33aeac0cf50503",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_6d8429fad86c4df5932411a7a0baae13",
            "value": "â€‡4/4â€‡[01:09&lt;00:00,â€‡15.02s/it]"
          }
        },
        "5722b55b64734b15bdfb872655a3024b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "47b5ecf26a164407836ecb81dcd82698": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "add5bbc6398142b3840ca97a65cc5b25": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "532d6a79362a409fbd5becea59d8a8e6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bd376012f67e41c790d405c8cd95f39b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "96e64dea42c241e7ac33aeac0cf50503": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6d8429fad86c4df5932411a7a0baae13": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}