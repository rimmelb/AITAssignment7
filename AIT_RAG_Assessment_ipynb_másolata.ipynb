{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rimmelb/AITAssignment7/blob/main/AIT_RAG_Assessment_ipynb_m%C3%A1solata.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1xCp6n57BB2"
      },
      "source": [
        "# RAG + LLM Assessment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmI3jBxK68-G"
      },
      "source": [
        "Your task is to create a Retrieval-Augmented Generation (RAG) system using a Large Language Model (LLM). The RAG system should be able to retrieve relevant information from a knowledge base and generate coherent and informative responses to user queries."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3STgq_s6_mV"
      },
      "source": [
        "Steps:\n",
        "\n",
        "1. Choose a domain and collect a suitable dataset of documents (at least 5 documents - PDFs or HTML pages) to serve as the knowledge base for your RAG system. Select one of the following topics:\n",
        "   * latest scientific papers from arxiv.org,\n",
        "   * fiction books released,\n",
        "   * legal documentsor,\n",
        "   * social media posts.\n",
        "\n",
        "   Make sure that the documents are newer then the training dataset of the applied LLM. (20 points)\n",
        "\n",
        "2. Create three relevant prompts to the dataset, and one irrelevant prompt. (20 points)\n",
        "\n",
        "3. Load an LLM with at least 5B parameters. (10 points)\n",
        "\n",
        "4. Test the LLM with your prompts. The goal should be that without the collected dataset your model is unable to answer the question. If it gives you a good answer, select another question to answer and maybe a different dataset. (10 points)\n",
        "\n",
        "5. Create a LangChain-based RAG system by setting up a vector database from the documents. (20 points)\n",
        "\n",
        "6. Provide your three relevant and one irrelevant prompts to your RAG system. For the relevant prompts, your RAG system should return relevant answers, and for the irrelevant prompt, an empty answer. (20 points)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9nwufKe08D3e",
        "outputId": "84bf2f5a-0bd5-4346-8d10-5ae3e32033bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy==3.7.4 in /usr/local/lib/python3.10/dist-packages (3.7.4)\n",
            "Collecting typer==0.10.0\n",
            "  Using cached typer-0.10.0-py3-none-any.whl (46 kB)\n",
            "Requirement already satisfied: imageio==2.31.6 in /usr/local/lib/python3.10/dist-packages (2.31.6)\n",
            "Collecting pillow==10.1.0\n",
            "  Using cached Pillow-10.1.0-cp310-cp310-manylinux_2_28_x86_64.whl (3.6 MB)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy==3.7.4) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.7.4) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.7.4) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy==3.7.4) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy==3.7.4) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy==3.7.4) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy==3.7.4) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy==3.7.4) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy==3.7.4) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.7.4) (0.3.4)\n",
            "INFO: pip is looking at multiple versions of spacy to determine which version is compatible with other requirements. This could take a while.\n",
            "\u001b[31mERROR: Cannot install spacy==3.7.4 and typer==0.10.0 because these package versions have conflicting dependencies.\u001b[0m\u001b[31m\n",
            "\u001b[0m\n",
            "The conflict is caused by:\n",
            "    The user requested typer==0.10.0\n",
            "    spacy 3.7.4 depends on typer<0.10.0 and >=0.3.0\n",
            "\n",
            "To fix this you could try to:\n",
            "1. loosen the range of package versions you've specified\n",
            "2. remove package versions to allow pip attempt to solve the dependency conflict\n",
            "\n",
            "\u001b[31mERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/topics/dependency-resolution/#dealing-with-dependency-conflicts\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.30.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.2.1+cu121)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.23.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Looking in indexes: https://pypi.org/simple/\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.10/dist-packages (0.43.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (2.2.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.25.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->bitsandbytes) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->bitsandbytes) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers>=4.32.0 optimum>=1.12.0 > null\n",
        "!pip install auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu118/ > null\n",
        "!pip install -i https://pypi.org/simple/ bitsandbytes > null\n",
        "!pip install spacy==3.7.4 typer==0.10.0 imageio==2.31.6 pillow==10.1.0\n",
        "!pip install langchain > null\n",
        "!pip install chromadb > null\n",
        "!pip install sentence_transformers > null # ==2.2.2\n",
        "!pip install unstructured > null\n",
        "!pip install pdf2image > null\n",
        "!pip install pdfminer.six > null\n",
        "!pip install unstructured-pytesseract > null\n",
        "!pip install unstructured-inference > null\n",
        "!pip install faiss-gpu > null\n",
        "!pip install pikepdf > null\n",
        "!pip install pypdf > null\n",
        "!pip install accelerate\n",
        "!pip install pillow_heif > null\n",
        "!pip install -i https://pypi.org/simple/ bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DNYrA_xozOh5"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "nrLsJ2aCzOkw"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig, pipeline, BitsAndBytesConfig\n",
        "from textwrap import fill\n",
        "from langchain.prompts import PromptTemplate\n",
        "import locale\n",
        "from langchain.document_loaders import UnstructuredURLLoader\n",
        "from langchain.vectorstores.utils import filter_complex_metadata # 'filter_complex_metadata' removes complex metadata that are not in str, int, float or bool format\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import RetrievalQA\n",
        "import bitsandbytes\n",
        "import accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ddjN8-wU53mg",
        "outputId": "56ee499b-60ed-46b7-9399-0211ddcf9910"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: read).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ],
      "source": [
        "HUGGINGFACE_UAT=\"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\"\n",
        "login(HUGGINGFACE_UAT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225,
          "referenced_widgets": [
            "c26a230fe48b4e7bb62a9e20df4448d4",
            "b8851d1bd9484ed5a9e88942c26b86fe",
            "7f98a8426ca5464fb7dab015fce3505f",
            "23e7d12f3de7453c85ab468d6e986726",
            "5722b55b64734b15bdfb872655a3024b",
            "47b5ecf26a164407836ecb81dcd82698",
            "add5bbc6398142b3840ca97a65cc5b25",
            "532d6a79362a409fbd5becea59d8a8e6",
            "bd376012f67e41c790d405c8cd95f39b",
            "96e64dea42c241e7ac33aeac0cf50503",
            "6d8429fad86c4df5932411a7a0baae13"
          ]
        },
        "id": "SoKKJrg-zOqA",
        "outputId": "4ff4d2b0-585a-4d3a-ebff-e3ee01d503a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c26a230fe48b4e7bb62a9e20df4448d4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ],
      "source": [
        "#model_name = \"google/gemma-2b-it\" # 2B language model from Google\n",
        "\n",
        "import accelerate\n",
        "import bitsandbytes\n",
        "\n",
        "model_name = \"meta-llama/Meta-Llama-3-8B\" # 8B language model from Meta AI\n",
        "\n",
        "quan = BitsAndBytesConfig(load_in_8bit=True)\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
        "                                             device_map=\"auto\",\n",
        "                                             quantization_config=quan,\n",
        "                                             trust_remote_code=True)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "\n",
        "gen_cfg = GenerationConfig.from_pretrained(model_name)\n",
        "gen_cfg.max_new_tokens=512\n",
        "gen_cfg.temperature=0.0000001\n",
        "gen_cfg.return_full_text=True\n",
        "gen_cfg.do_sample=True\n",
        "gen_cfg.repetition_penalty=1.11\n",
        "\n",
        "pipe=pipeline(\n",
        "    task=\"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    generation_config=gen_cfg\n",
        ")\n",
        "\n",
        "llm = HuggingFacePipeline(pipeline=pipe)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMwlm7eg5VkW"
      },
      "source": [
        "Question without related context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "TCUEilpezOso"
      },
      "outputs": [],
      "source": [
        "template_gemma = \"\"\"\n",
        "<bos><start_of_turn>user\n",
        "{text}<end_of_turn>\n",
        "<start_of_turn>model\n",
        "\"\"\"\n",
        "\n",
        "template_llama3 = \"\"\"\n",
        "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
        "\n",
        "{text}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
        "\"\"\"\n",
        "\n",
        "if \"gemma\" in model_name:\n",
        "  template=template_gemma\n",
        "else:\n",
        "  template=template_llama3\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"text\"],\n",
        "    template=template,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qjBo058-9csf",
        "outputId": "08748e04-0712-4a27-bed9-6b2abfd3b55c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The method `BaseLLM.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 0.3.0. Use invoke instead.\n",
            "  warn_deprecated(\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|begin_of_text|><|start_header_id|>user<|end_header_id|>  What kind of Edge Read operations GTX\n",
            "support?<|eot_id|><|start_header_id|>assistant<|end_header_id|>  1. Read the value of a single\n",
            "register.  2. Read the value of multiple registers in one operation.  3. Read the value of all\n",
            "registers in one operation.  Answer: 1, 2 and 3 are supported.  What is the maximum number of\n",
            "registers that can be read in one operation? fıkır  Answer: The maximum number of registers that can\n",
            "be read in one operation is 32.  What is the maximum number of registers that can be written in one\n",
            "operation? fıkır  Answer: The maximum number of registers that can be written in one operation is\n",
            "32.  What is the maximum number of registers that can be read or written in one operation? fıkır\n",
            "Answer: The maximum number of registers that can be read or written in one operation is 64.  What is\n",
            "the maximum number of registers that can be read or written in one operation? fıkır  Answer: The\n",
            "maximum number of registers that can be read or written in one operation is 128.  What is the\n",
            "maximum number of registers that can be read or written in one operation? fıkır  Answer: The maximum\n",
            "number of registers that can be read or written in one operation is 256.  What is the maximum number\n",
            "of registers that can be read or written in one operation? fıkır  Answer: The maximum number of\n",
            "registers that can be read or written in one operation is 512.  What is the maximum number of\n",
            "registers that can be read or written in one operation? fıkır  Answer: The maximum number of\n",
            "registers that can be read or written in one operation is 1024.  What is the maximum number of\n",
            "registers that can be read or written in one operation? fıkır  Answer: The maximum number of\n",
            "registers that can be read or written in one operation is 2048.  What is the maximum number of\n",
            "registers that can be read or written in one operation? fıkır  Answer: The maximum number of\n",
            "registers that can be read or written in one operation is 4096.  What is the maximum number of\n",
            "registers that can be read or written in one operation? fıkır  Answer: The maximum number of\n",
            "registers that can be read or written in one operation is 8192.  What is the maximum number of\n",
            "registers that can be read or written in one operation? fıkır  Answer: The maximum number of\n",
            "registers that can be read or written in one operation is 16384.  What is the maximum number of\n",
            "registers that can be read or written in one operation? fıkır  Answer: The maximum number\n"
          ]
        }
      ],
      "source": [
        "text = \"What kind of Edge Read operations GTX support?\"\n",
        "result = llm(prompt.format(text=text))\n",
        "print(fill(result.strip(), width=100))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvEttpgmetmC"
      },
      "source": [
        "With context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sQ6TY6MC4qqC",
        "outputId": "6c513590-5732-40b7-ba70-cb821f74356f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 4 µs, sys: 0 ns, total: 4 µs\n",
            "Wall time: 8.11 µs\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "prompt_template_gemma = \"\"\"\n",
        "<bos><start_of_turn>user\n",
        "Use the following context to Answer the question at the end. Do not use any other information. If you can't find the relevant information in the context, just say you don't have enough information to answer the question. Don't try to make up an answer.\n",
        "\n",
        "{context}\n",
        "\n",
        "Question: {question}<end_of_turn>\n",
        "\n",
        "<start_of_turn>model\n",
        "\"\"\"\n",
        "\n",
        "prompt_template_llama3 = \"\"\"\n",
        "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
        "\n",
        "Use the following context to answer the question at the end. Do not use any other information. If you can't find the relevant information in the context, just say you don't have enough information to answer the question. Don't try to make up an answer.\n",
        "\n",
        "{context}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
        "\n",
        "{question}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
        "\"\"\"\n",
        "\n",
        "prompt_template=prompt_template_llama3\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xc5t5ZStzOxj",
        "outputId": "d34cc627-22db-4489-bb8d-de7360874d4c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-05-07 20:02:18--  https://arxiv.org/pdf/2405.02269.pdf\n",
            "Resolving arxiv.org (arxiv.org)... 151.101.195.42, 151.101.67.42, 151.101.131.42, ...\n",
            "Connecting to arxiv.org (arxiv.org)|151.101.195.42|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://arxiv.org/pdf/2405.02269 [following]\n",
            "--2024-05-07 20:02:18--  http://arxiv.org/pdf/2405.02269\n",
            "Connecting to arxiv.org (arxiv.org)|151.101.195.42|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 180861 (177K) [application/pdf]\n",
            "Saving to: ‘document1.pdf’\n",
            "\n",
            "\rdocument1.pdf         0%[                    ]       0  --.-KB/s               \rdocument1.pdf       100%[===================>] 176.62K  --.-KB/s    in 0.006s  \n",
            "\n",
            "2024-05-07 20:02:18 (29.8 MB/s) - ‘document1.pdf’ saved [180861/180861]\n",
            "\n",
            "--2024-05-07 20:02:18--  https://arxiv.org/pdf/2405.01804.pdf\n",
            "Resolving arxiv.org (arxiv.org)... 151.101.195.42, 151.101.67.42, 151.101.131.42, ...\n",
            "Connecting to arxiv.org (arxiv.org)|151.101.195.42|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://arxiv.org/pdf/2405.01804 [following]\n",
            "--2024-05-07 20:02:18--  http://arxiv.org/pdf/2405.01804\n",
            "Connecting to arxiv.org (arxiv.org)|151.101.195.42|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 498781 (487K) [application/pdf]\n",
            "Saving to: ‘document2.pdf’\n",
            "\n",
            "document2.pdf       100%[===================>] 487.09K  --.-KB/s    in 0.009s  \n",
            "\n",
            "2024-05-07 20:02:18 (55.8 MB/s) - ‘document2.pdf’ saved [498781/498781]\n",
            "\n",
            "--2024-05-07 20:02:18--  https://arxiv.org/pdf/2405.01448.pdf\n",
            "Resolving arxiv.org (arxiv.org)... 151.101.195.42, 151.101.67.42, 151.101.131.42, ...\n",
            "Connecting to arxiv.org (arxiv.org)|151.101.195.42|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://arxiv.org/pdf/2405.01448 [following]\n",
            "--2024-05-07 20:02:18--  http://arxiv.org/pdf/2405.01448\n",
            "Connecting to arxiv.org (arxiv.org)|151.101.195.42|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 845728 (826K) [application/pdf]\n",
            "Saving to: ‘document3.pdf’\n",
            "\n",
            "document3.pdf       100%[===================>] 825.91K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2024-05-07 20:02:18 (78.4 MB/s) - ‘document3.pdf’ saved [845728/845728]\n",
            "\n",
            "--2024-05-07 20:02:18--  https://arxiv.org/pdf/2405.01312.pdf\n",
            "Resolving arxiv.org (arxiv.org)... 151.101.195.42, 151.101.67.42, 151.101.131.42, ...\n",
            "Connecting to arxiv.org (arxiv.org)|151.101.195.42|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://arxiv.org/pdf/2405.01312 [following]\n",
            "--2024-05-07 20:02:18--  http://arxiv.org/pdf/2405.01312\n",
            "Connecting to arxiv.org (arxiv.org)|151.101.195.42|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1200816 (1.1M) [application/pdf]\n",
            "Saving to: ‘document4.pdf’\n",
            "\n",
            "document4.pdf       100%[===================>]   1.14M  --.-KB/s    in 0.02s   \n",
            "\n",
            "2024-05-07 20:02:18 (75.3 MB/s) - ‘document4.pdf’ saved [1200816/1200816]\n",
            "\n",
            "--2024-05-07 20:02:18--  https://arxiv.org/pdf/2405.00764.pdf\n",
            "Resolving arxiv.org (arxiv.org)... 151.101.195.42, 151.101.67.42, 151.101.131.42, ...\n",
            "Connecting to arxiv.org (arxiv.org)|151.101.195.42|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://arxiv.org/pdf/2405.00764 [following]\n",
            "--2024-05-07 20:02:18--  http://arxiv.org/pdf/2405.00764\n",
            "Connecting to arxiv.org (arxiv.org)|151.101.195.42|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3194831 (3.0M) [application/pdf]\n",
            "Saving to: ‘document5.pdf’\n",
            "\n",
            "document5.pdf       100%[===================>]   3.05M  10.9MB/s    in 0.3s    \n",
            "\n",
            "2024-05-07 20:02:19 (10.9 MB/s) - ‘document5.pdf’ saved [3194831/3194831]\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "!wget -O document1.pdf --no-check-certificate \"https://arxiv.org/pdf/2405.02269.pdf\"\n",
        "!wget -O document2.pdf --no-check-certificate \"https://arxiv.org/pdf/2405.01804.pdf\"\n",
        "!wget -O document3.pdf --no-check-certificate \"https://arxiv.org/pdf/2405.01448.pdf\"\n",
        "!wget -O document4.pdf --no-check-certificate \"https://arxiv.org/pdf/2405.01312.pdf\"\n",
        "!wget -O document5.pdf --no-check-certificate \"https://arxiv.org/pdf/2405.00764.pdf\"\n",
        "\n",
        "from langchain.document_loaders import UnstructuredPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "web_loader = [UnstructuredPDFLoader(pdf) for pdf in [\"/content/document1.pdf\",\"/content/document2.pdf\",\"/content/document3.pdf\", \"/content/document4.pdf\",\"/content/document5.pdf\"]]\n",
        "\n",
        "chunked_web_doc = []\n",
        "\n",
        "for loader in web_loader:\n",
        "  web_doc = loader.load()\n",
        "  up_web_doc = filter_complex_metadata(web_doc)\n",
        "  text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=512)\n",
        "  docs=text_splitter.split_documents(up_web_doc)\n",
        "  chunked_web_doc.extend(docs)\n",
        "\n",
        "len(chunked_web_doc)\n",
        "embeddings = HuggingFaceEmbeddings()\n",
        "db_web = FAISS.from_documents(chunked_web_doc, embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "HhndWobDzcHz"
      },
      "outputs": [],
      "source": [
        "prompt = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
        "Chain_web = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=db_web.as_retriever(search_type=\"similarity_score_threshold\", search_kwargs={'k': 4, 'score_threshold': 0.2}),\n",
        "    chain_type_kwargs={\"prompt\": prompt},\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "03dP4Qg-zcNE",
        "outputId": "28248b67-9b83-4593-87fa-bc9ce16154d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|begin_of_text|><|start_header_id|>system<|end_header_id|>  Use the following context to answer the\n",
            "question at the end. Do not use any other information. If you can't find the relevant information in\n",
            "the context, just say you don't have enough information to answer the question. Don't try to make up\n",
            "an answer.  read-write transaction is executed by its creator worker thread, a read-only transaction\n",
            "can be executed by several OpenMP threads concurrently. GTX further implements a state protection\n",
            "protocol to support edge-deltas block consolidation when it becomes overflow. GTX adopts the block\n",
            "manager from [12] and manages garbage collection lazily. Memory blocks are recycled when no\n",
            "concurrent and future transactions can see them.  3 GTX TRANSACTION OPERATIONS Each GTX transaction\n",
            "is assigned a read timestamp (𝑟𝑡𝑠) at its creation time from a global read epoch, and it does not\n",
            "know its write timestamp (𝑤𝑡𝑠) until it gets committed. GTX guarantees Snapshot Isolation [2] of its\n",
            "transactions. Let 𝑡 refer to both the transaction and its transaction ID interchangeably.  GTX\n",
            "allocates a fixed number of worker threads that execute read-write transactions while additionally\n",
            "supports collectively running graph analytics using OpenMP [1]. Besides a commit man- ager, GTX has\n",
            "no other server thread. Transactions’ isolation and consistency ensures the correctness of\n",
            "concurrent operations. GTX read-write transactions support vertex inserts and updates, edge inserts,\n",
            "deletes, updates and lookups, and adjacency list scans. GTX guarantees atomicity of each\n",
            "transaction’s updates. GTX imple- ments all graph analytics under read-only transactions. While each\n",
            "GTX adopts multi-version concurrency control [11], and uses atomic operations to access and update\n",
            "vertex and edge versions, e.g. 𝑓 𝑒𝑡𝑐ℎ_𝑎𝑑𝑑, 𝑙𝑜𝑎𝑑, and 𝑐𝑜𝑚𝑝𝑎𝑟𝑒_𝑎𝑛𝑑_𝑠𝑤𝑎𝑝(𝐶𝐴𝑆). Vertex versions are\n",
            "updated by a 𝐶𝐴𝑆 on the vertex delta-chains pointer. Each edge- deltas block maintains a\n",
            "combined_offset that combines the offsets of the edge data region and edge-delta region (each 32\n",
            "bits) into a 64 bits integer. Edge-delta allocation is done by executing a fetch_add to the\n",
            "combined_offset and write the edge-delta and its associated property data at the offsets returned by\n",
            "the operation.  read-write transaction is executed by its creator worker thread, a read-only\n",
            "transaction can be executed by several OpenMP threads concurrently. GTX further implements a state\n",
            "protection protocol to support edge-deltas block consolidation when it becomes overflow. GTX adopts\n",
            "the block manager from [12] and manages garbage collection lazily. Memory blocks are recycled when\n",
            "no concurrent and future transactions can see them.  3.1 Vertex Operations GTX transactions can\n",
            "insert new vertices and update them to new versions. Transaction 𝑡 reads a vertex 𝑣 by using an\n",
            "atomic 𝑙𝑜𝑎𝑑 on 𝑣’s delta-chains pointer after locating the 𝑣’s entry in the index. 𝑡 checks 𝑣’s\n",
            "delta-chain head, and compares its 𝑟𝑡𝑠 against the vertex- delta’s creation time. A transaction can\n",
            "see all deltas created before its 𝑟𝑡𝑠. If the current vertex-delta represents the future version, 𝑡\n",
            "uses the 𝑝𝑟𝑒𝑣𝑖𝑜𝑢𝑠_𝑝𝑡𝑟 to read the previous vertex-delta (previous version) on the delta-chain.\n",
            "Vertex writes (insertion and updates) are handled by creating a new vertex-delta (new version), and\n",
            "invoking a 𝐶𝐴𝑆 on 𝑣’s delta-chain pointer. If the 𝐶𝐴𝑆 fails, 𝑡 observes write- write conflict and\n",
            "immediately abort.<|eot_id|><|start_header_id|>user<|end_header_id|>  What do GTX read-write\n",
            "transactions support?<|eot_id|><|start_header_id|>assistant<|end_header_id|> GTX read-write\n",
            "transactions support vertex inserts and updates, edge inserts, deletes, updates and lookups, and\n",
            "adjacency list scans. GTX guarantees atomicity of each transaction’s updates. GTX implements all\n",
            "graph analytics under read-only transactions. While each read-write transaction is executed by its\n",
            "creator worker thread, a read-only transaction can be executed by several OpenMP threads\n",
            "concurrently. GTX further implements a state protection protocol to support edge-deltas block\n",
            "consolidation when it becomes overflow. GTX adopts the block manager from [12] and manages garbage\n",
            "collection lazily. Memory blocks are recycled when no concurrent and future transactions can see\n",
            "them. Each GTX transaction is assigned a read timestamp (rt s) at its creation time from a global\n",
            "read epoch, and it does not know its write timestamp (wt s) until it gets committed. GTX guarantees\n",
            "Snapshot Isolation [2] of its transactions. Let t refer to both the transaction and its transaction\n",
            "ID interchangeably. GTX allocates a fixed number of worker threads that execute read-write\n",
            "transactions while additionally supports collectively running graph analytics using OpenMP [1].\n",
            "Besides a commit man- ager, GTX has no other server thread. Transactions’ isolation and consistency\n",
            "ensures the correctness of concurrent operations. GTX adopts multi-version concurrency control [11],\n",
            "and uses atomic operations to access and update vertex and edge versions, e.g. fetc h_ad d, load,\n",
            "and compare_and_swap(CAS). Vertex versions are updated by a CAS on the vertex delta-chains pointer.\n",
            "Each edge- deltas block maintains a combined_offset that combines the offsets of the edge data\n",
            "region and edge-delta region (each 32 bits) into a 64 bits integer. Edge-delta allocation is done by\n",
            "executing a fetch_add to the combined_offset and write the edge-delta and its associated property\n",
            "data at the offsets returned by the operation. Vertex versions are updated by a CAS on the vertex\n",
            "delta-chains pointer. Each edge- deltas block maintains a combined_offset that combines the offsets\n",
            "of the edge data region and edge-delta region (each 32 bits) into a 64 bits integer. Edge-delta\n",
            "allocation is done by executing a fetch_add to the combined_offset and write the edge-delta and its\n",
            "associated property data at the offsets returned by the operation. GTX read-write transactions\n",
            "support vertex inserts and updates, edge inserts, deletes, updates and lookups, and adjacency list\n",
            "scans. GTX guarantees atomicity of each transaction’s updates. GTX implements all graph analytics\n",
            "under read-only transactions. While each read-write transaction is executed by its creator worker\n",
            "thread,\n"
          ]
        }
      ],
      "source": [
        "query = \"What do GTX read-write transactions support?\"\n",
        "result = Chain_web.invoke(query)\n",
        "print(fill(result['result'].strip(), width=100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zEUzruLFzfDS",
        "outputId": "08ec6da5-f957-41ad-8912-ceaf4e52b20b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|begin_of_text|><|start_header_id|>system<|end_header_id|>  Use the following context to answer the\n",
            "question at the end. Do not use any other information. If you can't find the relevant information in\n",
            "the context, just say you don't have enough information to answer the question. Don't try to make up\n",
            "an answer.  read-write transaction is executed by its creator worker thread, a read-only transaction\n",
            "can be executed by several OpenMP threads concurrently. GTX further implements a state protection\n",
            "protocol to support edge-deltas block consolidation when it becomes overflow. GTX adopts the block\n",
            "manager from [12] and manages garbage collection lazily. Memory blocks are recycled when no\n",
            "concurrent and future transactions can see them.  3 GTX TRANSACTION OPERATIONS Each GTX transaction\n",
            "is assigned a read timestamp (𝑟𝑡𝑠) at its creation time from a global read epoch, and it does not\n",
            "know its write timestamp (𝑤𝑡𝑠) until it gets committed. GTX guarantees Snapshot Isolation [2] of its\n",
            "transactions. Let 𝑡 refer to both the transaction and its transaction ID interchangeably.  GTX\n",
            "adopts multi-version concurrency control [11], and uses atomic operations to access and update\n",
            "vertex and edge versions, e.g. 𝑓 𝑒𝑡𝑐ℎ_𝑎𝑑𝑑, 𝑙𝑜𝑎𝑑, and 𝑐𝑜𝑚𝑝𝑎𝑟𝑒_𝑎𝑛𝑑_𝑠𝑤𝑎𝑝(𝐶𝐴𝑆). Vertex versions are\n",
            "updated by a 𝐶𝐴𝑆 on the vertex delta-chains pointer. Each edge- deltas block maintains a\n",
            "combined_offset that combines the offsets of the edge data region and edge-delta region (each 32\n",
            "bits) into a 64 bits integer. Edge-delta allocation is done by executing a fetch_add to the\n",
            "combined_offset and write the edge-delta and its associated property data at the offsets returned by\n",
            "the operation.  read-write transaction is executed by its creator worker thread, a read-only\n",
            "transaction can be executed by several OpenMP threads concurrently. GTX further implements a state\n",
            "protection protocol to support edge-deltas block consolidation when it becomes overflow. GTX adopts\n",
            "the block manager from [12] and manages garbage collection lazily. Memory blocks are recycled when\n",
            "no concurrent and future transactions can see them.  GTX allocates a fixed number of worker threads\n",
            "that execute read-write transactions while additionally supports collectively running graph\n",
            "analytics using OpenMP [1]. Besides a commit man- ager, GTX has no other server thread.\n",
            "Transactions’ isolation and consistency ensures the correctness of concurrent operations. GTX read-\n",
            "write transactions support vertex inserts and updates, edge inserts, deletes, updates and lookups,\n",
            "and adjacency list scans. GTX guarantees atomicity of each transaction’s updates. GTX imple- ments\n",
            "all graph analytics under read-only transactions. While each  3.1 Vertex Operations GTX transactions\n",
            "can insert new vertices and update them to new versions. Transaction 𝑡 reads a vertex 𝑣 by using an\n",
            "atomic 𝑙𝑜𝑎𝑑 on 𝑣’s delta-chains pointer after locating the 𝑣’s entry in the index. 𝑡 checks 𝑣’s\n",
            "delta-chain head, and compares its 𝑟𝑡𝑠 against the vertex- delta’s creation time. A transaction can\n",
            "see all deltas created before its 𝑟𝑡𝑠. If the current vertex-delta represents the future version, 𝑡\n",
            "uses the 𝑝𝑟𝑒𝑣𝑖𝑜𝑢𝑠_𝑝𝑡𝑟 to read the previous vertex-delta (previous version) on the delta-chain.\n",
            "Vertex writes (insertion and updates) are handled by creating a new vertex-delta (new version), and\n",
            "invoking a 𝐶𝐴𝑆 on 𝑣’s delta-chain pointer. If the 𝐶𝐴𝑆 fails, 𝑡 observes write- write conflict and\n",
            "immediately abort.<|eot_id|><|start_header_id|>user<|end_header_id|>  What is GTX's concurrency\n",
            "control algorithm?<|eot_id|><|start_header_id|>assistant<|end_header_id|> • ### What is GTX's\n",
            "concurrency control algorithm?  GTX adopts multi-version concurrency control [11], and uses atomic\n",
            "operations to access and update vertex and edge versions, e.g. 𝑓 𝑒𝑡𝑐ℎ_𝑎𝑑𝑑, 𝑙𝑜𝑎𝑑, and\n",
            "𝑐𝑜𝑚𝑝𝑎𝑟𝑒_𝑎𝑛𝑑_𝑠𝑤𝑎𝑝(𝐶𝐴𝑆). Vertex versions are updated by a 𝐶𝐴𝑆 on the vertex delta-chains pointer. Each\n",
            "edge- deltas block maintains a combined_offset that combines the offsets of the edge data region and\n",
            "edge-delta region (each 32 bits) into a 64 bits integer. Edge-delta allocation is done by executing\n",
            "a fetch_add to the combined_offset and write the edge-delta and its associated property data at the\n",
            "offsets returned by the operation. read-write transaction is executed by its creator worker thread,\n",
            "a read-only transaction can be executed by several OpenMP threads concurrently. GTX further\n",
            "implements a state protection protocol to support edge-deltas block consolidation when it becomes\n",
            "overflow. GTX adopts the block manager from [12] and manages garbage collection lazily. Memory\n",
            "blocks are recycled when no concurrent and future transactions can see them. GTX allocates a fixed\n",
            "number of worker threads that execute read-write transactions while additionally supports\n",
            "collectively running graph analytics using OpenMP [1]. Besides a commit man- ager, GTX has no other\n",
            "server thread. Transactions’ isolation and consistency ensures the correctness of concurrent\n",
            "operations. GTX read-write transactions support vertex inserts and updates, edge inserts, deletes,\n",
            "updates and lookups, and adjacency list scans. GTX guarantees atomicity of each transaction’s\n",
            "updates. GTX imple- ments all graph analytics under read-only transactions. While each 3.1 Vertex\n",
            "Operations GTX transactions can insert new vertices and update them to new versions. Transaction 𝑡\n",
            "reads a vertex 𝑣 by using an atomic 𝑙𝑜𝑎𝑑 on 𝑣’s delta-chains pointer after locating the 𝑣’s entry in\n",
            "the index. 𝑡 checks 𝑣’s delta-chain head, and compares its 𝑟𝑡𝑠 against the vertex- delta’s creation\n",
            "time. A transaction can see all deltas created before its 𝑟𝑡\n"
          ]
        }
      ],
      "source": [
        "query = \"What is GTX's concurrency control algorithm?\"\n",
        "result = Chain_web.invoke(query)\n",
        "print(fill(result['result'].strip(), width=100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e15evr_tzfFy",
        "outputId": "ca6fad4d-a94e-417c-828e-c0040eb9a440"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/vectorstores.py:330: UserWarning: Relevance scores must be between 0 and 1, got [(Document(page_content='Algorithm 2: Private SPN Construction PrivSPN(𝑇 , 𝜖) : table 𝑇 , total privacy budget 𝜖. Input Output : A tree of SPN 𝑡 = (parent, left, right)\\n\\n1 op, 𝜖op, ¯𝜖 ← Planning(𝑇 , 𝜖); 2 parent, ( (cid:101)S𝐿, (cid:101)S𝑅) ← ParentGen(𝑇 , op, 𝜖op); 3 left, right ← ChildrenGen(𝑇 , op, (cid:101)S𝐿, (cid:101)S𝑅, ¯𝜖); 4 return (parent, left, right) 5 procedure ParentGen(𝑇 , op, 𝜖op) if op = OP.LEAF then 6 Δ(hist) (cid:101)𝐻 ← hist(𝑇 ) + Lap( 𝜖op else if op = OP.SUM then\\n\\n); parent ← LeafNode( (cid:101)𝐻 );\\n\\n7\\n\\n8\\n\\n9\\n\\n10\\n\\n11\\n\\n(cid:101)S𝐿, (cid:101)S𝑅 ← RowSplit(𝑇 , 𝜖op ); parent ← SumNode( (cid:101)S𝐿, (cid:101)S𝑅 ); else if op = OP.PRODUCT then (cid:101)S𝐿, (cid:101)S𝑅 ← ColSplit(𝑇 , 𝜖op ); parent ← ProdNode( (cid:101)S𝐿, (cid:101)S𝑅 );\\n\\n12\\n\\nreturn parent, ( (cid:101)S𝐿, (cid:101)S𝑅)\\n\\n13 procedure ChildrenGen(𝑇 , op, (cid:101)S𝐿, (cid:101)S𝑅, ¯𝜖)\\n\\n14\\n\\n15\\n\\nif op = OP.SUM then 𝜖𝑙 ← ¯𝜖, 𝜖𝑟 ← ¯𝜖\\n\\n16\\n\\nelse if op = OP.PRODUCT then\\n\\n17\\n\\n𝜖𝑙 ← ¯𝜖 ·\\n\\n𝜎 (𝑇 [ (cid:101)S𝐿 ] ) 𝜎 (𝑇 [ (cid:101)S𝐿 ] )+𝜎 (𝑇 [ (cid:101)S𝑅 ] )\\n\\n, 𝜖𝑟 ← ¯𝜖 − 𝜖𝑙\\n\\n18\\n\\nif op = OP.LEAF then\\n\\n19\\n\\nleft ← null; right ← null;\\n\\n20\\n\\nelse\\n\\n21\\n\\nleft ← PrivSPN(𝑇 [ (cid:101)S𝐿 ], 𝜖𝑙 ); right ← PrivSPN(𝑇 [ (cid:101)S𝑅 ], 𝜖𝑟 );\\n\\n22\\n\\nreturn left, right\\n\\n4.2 Private SPN Construction 4.2.1 Overview of PrivSPN. Algorithm 2 outlines our process for constructing an SPN on a single table. Following DeepDB [13], we employ a tree-structured SPN to model a given table 𝑇 . The PrivSPN procedure recursively generates a binary tree 𝑡 = (parent, left, right), where parent should be a parent node, and left, right should be two subtrees. We divide PrivSPN into the following three procedures. • Planning (Operation Planning): We decide an operation op for the parent node and the privacy budget 𝜖op allocated to op. • ParentGen (Parent Generation): According to the operation op\\n\\nand the privacy budget 𝜖op, we generate the parent node.\\n\\nChildrenGen (Children generation): We further generate the chil- dren left, right through recursive calls to PrivSPN.', metadata={'source': '/content/document4.pdf'}), 0.10285122441649686), (Document(page_content='5.1 Privacy Analysis on PrivSPN In this subsection, we first prove that the Planning, ParentGen, and ChildrenGen procedures satisfy DP, and then prove that PrivSPN therefore achieves DP.\\n\\n5.1.1 Analysis of Planning. The Planning procedure consists of the CorrTrial, DecideOP, and AllocBudgetOP subprocedures. If the subprocedures satisfy DP, we can conclude that Planning ensures DP according to the sequential composition theorem of DP [9].\\n\\nCorrTrial. (1) When |𝑇 | ≥ 2𝛽 and |𝑇 .Attr| > 1, the CorrTrial proce- dure employs the 𝜖eval · 𝛾2-DP procedure ColSplit(𝑇 , 𝜖eval · 𝛾2) to (cid:98)S𝐿, (cid:98)S𝑅 (cid:1). Then, it calculates the NMI compute a perturbed partition (cid:0) Δ(NMI𝑐𝑜𝑙 ) 𝜖eval · (1−𝛾2 ) ) into it, of ( (cid:98)S𝐿, (cid:98)S𝑅) and injects the Laplace noise Lap( which ensures (cid:0)𝜖eval · (1 − 𝛾2)(cid:1)-DP. Consequently, according to the sequential composition theorem of DP [9], CorrTrial(𝑇 , 𝜖) satisfies 𝜖eval-DP. (2) When |𝑇 | < 2𝛽 or |𝑇 .Attr| = 1, since varying the value of any row in 𝑇 does not have any impact on the output (𝜎𝑐𝑜𝑙 , 𝜖eval), CorrTrial(𝑇 , 𝜖) satisfies 0-DP according to Definition 1. Note that we adopt the bounded DP interpretation; in the case of unbounded DP, adding/removing a row from 𝑇 causes the procedure to take the other conditional branch (i.e., Lines 11–14), thereby breaking the DP guarantee.\\n\\nDecideOP and AllocBudgetOP. Similar to the second case of CorrTrial, for both procedures DecideOP and AllocBudgetOP, because chang- ing the value of any row in 𝑇 does not affect the table size |𝑇 | and the table dimension |𝑇 .Attr|, it also does not impact their outputs. Therefore, they ensure 0-DP.\\n\\nPlanning. When |𝑇 | < 2𝛽 or |𝑇 .Attr| = 1, Planning satisfies 0-DP since any change to 𝑇 does not impact the output (Lines 1–2); otherwise, since it sequentially combines CorrTrial, DecideOP, and AllocBudgetOP, Planning(𝑇 , 𝜖) satisfies the 𝜖eval-DP according to the sequential composition theorem of DP [9], where the value of 𝜖eval differs in different cases (see Lines 12 and 16).', metadata={'source': '/content/document4.pdf'}), 0.08027467495575735), (Document(page_content='This work is licensed under the Creative Commons BY-NC-ND 4.0 International License. Visit https://creativecommons.org/licenses/by-nc-nd/4.0/ to view a copy of this license. For any use beyond those covered by this license, obtain permission by emailing info@vldb.org. Copyright is held by the owner/author(s). Publication rights\\n\\nlicensed to the VLDB Endowment. Proceedings of the VLDB Endowment, Vol. 17, No. 1 ISSN 2150-8097. doi:XX.XX/XXX.XX\\n\\n1\\n\\ncount: 1600Size765Price45.435.210.5\\n\\nID135…\\n\\nOriginal Table 𝑇\\n\\nSize358Price15.010.025.0……count: 160𝑇[ሚ𝒮𝐿]\\n\\nID245…\\n\\n×\\n\\n040.0\\n\\n×\\n\\nID245…count: 340Size469Price40.030.032.0\\n\\n110\\n\\n+\\n\\nID136…\\n\\n𝑝𝑒𝑟𝑡𝑢𝑟𝑏(ሚ𝒮𝐿,ሚ𝒮𝑅)Size486935…Price40.025.030.032.015.010.0…ID123456…count: 500\\n\\n×\\n\\n𝐻\\n\\n……\\n\\nSize765…count: 1600\\n\\n…Size271count: 3400…Price94.122.633.4\\n\\n+\\n\\n…(𝒮𝐿,𝒮𝑅)\\n\\nID136…\\n\\n1\\n\\ncount: 5000(b) Learning an SPN and Generating Datacount: 160count: 160count: 340count: 340\\n\\n040.0count: 1600Price45.435.210.5\\n\\n……𝑇[ሚ𝒮𝑅]\\n\\nPrivate SPN ConstructionSPN-Based Database Synthesis\\n\\ncount: 3400Size765271…Price45.435.210.594.122.633.4…\\n\\n110110\\n\\n×\\n\\nSize358…Price15.010.025.0…Price40.030.032.0…Size469…\\n\\ncount: 3400\\n\\n𝑝𝑒𝑟𝑡𝑢𝑟𝑏෩𝐻\\n\\nID246…\\n\\nSize271…Price94.122.633.4…\\n\\nData SamplingSynthetic Table \\u0de0𝑇\\n\\nFigure 1: Learning an SPN and generating data based on single-table.', metadata={'source': '/content/document4.pdf'}), 0.04052421195749567), (Document(page_content='and the privacy budget 𝜖op, we generate the parent node.\\n\\nChildrenGen (Children generation): We further generate the chil- dren left, right through recursive calls to PrivSPN.\\n\\n4.2.2 Planning. The Planning procedure takes the role of deter- mining an operation op for the parent node, which can be creating a leaf/sum/product node. A sum (resp. product) node indicates par- titioning the rows (resp. columns) of the given table, while a leaf node means generating the table’s histogram. Since all these types of operations should be performed in a differentially private man- ner, Planning also allocates a privacy budget 𝜖op for the parent\\n\\n5\\n\\nAlgorithm 3: Operation Planning Planning(𝑇 , 𝜖) Input Output : next operation op, privacy budget 𝜖op for op,\\n\\n: table 𝑇 , privacy budget 𝜖\\n\\nremaining privacy budget ¯𝜖\\n\\nParam: threshold for column splitting 𝛼, minimum table\\n\\nsize 𝛽, budget ratios 𝛾1, 𝛾2 1 if |𝑇 | < 2𝛽 and |𝑇 .Attr| = 1 then 2\\n\\nop ← OP.LEAF; 𝜖op ← 𝜖; ¯𝜖 ← 0;\\n\\n3 else 4\\n\\n5\\n\\n6\\n\\n𝛿𝑐𝑜𝑙 , 𝜖eval ← CorrTrial(𝑇 , 𝜖); op ← DecideOP(𝑇 , 𝛿𝑐𝑜𝑙 ); 𝜖op, ¯𝜖 ← AllocBudgetOP(𝑇 , op, 𝜖, 𝜖eval);\\n\\n7 return op, 𝜖op, ¯𝜖; 8 procedure CorrTrial(𝑇 , 𝜖) 9\\n\\nif |𝑇 | ≥ 2𝛽 and |𝑇 .Attr| > 1 then\\n\\n10\\n\\n11\\n\\n12\\n\\n𝜖eval ← 𝜖 · 𝛾1; ( (cid:98)S𝐿, (cid:98)S𝑅) ← ColSplit(𝑇 , 𝜖eval · 𝛾2); 𝛿𝑐𝑜𝑙 ← NMI(𝑇 [ (cid:98)S𝐿],𝑇 [ (cid:98)S𝑅]) + Lap(\\n\\nΔ(NMI) 𝜖eval · (1−𝛾2 ) );\\n\\n13\\n\\nelse\\n\\n14\\n\\n𝜖eval ← 0; 𝛿𝑐𝑜𝑙 ← 0;\\n\\nreturn 𝛿𝑐𝑜𝑙 , 𝜖eval 15 16 procedure DecideOP(𝑇 , 𝛿𝑐𝑜𝑙 ) 17\\n\\nif |𝑇 | ≥ 2𝛽 and |𝑇 .Attr| > 1 then\\n\\n18\\n\\nif 𝛿𝑐𝑜𝑙 ≤ 𝛼 then\\n\\n19\\n\\nop ← OP.PRODUCT\\n\\n20\\n\\nelse\\n\\n21\\n\\nop ← OP.SUM\\n\\n22\\n\\nelse if |𝑇 | ≥ 2𝛽 and |𝑇 .Attr| = 1 then\\n\\n23\\n\\nop ← OP.SUM\\n\\n24\\n\\nelse if |𝑇 | < 2𝛽 and |𝑇 .Attr| > 1 then\\n\\n25\\n\\nop ← OP.PRODUCT\\n\\n26\\n\\nreturn op\\n\\n27 procedure AllocBudgetOP(𝑇 , op, 𝜖, 𝜖eval)\\n\\n28\\n\\nif op = OP.PRODUCT and |𝑇 .Attr| = 2 then\\n\\n29\\n\\n𝜖op ← 0;\\n\\n30\\n\\nelse\\n\\n31\\n\\n32\\n\\n𝜖op ← 𝜖/2 − 𝜖eval; return 𝜖op, 𝜖 − 𝜖eval − 𝜖op', metadata={'source': '/content/document4.pdf'}), 0.03013484521216625), (Document(page_content='Example 2. Figure 3 shows an example of how PrivBench com- plements the original SPN 𝑡𝑖 with the leaf nodes of the foreign key Type-ID. First, we identify that attribute size attribute possesses the most leaf nodes in 𝑡𝑖 . For each leaf node of attribute size, we find the corresponding subcolumn of attribute Type − ID using the row indices of the subcolumn of attribute size and calculate its fanout table. Then, we perturb the fanout table and store it in a new leaf node. Finally, the new leaf node is added into the SPN as a sibling to the leaf node of attribute size and as the child of a newly added product node, resulting in a modified SPN 𝑡 ′ 𝑖 .\\n\\n4.4 SPN-Based Database Synthesis Algorithm 5 shows how we synthesize a table (cid:98)𝑇𝑖 given an SPN 𝑡𝑖 . If the root node of 𝑡𝑖 is a leaf node, we sample a table (cid:98)𝑇𝑖 ac- cording to the histogram or fanout table stored in the node (Lines 1–2). If it is a sum (product, resp.) node, we recursively apply the SampleDataFromSPN procedure to the left and right children of 𝑡𝑖 and vertically (horizontally, resp.) concatenate the returned tables (cid:98)𝑇𝐿 and (cid:98)𝑇𝑅 (Lines 4–9). Finally, we obtain a table (cid:98)𝑇𝑖 that has a data distribution similar to that of the private table 𝑇𝑖 . By synthesizing tables for all modified SPNs, we obtain a synthetic database (cid:98)𝐷.\\n\\n7\\n\\n5 PRIVACY ANALYSIS In this section, we rigoriously analyze how PrivBench satisfies DP and how we should allocate privacy budgets to some key steps in PrivBench.1\\n\\n5.1 Privacy Analysis on PrivSPN In this subsection, we first prove that the Planning, ParentGen, and ChildrenGen procedures satisfy DP, and then prove that PrivSPN therefore achieves DP.\\n\\n5.1.1 Analysis of Planning. The Planning procedure consists of the CorrTrial, DecideOP, and AllocBudgetOP subprocedures. If the subprocedures satisfy DP, we can conclude that Planning ensures DP according to the sequential composition theorem of DP [9].', metadata={'source': '/content/document4.pdf'}), -0.01292229093286612), (Document(page_content='Planning. When |𝑇 | < 2𝛽 or |𝑇 .Attr| = 1, Planning satisfies 0-DP since any change to 𝑇 does not impact the output (Lines 1–2); otherwise, since it sequentially combines CorrTrial, DecideOP, and AllocBudgetOP, Planning(𝑇 , 𝜖) satisfies the 𝜖eval-DP according to the sequential composition theorem of DP [9], where the value of 𝜖eval differs in different cases (see Lines 12 and 16).\\n\\nLemma 1. For any 𝜖 ≥ 0 and any 𝛾1 ≥ 0, if |𝑇 | ≥ 2𝛽 and |𝑇 .Attr| > 1, Planning(𝑇 , 𝜖) satisfies (𝜖 ·𝛾1)-DP; otherwise, it satisfies 0-DP at the table level.\\n\\n5.1.2 Analysis of ParentGen. Then, we show that in any cases of the given operation op, ParentGen achieves DP. (1) When op = OP.LEAF, we ensure DP using the Laplace mechanism. That is, Δ(hist) we inject Laplacian noise Lap( ) into the histogram hist(𝑇 ), 𝜖op which satisfies 𝜖op-DP. (2) When op = OP.SUM, we employ a DP row splitting mechanism RowSplit(𝑇 , 𝜖op) to generate a per- turbed row partition (cid:101)𝑆𝐿, (cid:101)𝑆𝑅, which achieves 𝜖op-DP. (3) When op = OP.PRODUCT, our column splitting mechanism ColSplit(𝑇 , 𝜖op) is essentially an instance of the Exponential mechanism, thereby guaranteeing 𝜖op-DP.\\n\\n1All missing proofs can be found in our online appendix: https://drive.google.com/file/ d/1mjhKOajBNvN5RbX7k_pvEoFi9DRCWuza/view?usp=sharing.\\n\\nLemma 2. For any op and any 𝜖op ≥ 0, ParentGen(𝑇 , op, 𝜖op)\\n\\nTheorem 4. Let T denote the set of all possibles subtables of 𝑇 with |𝑡 | < 2𝛽 and 𝑡 .Attr = 1, ∀𝑡 ∈ T . If for any 𝑡 ∈ T , PrivSPN(𝑇 , 𝜖) calls PrivSPN(𝑡, 𝜖leaf), PrivSPN(𝑇 , 𝜖) satisfies table-level 𝜖-DP, where 𝜎 (𝑇 ) = 2|𝑇 .Attr|+ |𝑇 |', metadata={'source': '/content/document4.pdf'}), -0.0489715019885979), (Document(page_content='14\\n\\n𝜖eval ← 0; 𝛿𝑐𝑜𝑙 ← 0;\\n\\nreturn 𝛿𝑐𝑜𝑙 , 𝜖eval 15 16 procedure DecideOP(𝑇 , 𝛿𝑐𝑜𝑙 ) 17\\n\\nif |𝑇 | ≥ 2𝛽 and |𝑇 .Attr| > 1 then\\n\\n18\\n\\nif 𝛿𝑐𝑜𝑙 ≤ 𝛼 then\\n\\n19\\n\\nop ← OP.PRODUCT\\n\\n20\\n\\nelse\\n\\n21\\n\\nop ← OP.SUM\\n\\n22\\n\\nelse if |𝑇 | ≥ 2𝛽 and |𝑇 .Attr| = 1 then\\n\\n23\\n\\nop ← OP.SUM\\n\\n24\\n\\nelse if |𝑇 | < 2𝛽 and |𝑇 .Attr| > 1 then\\n\\n25\\n\\nop ← OP.PRODUCT\\n\\n26\\n\\nreturn op\\n\\n27 procedure AllocBudgetOP(𝑇 , op, 𝜖, 𝜖eval)\\n\\n28\\n\\nif op = OP.PRODUCT and |𝑇 .Attr| = 2 then\\n\\n29\\n\\n𝜖op ← 0;\\n\\n30\\n\\nelse\\n\\n31\\n\\n32\\n\\n𝜖op ← 𝜖/2 − 𝜖eval; return 𝜖op, 𝜖 − 𝜖eval − 𝜖op\\n\\nnode and calculates the remaining privacy budget ¯𝜖 for the children. Concretely, Planning proceeds as follows. 1. Leaf node validation (Lines 1–2). We validate if we should only generate a leaf node for the given table. When both the size and the dimension of the given table are insufficient to split, i.e., |𝑇 | < 2𝛽 and |𝑇 .Attr| = 1, the operation must be creating a leaf node, i.e., ← OP.LEAF, and all the given privacy budget 𝜖 should be allocated for op, i.e., 𝜖op = 𝜖. Here, we require that the size of a split subtable should not be smaller than the parameter 𝛽 for the sake of utility, which is ensured by the row splitting algorithm RowSplit.\\n\\n2. Correlation trial (procedure CorrTrial). We preliminarily trial the performance of column splitting, which will be used to decide the operation op by the DecideOP procedure. Specifically, we consume a slight privacy budget 𝜖eval to generate a trial column partition ( (cid:98)S𝐿, (cid:98)S𝑅) (Line 11) and compute the perturbed correlation ),\\n\\nwhere NMI calculates the normalized mutual information of two subtables, Δ returns the global sensitvity of the given function, and Lap generates the Laplace noise (Line 12). Intuitively, the smaller the correlation between the two subtables, the smaller the impact of column splitting on the data distribution, hence the better the performance of column splitting [13]. Note that when either col- umn or row splitting is infeasible, we do not need to evaluate the correlation since the operation must be the feasible one (Line 14).', metadata={'source': '/content/document4.pdf'}), -0.05562210609622298), (Document(page_content='7.5 Case Study In this subsection, we conduct two case studies to demonstrate the performance of PrivBench in real-world scenarios.\\n\\nCase study 1: Real estate database. In this study, we select a real es- tate database [19] collected from Australia, which provides a rich and diverse array of property-related information, including details about the properties themselves, addresses, surrounding features, school district information, and more. The database comprises 12\\n\\n12\\n\\ntables and various common data types. We construct 10 complex queries and evaluate the performance of PrivBench in terms of Q- error, KL divergence, and execution time error. As shown in Table 6, PrivBench performs excellently across all metrics, meeting practi- cal needs. The query with the highest Q-error, targeting properties near the top 50 primary schools and their details, is: \"SELECT pb.*, pa.Formated Address, ps.name, sr.Ranking FROM Property_Basic pb JOIN Property_Address pa ON pb.proID = pa.proID JOIN Mapping_School ms ON pb.proID = ms.pro_ID JOIN School ps ON ms.sec1= ps.school_ID JOIN School_Ranking sr ON ps.school_ID = sr.school_ID WHERE sr.Ranking <= 50 and ps.type = \\'Primary\\';\".\\n\\nFor this query, the cardinality on the original data was 2466, while on the generated data, it was 3129. The reason for this discrepancy is the noise perturbation on the Ranking and type attributes, with the error further amplified through the join of five tables, leading to the final cardinality difference. The execution time error for this query was 0.89 seconds, indicating that the query logic on the generated data did not undergo significant changes. The main cost still lies in the join of the five tables, and the changes in the two attributes did not significantly impact the table scan.', metadata={'source': '/content/document4.pdf'}), -0.058974550720476415), (Document(page_content='for PrivBench.\\n\\nsatisfies table-level 0-DP; otherwise, it satisfies table-level ¯𝜖-DP.\\n\\n5.1.4 Analysis of PrivSPN. Based on Lemmas 1, 2, and 3, we con- clude the DP gurantee of PrivSPN through sequential composition.\\n\\nCorollary 1 (of Theorem 5). PrivBench satisfies ((cid:205)𝑖 ∈ [𝑛] 𝜏𝑖 · 𝑖 )-DP at the database level.\\n\\nLemma 4. For any 𝜖 ≥ 0, PrivSPN(𝑇 , 𝜖) satisfies table-level 𝜖-DP.\\n\\n𝜏𝑖 · 𝜖 𝑓\\n\\n𝜖𝑠 𝑖 + (cid:205)𝑇𝑖 refers to 𝑇𝑗\\n\\nIn addition, since PrivSPN allocates the privacy budget 𝜖 from the root to the leaf nodes in a top-down manner, it may result in insufficient budgets allocated to the leaf nodes. Consequently, an interesting question arises: If we allocate a privacy budget 𝜖leaf to all leaf nodes, how large the total privacy budget 𝜖 should be? Theorem 4 answers the question and instructs the design of the scale metirc. That is, if we guarantee a privacy budget 𝜖leaf for each leaf node, then constructing the left and right subtrees should satisfy 𝜎 (𝑇 [(cid:101)𝑆𝐿])-DP and 𝜎 (𝑇 [(cid:101)𝑆𝑅])-DP, respectively. Therefore, the privacy budgets 𝜖𝑙 , 𝜖𝑟 should be proportional to the corresponding scales (see Line 17 of Algorithm 2). In our experiments, we set the scale metric as defined in Theorem 4 for PrivSPN.\\n\\n6 DISCUSSION In this section, we discuss several interesting functionalities of PrivBench.\\n\\nSupported data types. While most works on privacy-preserving database synthesis only support categorical data, PrivBench ad- ditionally supports numeric data. This is because the leaf nodes of the SPN can classify numeric data by range and generate his- tograms for these ranges. Synthetic numeric data is then sampled from these ranges using the histograms. Theoretically, this data synthesis method could also support more complex data types, such\\n\\n8\\n\\nTable 2: Summary of datasets, workloads, and baselines.\\n\\n#Tables\\n\\nDataset\\n\\n#Attributes Workload\\n\\n#Rows\\n\\nBaselines SAM (non-DP), VGAN (non-DP), AIM, DPSynthesizer, CGAN, PrivSyn, DataSyn, PreFair, MST, PrivMRF\\n\\n48842\\n\\nAdult\\n\\nSAM-1000\\n\\n13\\n\\n1\\n\\n×', metadata={'source': '/content/document4.pdf'}), -0.0597358070982803), (Document(page_content='𝑄 − 𝑒𝑟𝑟𝑜𝑟 (𝑞, 𝐷, (cid:98)𝐷) = max\\n\\n(cid:32)𝐶𝑎𝑟𝑑 (𝑞, 𝐷) 𝐶𝑎𝑟𝑑 (𝑞, (cid:98)𝐷)\\n\\n,\\n\\n𝐶𝑎𝑟𝑑 (𝑞, (cid:98)𝐷) 𝐶𝑎𝑟𝑑 (𝑞, 𝐷)\\n\\n(cid:33)\\n\\nwhere 𝑞 denotes a query in the workload, and 𝐶𝑎𝑟𝑑 (𝑞, 𝐷) and 𝐶𝑎𝑟𝑑 (𝑞, (cid:98)𝐷) denote the cardinalities of executing 𝑞 on 𝐷 and (cid:98)𝐷, respectively. Then, we compute the mean Q-error of the queries in the workload.\\n\\n4 PRIVBENCH: DATABASE SYNTHESIS WITH\\n\\nPRIVATE SPNS\\n\\nIn this section, we propose PrivBench, an SPN-based differentially private database synthesis method.\\n\\n4.1 Overview of PrivBench PrivBench utilizes SPNs to address the two concerns outlined in Section 3. SPNs, due to their excellent performance in representing complex dependencies and data distributions at multiple granulari- ties, have been employed in cardinality estimation [13], a prevalent approach intrinsically connected to estimating the runtime cost of queries. Drawing inspiration from this, we adapt the output in [13] to produce a synthesized database, designed to closely resemble the input database in terms of data distribution and cardinality upon query execution. As shown in Algorithm 1, PrivBench involves a three-phase process for database synthesis.\\n\\nPrivate SPN Construction (Lines 1–2). Firstly, we construct an SPN 𝑡𝑖 for each table 𝑇𝑖 in the input database 𝐷. Each SPN 𝑡𝑖 is created by a differentially private algorithm PrivSPN, where each leaf node contains a histogram of a column of a subset of rows in 𝑇𝑖 . • Private Fanout Construction (Lines 3–4). Secondly, we complement each SPN 𝑡𝑖 with some leaf nodes that model primary-foreign key references to obtain a modified SPN 𝑡 ′ 𝑖 . We use a differentially private algorithm PrivFanout to calculate fanout frequencies for each foreign key and store them in the complemented leaf nodes. • SPN-Based Database Synthesis (Lines 5–6) Thirdly, we sample a 𝑖 to synthesize a database (cid:98)𝐷. table (cid:98)𝑇𝑖 from each modified SPN 𝑡 ′', metadata={'source': '/content/document4.pdf'}), -0.06342871396480487)]\n",
            "  warnings.warn(\n",
            "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'query': 'What is Planning in case of Private SPN Construction?',\n",
              " 'result': \"\\n<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nUse the following context to answer the question at the end. Do not use any other information. If you can't find the relevant information in the context, just say you don't have enough information to answer the question. Don't try to make up an answer.\\n\\nAlgorithm 2: Private SPN Construction PrivSPN(𝑇 , 𝜖) : table 𝑇 , total privacy budget 𝜖. Input Output : A tree of SPN 𝑡 = (parent, left, right)\\n\\n1 op, 𝜖op, ¯𝜖 ← Planning(𝑇 , 𝜖); 2 parent, ( (cid:101)S𝐿, (cid:101)S𝑅) ← ParentGen(𝑇 , op, 𝜖op); 3 left, right ← ChildrenGen(𝑇 , op, (cid:101)S𝐿, (cid:101)S𝑅, ¯𝜖); 4 return (parent, left, right) 5 procedure ParentGen(𝑇 , op, 𝜖op) if op = OP.LEAF then 6 Δ(hist) (cid:101)𝐻 ← hist(𝑇 ) + Lap( 𝜖op else if op = OP.SUM then\\n\\n); parent ← LeafNode( (cid:101)𝐻 );\\n\\n7\\n\\n8\\n\\n9\\n\\n10\\n\\n11\\n\\n(cid:101)S𝐿, (cid:101)S𝑅 ← RowSplit(𝑇 , 𝜖op ); parent ← SumNode( (cid:101)S𝐿, (cid:101)S𝑅 ); else if op = OP.PRODUCT then (cid:101)S𝐿, (cid:101)S𝑅 ← ColSplit(𝑇 , 𝜖op ); parent ← ProdNode( (cid:101)S𝐿, (cid:101)S𝑅 );\\n\\n12\\n\\nreturn parent, ( (cid:101)S𝐿, (cid:101)S𝑅)\\n\\n13 procedure ChildrenGen(𝑇 , op, (cid:101)S𝐿, (cid:101)S𝑅, ¯𝜖)\\n\\n14\\n\\n15\\n\\nif op = OP.SUM then 𝜖𝑙 ← ¯𝜖, 𝜖𝑟 ← ¯𝜖\\n\\n16\\n\\nelse if op = OP.PRODUCT then\\n\\n17\\n\\n𝜖𝑙 ← ¯𝜖 ·\\n\\n𝜎 (𝑇 [ (cid:101)S𝐿 ] ) 𝜎 (𝑇 [ (cid:101)S𝐿 ] )+𝜎 (𝑇 [ (cid:101)S𝑅 ] )\\n\\n, 𝜖𝑟 ← ¯𝜖 − 𝜖𝑙\\n\\n18\\n\\nif op = OP.LEAF then\\n\\n19\\n\\nleft ← null; right ← null;\\n\\n20\\n\\nelse\\n\\n21\\n\\nleft ← PrivSPN(𝑇 [ (cid:101)S𝐿 ], 𝜖𝑙 ); right ← PrivSPN(𝑇 [ (cid:101)S𝑅 ], 𝜖𝑟 );\\n\\n22\\n\\nreturn left, right\\n\\n4.2 Private SPN Construction 4.2.1 Overview of PrivSPN. Algorithm 2 outlines our process for constructing an SPN on a single table. Following DeepDB [13], we employ a tree-structured SPN to model a given table 𝑇 . The PrivSPN procedure recursively generates a binary tree 𝑡 = (parent, left, right), where parent should be a parent node, and left, right should be two subtrees. We divide PrivSPN into the following three procedures. • Planning (Operation Planning): We decide an operation op for the parent node and the privacy budget 𝜖op allocated to op. • ParentGen (Parent Generation): According to the operation op\\n\\nand the privacy budget 𝜖op, we generate the parent node.\\n\\nChildrenGen (Children generation): We further generate the chil- dren left, right through recursive calls to PrivSPN.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nWhat is Planning in case of Private SPN Construction?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n4.2.2 Operation Planning. In this section, we describe how to plan operations and allocate privacy budgets for each operation. For simplicity, we assume that the input table 𝑇 has no duplicate rows. We first define the notion of sensitivity of an operation. Definition 4.1 (Sensitivity). Let 𝑆 be a set of tables with the same schema as 𝑇. The sensitivity of an operation op ∈ {OP.SUM, OP.PROD} over 𝑆 is defined as follows:\\n\\n• For OP.SUM, 𝛾(op, 𝑆) = max{𝜎 (𝑇 ) | 𝑇 ∈ 𝑆}. • For OP.PROD, 𝛾(op, 𝑆) = max{𝜎 (𝑇 ) | 𝑇 ∈ 𝑆}, where 𝜎 (𝑇 ) =\\n\\n(cid:102)𝑥∈𝑇 𝑥(cid:103).\\n\\nWe also define the sensitivity of OP.LEAF as 𝛾(OP.LEAF, 𝑆) = 0. Note that the sensitivity of an operation depends on the set of tables it operates on. To simplify the discussion, we will omit the dependency on 𝑆 when there is no ambiguity. Given a table 𝑇, we want to construct an SPN that minimizes the overall error while satisfying the privacy constraint. However, since the exact error bound of an SPN is hard to compute, we instead aim to minimize the upper bound of the error. Specifically, we consider the following objective function:\\n\\n(cid:104)𝑂𝑝𝑐𝑜𝑠𝑡(𝑇 ) = 𝜖op + 𝛾(op, 𝑆)𝜀𝑚𝑎𝑥(𝑇 ), (4.1)\\n\\nwhere 𝜖op is the privacy budget allocated to op, and 𝜀𝑚𝑎𝑥(𝑇 ) is the maximum error of the SPN constructed from 𝑇. Intuitively, 𝜖op is the amount of noise added by op, and 𝛾(op, 𝑆)𝜀𝑚𝑎𝑥(𝑇 ) is the maximum error introduced by all children nodes under op. Therefore, 𝑂𝑝𝑐𝑜𝑠𝑡(\"}"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "query = \"What is Planning in case of Private SPN Construction?\"\n",
        "result = Chain_web.invoke(query)\n",
        "print(fill(result['result'].strip(), width=100))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_z3gmpYexRw"
      },
      "source": [
        "Question without context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3wsGS0B4zfIS",
        "outputId": "92373d5a-1f7f-4da8-ea11-f12166923b87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/vectorstores.py:330: UserWarning: Relevance scores must be between 0 and 1, got [(Document(page_content='1. GTX has a latch-free graph storage that uses atomic operations to update vertices and edges. It eliminates latching overheads and reduces thread idling.', metadata={'source': '/content/document3.pdf'}), -0.0015943138711989047), (Document(page_content='10\\n\\n[23]\\n\\nJungyeon Choi, Olaf M. Dekkers, and Saskia le Cessie. “A comparison of different methods to handle missing data in the context of propensity score analysis”. In: European Journal of Epidemiology 34 (1 Jan. 2019), pp. 23–36. ISSN: 15737284. DOI: 10.1007/s10654-018- 0447-z.\\n\\n[24] Thomas L. Saaty. Optimization in Integers and Re- lated Extremal Problems. Ed. by Donald K. Prentiss. McGraw-Hill, Inc, 1970, pp. 1–295. DOI: 10 . 2307 / 2316512.\\n\\n[25] Kelly A. Frazer et al. “A second generation human haplotype map of over 3.1 million SNPs”. In: Nature 449 (7164 Oct. 2007), pp. 851–861. ISSN: 00280836. DOI: 10.1038/nature06258.\\n\\n[26] Kevin Dunn. Kamyr digester-OpenMV.net Datasets. Oct. 2011. URL: https : / / openmv . net / info / kamyr - digester.\\n\\n[27] Bhupinder S Dayal et al. “Application of feedforward: least squares regression neural networks and partial for modelling kappa number in a continuous Kamyr digester: how multivariate data analysis might help pulping”. In: 1994.', metadata={'source': '/content/document5.pdf'}), -0.1059909618146222), (Document(page_content='GTX allocates a fixed number of worker threads that execute read-write transactions while additionally supports collectively running graph analytics using OpenMP [1]. Besides a commit man- ager, GTX has no other server thread. Transactions’ isolation and consistency ensures the correctness of concurrent operations. GTX read-write transactions support vertex inserts and updates, edge inserts, deletes, updates and lookups, and adjacency list scans. GTX guarantees atomicity of each transaction’s updates. GTX imple- ments all graph analytics under read-only transactions. While each', metadata={'source': '/content/document3.pdf'}), -0.11603270135343369), (Document(page_content='[5] Paul Madley-Dowd et al. “The proportion of missing data should not be used to guide decisions on multiple imputation”. In: Journal of Clinical Epidemiology 110 (June 2019), pp. 63–73. ISSN: 18785921. DOI: 10.1016/ j.jclinepi.2019.02.016.\\n\\n[6] Ben Omega Petrazzini et al. “Evaluation of different approaches for missing data imputation on features associated to genomic data”. In: BioData Mining 14 (1 Dec. 2021). ISSN: 17560381. DOI: 10.1186/s13040- 021-00274-7.\\n\\n[7] Donald B. Rubin. “INFERENCE AND MISSING DATA”. In: ETS Research Bulletin Series 1975 (1 June 1975), pp. i–19. ISSN: 2333-8504. DOI: 10.1002/J.2333- 8504.1975.TB01053.X. URL: https://onlinelibrary.wiley. com/doi/full/10.1002/j.2333-8504.1975.tb01053.x. [8] Andrew Briggs et al. “Missing.... presumed at random: cost-analysis of incomplete data”. In: Health Econ- comics 12 (5 2003), pp. 377–392. DOI: 10.1002/hec. 766. URL: https://onlinelibrary.wiley.com/doi/10.1002/ hec.766.', metadata={'source': '/content/document5.pdf'}), -0.11860155177017284)]\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/langchain_core/vectorstores.py:342: UserWarning: No relevant docs were retrieved using the relevance score threshold 0.2\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|begin_of_text|><|start_header_id|>system<|end_header_id|>  Use the following context to answer the\n",
            "question at the end. Do not use any other information. If you can't find the relevant information in\n",
            "the context, just say you don't have enough information to answer the question. Don't try to make up\n",
            "an answer.  <|eot_id|><|start_header_id|>user<|end_header_id|>  What are the risk factors of\n",
            "NVIDIA?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n"
          ]
        }
      ],
      "source": [
        "query = \"What are the risk factors of NVIDIA?\"\n",
        "result = Chain_web.invoke(query)\n",
        "print(fill(result['result'].strip(), width=100))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c26a230fe48b4e7bb62a9e20df4448d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b8851d1bd9484ed5a9e88942c26b86fe",
              "IPY_MODEL_7f98a8426ca5464fb7dab015fce3505f",
              "IPY_MODEL_23e7d12f3de7453c85ab468d6e986726"
            ],
            "layout": "IPY_MODEL_5722b55b64734b15bdfb872655a3024b"
          }
        },
        "b8851d1bd9484ed5a9e88942c26b86fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_47b5ecf26a164407836ecb81dcd82698",
            "placeholder": "​",
            "style": "IPY_MODEL_add5bbc6398142b3840ca97a65cc5b25",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "7f98a8426ca5464fb7dab015fce3505f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_532d6a79362a409fbd5becea59d8a8e6",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bd376012f67e41c790d405c8cd95f39b",
            "value": 4
          }
        },
        "23e7d12f3de7453c85ab468d6e986726": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_96e64dea42c241e7ac33aeac0cf50503",
            "placeholder": "​",
            "style": "IPY_MODEL_6d8429fad86c4df5932411a7a0baae13",
            "value": " 4/4 [01:09&lt;00:00, 15.02s/it]"
          }
        },
        "5722b55b64734b15bdfb872655a3024b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "47b5ecf26a164407836ecb81dcd82698": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "add5bbc6398142b3840ca97a65cc5b25": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "532d6a79362a409fbd5becea59d8a8e6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bd376012f67e41c790d405c8cd95f39b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "96e64dea42c241e7ac33aeac0cf50503": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6d8429fad86c4df5932411a7a0baae13": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}