{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9efdc6eedd304a598172acfe7a8c37ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_16c39b395191424ea1aa45791bdab659",
              "IPY_MODEL_65e632d707754b1ba93599938392cb19",
              "IPY_MODEL_dec530bee945482eaad04bbcd233d80c"
            ],
            "layout": "IPY_MODEL_5091be0dd4964ccc9d0a9b8d61b749cb"
          }
        },
        "16c39b395191424ea1aa45791bdab659": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cccff80914584ae3b97f1f07e025f376",
            "placeholder": "​",
            "style": "IPY_MODEL_1cdb861b652047b494eb0411612d5dc4",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "65e632d707754b1ba93599938392cb19": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5d6abb746ad9411b9002ae8b528bf672",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d131aeec73f240d3a5148f61d50753ae",
            "value": 4
          }
        },
        "dec530bee945482eaad04bbcd233d80c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b4f9fec3ef7d42b4996cf15e0ca06f7c",
            "placeholder": "​",
            "style": "IPY_MODEL_914e42282bff46ccb8c74995b8667ac5",
            "value": " 4/4 [01:11&lt;00:00, 15.44s/it]"
          }
        },
        "5091be0dd4964ccc9d0a9b8d61b749cb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cccff80914584ae3b97f1f07e025f376": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1cdb861b652047b494eb0411612d5dc4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5d6abb746ad9411b9002ae8b528bf672": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d131aeec73f240d3a5148f61d50753ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b4f9fec3ef7d42b4996cf15e0ca06f7c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "914e42282bff46ccb8c74995b8667ac5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rimmelb/AITAssignment7/blob/main/AIT_RAG_Assessment_ipynb_m%C3%A1solata.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RAG + LLM Assessment"
      ],
      "metadata": {
        "id": "e1xCp6n57BB2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Your task is to create a Retrieval-Augmented Generation (RAG) system using a Large Language Model (LLM). The RAG system should be able to retrieve relevant information from a knowledge base and generate coherent and informative responses to user queries."
      ],
      "metadata": {
        "id": "qmI3jBxK68-G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Steps:\n",
        "\n",
        "1. Choose a domain and collect a suitable dataset of documents (at least 5 documents - PDFs or HTML pages) to serve as the knowledge base for your RAG system. Select one of the following topics:\n",
        "   * latest scientific papers from arxiv.org,\n",
        "   * fiction books released,\n",
        "   * legal documentsor,\n",
        "   * social media posts.\n",
        "\n",
        "   Make sure that the documents are newer then the training dataset of the applied LLM. (20 points)\n",
        "\n",
        "2. Create three relevant prompts to the dataset, and one irrelevant prompt. (20 points)\n",
        "\n",
        "3. Load an LLM with at least 5B parameters. (10 points)\n",
        "\n",
        "4. Test the LLM with your prompts. The goal should be that without the collected dataset your model is unable to answer the question. If it gives you a good answer, select another question to answer and maybe a different dataset. (10 points)\n",
        "\n",
        "5. Create a LangChain-based RAG system by setting up a vector database from the documents. (20 points)\n",
        "\n",
        "6. Provide your three relevant and one irrelevant prompts to your RAG system. For the relevant prompts, your RAG system should return relevant answers, and for the irrelevant prompt, an empty answer. (20 points)\n"
      ],
      "metadata": {
        "id": "K3STgq_s6_mV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers>=4.32.0 optimum>=1.12.0 > null\n",
        "!pip install auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu118/ > null\n",
        "!pip install -i https://pypi.org/simple/ bitsandbytes > null\n",
        "!pip install spacy==3.7.4 typer==0.10.0 imageio==2.31.6 pillow==10.1.0\n",
        "!pip install langchain > null\n",
        "!pip install chromadb > null\n",
        "!pip install sentence_transformers > null # ==2.2.2\n",
        "!pip install unstructured > null\n",
        "!pip install pdf2image > null\n",
        "!pip install pdfminer.six > null\n",
        "!pip install unstructured-pytesseract > null\n",
        "!pip install unstructured-inference > null\n",
        "!pip install faiss-gpu > null\n",
        "!pip install pikepdf > null\n",
        "!pip install pypdf > null\n",
        "!pip install accelerate\n",
        "!pip install pillow_heif > null\n",
        "!pip install -i https://pypi.org/simple/ bitsandbytes"
      ],
      "metadata": {
        "id": "9nwufKe08D3e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd01286d-db71-4dfd-a3cd-556c5d7cc1e2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy==3.7.4 in /usr/local/lib/python3.10/dist-packages (3.7.4)\n",
            "Collecting typer==0.10.0\n",
            "  Using cached typer-0.10.0-py3-none-any.whl (46 kB)\n",
            "Requirement already satisfied: imageio==2.31.6 in /usr/local/lib/python3.10/dist-packages (2.31.6)\n",
            "Collecting pillow==10.1.0\n",
            "  Using cached Pillow-10.1.0-cp310-cp310-manylinux_2_28_x86_64.whl (3.6 MB)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy==3.7.4) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.7.4) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.7.4) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy==3.7.4) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy==3.7.4) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy==3.7.4) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy==3.7.4) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy==3.7.4) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy==3.7.4) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.7.4) (0.3.4)\n",
            "INFO: pip is looking at multiple versions of spacy to determine which version is compatible with other requirements. This could take a while.\n",
            "\u001b[31mERROR: Cannot install spacy==3.7.4 and typer==0.10.0 because these package versions have conflicting dependencies.\u001b[0m\u001b[31m\n",
            "\u001b[0m\n",
            "The conflict is caused by:\n",
            "    The user requested typer==0.10.0\n",
            "    spacy 3.7.4 depends on typer<0.10.0 and >=0.3.0\n",
            "\n",
            "To fix this you could try to:\n",
            "1. loosen the range of package versions you've specified\n",
            "2. remove package versions to allow pip attempt to solve the dependency conflict\n",
            "\n",
            "\u001b[31mERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/topics/dependency-resolution/#dealing-with-dependency-conflicts\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.30.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.2.1+cu121)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.23.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Looking in indexes: https://pypi.org/simple/\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.10/dist-packages (0.43.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (2.2.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.25.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->bitsandbytes) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->bitsandbytes) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ],
      "metadata": {
        "id": "DNYrA_xozOh5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig, pipeline, BitsAndBytesConfig\n",
        "from textwrap import fill\n",
        "from langchain.prompts import PromptTemplate\n",
        "import locale\n",
        "from langchain.document_loaders import UnstructuredURLLoader\n",
        "from langchain.vectorstores.utils import filter_complex_metadata # 'filter_complex_metadata' removes complex metadata that are not in str, int, float or bool format\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import RetrievalQA\n",
        "import bitsandbytes\n",
        "import accelerate"
      ],
      "metadata": {
        "id": "nrLsJ2aCzOkw"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "HUGGINGFACE_UAT=\"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\"\n",
        "login(HUGGINGFACE_UAT)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ddjN8-wU53mg",
        "outputId": "5c8cd876-a70c-4424-e313-73a302acdabd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: read).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#model_name = \"google/gemma-2b-it\" # 2B language model from Google\n",
        "\n",
        "import accelerate\n",
        "import bitsandbytes\n",
        "\n",
        "model_name = \"meta-llama/Meta-Llama-3-8B\" # 8B language model from Meta AI\n",
        "\n",
        "quan = BitsAndBytesConfig(load_in_8bit=True)\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
        "                                             device_map=\"auto\",\n",
        "                                             quantization_config=quan,\n",
        "                                             trust_remote_code=True)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "\n",
        "gen_cfg = GenerationConfig.from_pretrained(model_name)\n",
        "gen_cfg.max_new_tokens=512\n",
        "gen_cfg.temperature=0.0000001\n",
        "gen_cfg.return_full_text=True\n",
        "gen_cfg.do_sample=True\n",
        "gen_cfg.repetition_penalty=1.11\n",
        "\n",
        "pipe=pipeline(\n",
        "    task=\"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    generation_config=gen_cfg\n",
        ")\n",
        "\n",
        "llm = HuggingFacePipeline(pipeline=pipe)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205,
          "referenced_widgets": [
            "9efdc6eedd304a598172acfe7a8c37ee",
            "16c39b395191424ea1aa45791bdab659",
            "65e632d707754b1ba93599938392cb19",
            "dec530bee945482eaad04bbcd233d80c",
            "5091be0dd4964ccc9d0a9b8d61b749cb",
            "cccff80914584ae3b97f1f07e025f376",
            "1cdb861b652047b494eb0411612d5dc4",
            "5d6abb746ad9411b9002ae8b528bf672",
            "d131aeec73f240d3a5148f61d50753ae",
            "b4f9fec3ef7d42b4996cf15e0ca06f7c",
            "914e42282bff46ccb8c74995b8667ac5"
          ]
        },
        "id": "SoKKJrg-zOqA",
        "outputId": "6dd7ccbe-ed0a-4e2c-d77b-27438bd26828"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9efdc6eedd304a598172acfe7a8c37ee"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question without related context"
      ],
      "metadata": {
        "id": "WMwlm7eg5VkW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "template_gemma = \"\"\"\n",
        "<bos><start_of_turn>user\n",
        "{text}<end_of_turn>\n",
        "<start_of_turn>model\n",
        "\"\"\"\n",
        "\n",
        "template_llama3 = \"\"\"\n",
        "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
        "\n",
        "{text}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
        "\"\"\"\n",
        "\n",
        "if \"gemma\" in model_name:\n",
        "  template=template_gemma\n",
        "else:\n",
        "  template=template_llama3\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"text\"],\n",
        "    template=template,\n",
        ")\n"
      ],
      "metadata": {
        "id": "TCUEilpezOso"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text=\"What kind of Edge Read operations GTX support?\"\n",
        "result=llm(prompt.format(text=text))\n",
        "print(fill(result.strip(), width=100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GCp88uuL0H9a",
        "outputId": "536f81b3-4525-47da-b8ac-484c66ae2258"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|begin_of_text|><|start_header_id|>user<|end_header_id|>  What kind of Edge Read operations GTX\n",
            "support?<|eot_id|><|start_header_id|>assistant<|end_header_id|>  1. Read the value of a single\n",
            "register.  2. Read the value of multiple registers in one operation.  3. Read the value of all\n",
            "registers in one operation.  Answer: 1, 2 and 3 are supported.  What is the maximum number of\n",
            "registers that can be read in one operation? fıkır  Answer: The maximum number of registers that can\n",
            "be read in one operation is 32.  What is the maximum number of registers that can be written in one\n",
            "operation? fıkır  Answer: The maximum number of registers that can be written in one operation is\n",
            "32.  What is the maximum number of registers that can be read or written in one operation? fıkır\n",
            "Answer: The maximum number of registers that can be read or written in one operation is 64.  What is\n",
            "the maximum number of registers that can be read or written in one operation? fıkır  Answer: The\n",
            "maximum number of registers that can be read or written in one operation is 128.  What is the\n",
            "maximum number of registers that can be read or written in one operation? fıkır  Answer: The maximum\n",
            "number of registers that can be read or written in one operation is 256.  What is the maximum number\n",
            "of registers that can be read or written in one operation? fıkır  Answer: The maximum number of\n",
            "registers that can be read or written in one operation is 512.  What is the maximum number of\n",
            "registers that can be read or written in one operation? fıkır  Answer: The maximum number of\n",
            "registers that can be read or written in one operation is 1024.  What is the maximum number of\n",
            "registers that can be read or written in one operation? fıkır  Answer: The maximum number of\n",
            "registers that can be read or written in one operation is 2048.  What is the maximum number of\n",
            "registers that can be read or written in one operation? fıkır  Answer: The maximum number of\n",
            "registers that can be read or written in one operation is 4096.  What is the maximum number of\n",
            "registers that can be read or written in one operation? fıkır  Answer: The maximum number of\n",
            "registers that can be read or written in one operation is 8192.  What is the maximum number of\n",
            "registers that can be read or written in one operation? fıkır  Answer: The maximum number of\n",
            "registers that can be read or written in one operation is 16384.  What is the maximum number of\n",
            "registers that can be read or written in one operation? fıkır  Answer: The maximum number\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text=\"When is the generalized arithmetic progression homogeneous?\"\n",
        "result=llm(prompt.format(text=text))\n",
        "print(fill(result.strip(), width=100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u0cEntzL9btO",
        "outputId": "c3685688-e9ab-47c7-f62c-1a7cf57eb66f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|begin_of_text|><|start_header_id|>user<|end_header_id|>  When is the generalized arithmetic\n",
            "progression homogeneous?<|eot_id|><|start_header_id|>assistant<|end_header_id|> Theorem 1. Let be a\n",
            "sequence of positive integers such that for all. Then there exists an integer such that for all.\n",
            "Proof. We proceed by induction on. The base case is trivial, so we assume and suppose that holds for\n",
            "some. If, then since, it follows that. Otherwise, let be the largest index such that. Since, we\n",
            "have. Thus, if, then ; otherwise,. In either case, we have, as desired.  QED  Corollary 2. For any\n",
            "sequence of positive integers, there exist distinct indices and such that.  Proof. By Theorem 1,\n",
            "there exists an integer such that for all. Let and. Then and are distinct indices such that.  QED\n",
            "Theorem 3. Let be a sequence of positive integers such that for all. Then there exists an integer\n",
            "such that for all.  Proof. We proceed by induction on. The base case is trivial, so we assume and\n",
            "suppose that holds for some. If, then since, it follows that. Otherwise, let be the largest index\n",
            "such that. Since, we have. Thus, if, then ; otherwise,. In either case, we have, as desired.  QED\n",
            "Corollary 4. For any sequence of positive integers, there exist distinct indices and such that.\n",
            "Proof. By Theorem 3, there exists an integer such that for all. Let and. Then and are distinct\n",
            "indices such that.  QED\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text=\"What is Planning in case of Private SPN Construction?\"\n",
        "result=llm(prompt.format(text=text))\n",
        "print(fill(result.strip(), width=100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xXVOe2ec9b2O",
        "outputId": "2803b994-31ac-4deb-813a-370bfbafbe20"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|begin_of_text|><|start_header_id|>user<|end_header_id|>  What is Planning in case of Private SPN\n",
            "Construction?<|eot_id|><|start_header_id|>assistant<|end_header_id|> 1. The first step is to\n",
            "determine the type of private SPN construction you want to build. This will help you narrow down\n",
            "your options and make it easier to find a contractor who specializes in that particular type of\n",
            "construction. 2. Once you have determined the type of private SPN construction you want, research\n",
            "contractors in your area who specialize in that type of work. Look for reviews online or ask friends\n",
            "and family for recommendations. 3. Contact several contractors and get quotes from each one. Be sure\n",
            "to ask about their experience with similar projects, as well as any warranties they offer on their\n",
            "workmanship. 4. Compare the quotes you receive and choose the contractor who offers the best value\n",
            "for your money. Make sure to read all contracts carefully before signing anything! 5. Schedule an\n",
            "appointment with your chosen contractor so they can come out and assess your property and give you\n",
            "an estimate for the project.  What are the benefits of using a private SPN construction company?\n",
            "There are many benefits to using a private SPN construction company when building your dream home.\n",
            "Here are just a few: 1. You'll have more control over the design process - When working with a\n",
            "private builder, you'll be able to work closely with them throughout the entire design phase,\n",
            "ensuring that your vision for your new home comes to life exactly how you want it. 2. You'll save\n",
            "money - Because you're not paying for overhead costs associated with larger companies, you'll likely\n",
            "end up saving quite a bit of money by going with a smaller, private builder. 3. You'll get better\n",
            "customer service - Smaller builders tend to put more emphasis on providing excellent customer\n",
            "service, which means you'll always feel like you're being taken care of and that your needs are\n",
            "being met. 4. Your home will be built faster - Since there's no need to wait for other homes to be\n",
            "completed before starting yours, your home will likely be finished much sooner than if you went\n",
            "through a larger builder.  How do I find a reputable private SPN construction company? When looking\n",
            "for a reputable private SPN construction company, it is important to do your research. There are a\n",
            "number of ways to go about this, but some of the most effective methods include checking online\n",
            "reviews, asking around for referrals, and contacting local business associations.  Once you have\n",
            "compiled a list of potential candidates, take the time to interview each one in order to get a sense\n",
            "of their qualifications and experience. It is also important to request references from past clients\n",
            "so that you can get an idea of what kind of quality work they\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text=\"What are the risk factors of LG company?\"\n",
        "result=llm(prompt.format(text=text))\n",
        "print(fill(result.strip(), width=100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qjBo058-9csf",
        "outputId": "2b1a30b4-88fb-49a9-b81e-a633bbac14e2"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|begin_of_text|><|start_header_id|>user<|end_header_id|>  What are the risk factors of LG\n",
            "company?<|eot_id|><|start_header_id|>assistant<|end_header_id|> What is the difference between a 3D\n",
            "printer and a laser engraver?잡담 How to use the laser cutting machine to cut the metal plate?잡담 What\n",
            "is the difference between a laser cutter and a laser engraving machine?잡담 What is the difference\n",
            "between a laser engraving machine and a laser marking machine?잡담 What is the difference between a\n",
            "laser engraving machine and a laser cutting machine?잡담 What is the difference between a laser\n",
            "engraving machine and a laser marking machine?잡담 What is the difference between a laser engraving\n",
            "machine and a laser cutting machine?잡담 What is the difference between a laser engraving machine and\n",
            "a laser marking machine?잡담 What is the difference between a laser engraving machine and a laser\n",
            "cutting machine?잡담 What is the difference between a laser engraving machine and a laser marking\n",
            "machine?잡담 What is the difference between a laser engraving machine and a laser cutting machine?잡담\n",
            "What is the difference between a laser engraving machine and a laser marking machine?잡담 What is the\n",
            "difference between a laser engraving machine and a laser cutting machine?잡담 What is the difference\n",
            "between a laser engraving machine and a laser marking machine?잡담 What is the difference between a\n",
            "laser engraving machine and a laser cutting machine?잡담 What is the difference between a laser\n",
            "engraving machine and a laser marking machine?잡담 What is the difference between a laser engraving\n",
            "machine and a laser cutting machine?잡담 What is the difference between a laser engraving machine and\n",
            "a laser marking machine?잡담 What is the difference between a laser engraving machine and a laser\n",
            "cutting machine?잡담 What is the difference between a laser engraving machine and a laser marking\n",
            "machine?잡담 What is the difference between a laser engraving machine and a laser cutting machine?잡담\n",
            "What is the difference between a laser engraving machine and a laser marking machine?잡담 What is the\n",
            "difference between a laser engraving machine and a laser cutting machine?잡담 What is the difference\n",
            "between a laser engraving machine and a laser marking machine?잡담 What is the difference between a\n",
            "laser engraving machine and a laser cutting machine?잡담 What is the difference between a laser\n",
            "engraving machine and a laser marking machine?잡담 What is the difference between a laser engraving\n",
            "machine and a laser cutting machine?잡담 What is the difference between a laser engraving machine and\n",
            "a laser marking machine?잡담 What is the difference between a laser engraving machine and a\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "prompt_template_gemma = \"\"\"\n",
        "<bos><start_of_turn>user\n",
        "Use the following context to Answer the question at the end. Do not use any other information. If you can't find the relevant information in the context, just say you don't have enough information to answer the question. Don't try to make up an answer.\n",
        "\n",
        "{context}\n",
        "\n",
        "Question: {question}<end_of_turn>\n",
        "\n",
        "<start_of_turn>model\n",
        "\"\"\"\n",
        "\n",
        "prompt_template_llama3 = \"\"\"\n",
        "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
        "\n",
        "Use the following context to answer the question at the end. Do not use any other information. If you can't find the relevant information in the context, just say you don't have enough information to answer the question. Don't try to make up an answer.\n",
        "\n",
        "{context}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
        "\n",
        "{question}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
        "\"\"\"\n",
        "\n",
        "prompt_template=prompt_template_llama3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sQ6TY6MC4qqC",
        "outputId": "59f70a83-3bce-47ea-dd8c-b20f0e593034"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 4 µs, sys: 0 ns, total: 4 µs\n",
            "Wall time: 8.11 µs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O document1.pdf --no-check-certificate \"https://arxiv.org/pdf/2405.02269.pdf\"\n",
        "!wget -O document2.pdf --no-check-certificate \"https://arxiv.org/pdf/2405.01804.pdf\"\n",
        "!wget -O document3.pdf --no-check-certificate \"https://arxiv.org/pdf/2405.01448.pdf\"\n",
        "!wget -O document4.pdf --no-check-certificate \"https://arxiv.org/pdf/2405.01312.pdf\"\n",
        "!wget -O document5.pdf --no-check-certificate \"https://arxiv.org/pdf/2405.00764.pdf\"\n",
        "\n",
        "from langchain.document_loaders import UnstructuredPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "web_loader = [UnstructuredPDFLoader(pdf) for pdf in [\"/content/document1.pdf\",\"/content/document2.pdf\",\"/content/document3.pdf\", \"/content/document4.pdf\",\"/content/document5.pdf\"]]\n",
        "\n",
        "chunked_web_doc = []\n",
        "\n",
        "for loader in web_loader:\n",
        "  web_doc = loader.load()\n",
        "  up_web_doc = filter_complex_metadata(web_doc)\n",
        "  text_splitter = RecursiveCharacterTextSplitter(chunk_size=2048, chunk_overlap=512)\n",
        "  docs=text_splitter.split_documents(up_web_doc)\n",
        "  chunked_web_doc.extend(docs)\n",
        "\n",
        "len(chunked_web_doc)\n",
        "embeddings = HuggingFaceEmbeddings()\n",
        "db_web = FAISS.from_documents(chunked_web_doc, embeddings)"
      ],
      "metadata": {
        "id": "xc5t5ZStzOxj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "926586d4-38e3-4255-dcd4-f9408ba635d6"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-05-06 13:19:07--  https://arxiv.org/pdf/2405.02269.pdf\n",
            "Resolving arxiv.org (arxiv.org)... 151.101.67.42, 151.101.3.42, 151.101.131.42, ...\n",
            "Connecting to arxiv.org (arxiv.org)|151.101.67.42|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://arxiv.org/pdf/2405.02269 [following]\n",
            "--2024-05-06 13:19:08--  http://arxiv.org/pdf/2405.02269\n",
            "Connecting to arxiv.org (arxiv.org)|151.101.67.42|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 180861 (177K) [application/pdf]\n",
            "Saving to: ‘document1.pdf’\n",
            "\n",
            "document1.pdf       100%[===================>] 176.62K  --.-KB/s    in 0.004s  \n",
            "\n",
            "2024-05-06 13:19:08 (46.1 MB/s) - ‘document1.pdf’ saved [180861/180861]\n",
            "\n",
            "--2024-05-06 13:19:08--  https://arxiv.org/pdf/2405.01804.pdf\n",
            "Resolving arxiv.org (arxiv.org)... 151.101.67.42, 151.101.3.42, 151.101.131.42, ...\n",
            "Connecting to arxiv.org (arxiv.org)|151.101.67.42|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://arxiv.org/pdf/2405.01804 [following]\n",
            "--2024-05-06 13:19:08--  http://arxiv.org/pdf/2405.01804\n",
            "Connecting to arxiv.org (arxiv.org)|151.101.67.42|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 498781 (487K) [application/pdf]\n",
            "Saving to: ‘document2.pdf’\n",
            "\n",
            "document2.pdf       100%[===================>] 487.09K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2024-05-06 13:19:08 (27.7 MB/s) - ‘document2.pdf’ saved [498781/498781]\n",
            "\n",
            "--2024-05-06 13:19:08--  https://arxiv.org/pdf/2405.01448.pdf\n",
            "Resolving arxiv.org (arxiv.org)... 151.101.67.42, 151.101.3.42, 151.101.131.42, ...\n",
            "Connecting to arxiv.org (arxiv.org)|151.101.67.42|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://arxiv.org/pdf/2405.01448 [following]\n",
            "--2024-05-06 13:19:09--  http://arxiv.org/pdf/2405.01448\n",
            "Connecting to arxiv.org (arxiv.org)|151.101.67.42|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 845728 (826K) [application/pdf]\n",
            "Saving to: ‘document3.pdf’\n",
            "\n",
            "document3.pdf       100%[===================>] 825.91K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2024-05-06 13:19:09 (78.3 MB/s) - ‘document3.pdf’ saved [845728/845728]\n",
            "\n",
            "--2024-05-06 13:19:09--  https://arxiv.org/pdf/2405.01312.pdf\n",
            "Resolving arxiv.org (arxiv.org)... 151.101.67.42, 151.101.3.42, 151.101.131.42, ...\n",
            "Connecting to arxiv.org (arxiv.org)|151.101.67.42|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://arxiv.org/pdf/2405.01312 [following]\n",
            "--2024-05-06 13:19:09--  http://arxiv.org/pdf/2405.01312\n",
            "Connecting to arxiv.org (arxiv.org)|151.101.67.42|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1200816 (1.1M) [application/pdf]\n",
            "Saving to: ‘document4.pdf’\n",
            "\n",
            "document4.pdf       100%[===================>]   1.14M  --.-KB/s    in 0.01s   \n",
            "\n",
            "2024-05-06 13:19:09 (107 MB/s) - ‘document4.pdf’ saved [1200816/1200816]\n",
            "\n",
            "--2024-05-06 13:19:09--  https://arxiv.org/pdf/2405.00764.pdf\n",
            "Resolving arxiv.org (arxiv.org)... 151.101.67.42, 151.101.3.42, 151.101.131.42, ...\n",
            "Connecting to arxiv.org (arxiv.org)|151.101.67.42|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://arxiv.org/pdf/2405.00764 [following]\n",
            "--2024-05-06 13:19:10--  http://arxiv.org/pdf/2405.00764\n",
            "Connecting to arxiv.org (arxiv.org)|151.101.67.42|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3194831 (3.0M) [application/pdf]\n",
            "Saving to: ‘document5.pdf’\n",
            "\n",
            "document5.pdf       100%[===================>]   3.05M  --.-KB/s    in 0.02s   \n",
            "\n",
            "2024-05-06 13:19:10 (140 MB/s) - ‘document5.pdf’ saved [3194831/3194831]\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
        "Chain_web = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=db_web.as_retriever(search_type=\"similarity_score_threshold\", search_kwargs={'k': 10, 'score_threshold': 0.1}),\n",
        "    chain_type_kwargs={\"prompt\": prompt},\n",
        ")"
      ],
      "metadata": {
        "id": "HhndWobDzcHz"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What kind of Edge Read operations GTX support?\"\n",
        "result = Chain_web.invoke(query)\n",
        "result"
      ],
      "metadata": {
        "id": "03dP4Qg-zcNE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9db277b6-398f-4d46-962a-d3417b6a6b45"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'query': 'What kind of Edge Read operations GTX support?',\n",
              " 'result': '\\n<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nUse the following context to answer the question at the end. Do not use any other information. If you can\\'t find the relevant information in the context, just say you don\\'t have enough information to answer the question. Don\\'t try to make up an answer.\\n\\nGTX allocates a fixed number of worker threads that execute read-write transactions while additionally supports collectively running graph analytics using OpenMP [1]. Besides a commit man- ager, GTX has no other server thread. Transactions’ isolation and consistency ensures the correctness of concurrent operations. GTX read-write transactions support vertex inserts and updates, edge inserts, deletes, updates and lookups, and adjacency list scans. GTX guarantees atomicity of each transaction’s updates. GTX imple- ments all graph analytics under read-only transactions. While each\\n\\n3.2 Edge Write Operations GTX supports edge inserts, update and deletes, and they are han- dled similarly except having different delta types. GTX executes checked operations. An edge delete only takes place after deter- mining the edge’s existence. An edge insert only takes place after checking that the edge does not exist. Otherwise, the write oper- ation is executed as an update. 𝑡 inserts/updates/deletes an edge, say 𝑒 (𝑢, 𝑣), by creating an insert/update/delete edge-delta of 𝑒 (𝑢, 𝑣).\\n\\n2\\n\\nEdge-deltas blocks manage concurrency at the delta-chain level. 𝑡 calculates the delta-chain 𝑒 (𝑢, 𝑣) belongs to, say the 𝑖𝑡ℎ delta-chain, and locks it in the delta-chains index. After locking the delta-chain, 𝑡 searches the latest edge-delta(version) of 𝑒 (𝑢, 𝑣) using each edge- delta’s 𝑝𝑟𝑒𝑣𝑖𝑜𝑢𝑠_𝑜 𝑓 𝑓 𝑠𝑒𝑡, and writes 𝑡 as its invalidation timestamp if the edge-delta exists. Then, 𝑡 allocates memory by updating the combined_offset, and write the edge-delta in the corresponding loca- tions. The edge-delta stores the offset of the current 𝑖𝑡ℎ delta-chain head, and stores the offset of the previous edge-delta of 𝑒 (𝑢, 𝑣). Moreover, it stores 𝑡 as the new edge-delta’s creation timestamp. If 𝑡 fails to lock the 𝑖𝑡ℎ delta-chain, 𝑡 will abort.\\n\\nread-write transaction is executed by its creator worker thread, a read-only transaction can be executed by several OpenMP threads concurrently. GTX further implements a state protection protocol to support edge-deltas block consolidation when it becomes overflow. GTX adopts the block manager from [12] and manages garbage collection lazily. Memory blocks are recycled when no concurrent and future transactions can see them.\\n\\n3 GTX TRANSACTION OPERATIONS Each GTX transaction is assigned a read timestamp (𝑟𝑡𝑠) at its creation time from a global read epoch, and it does not know its write timestamp (𝑤𝑡𝑠) until it gets committed. GTX guarantees Snapshot Isolation [2] of its transactions. Let 𝑡 refer to both the transaction and its transaction ID interchangeably.\\n\\n3.1 Vertex Operations GTX transactions can insert new vertices and update them to new versions. Transaction 𝑡 reads a vertex 𝑣 by using an atomic 𝑙𝑜𝑎𝑑 on 𝑣’s delta-chains pointer after locating the 𝑣’s entry in the index. 𝑡 checks 𝑣’s delta-chain head, and compares its 𝑟𝑡𝑠 against the vertex- delta’s creation time. A transaction can see all deltas created before its 𝑟𝑡𝑠. If the current vertex-delta represents the future version, 𝑡 uses the 𝑝𝑟𝑒𝑣𝑖𝑜𝑢𝑠_𝑝𝑡𝑟 to read the previous vertex-delta (previous version) on the delta-chain. Vertex writes (insertion and updates) are handled by creating a new vertex-delta (new version), and invoking a 𝐶𝐴𝑆 on 𝑣’s delta-chain pointer. If the 𝐶𝐴𝑆 fails, 𝑡 observes write- write conflict and immediately abort.\\n\\nFigure 1: GTX Storage: 1.Vector-based vertex index (entry). 2.Vertex delta chain. 3.Edge-deltas block\\n\\nstorage is given in Figure 1. Each edge-delta takes 1 cache line to avoid false sharing and is optionally associated with a variable- sized property that either resides inside the delta or at the other end of the block depending on the property size. GTX stores vertex versions as vertex delta-chains, and each vertex’s adjacency list consecutively in a single edge-deltas block. Within each block, GTX organizes edge-deltas as delta-chains. Delta-chains support efficient single edge lookup and concurrency control and the linear edge- delta storage reduces random memory access, and improves cache performance during adjacency list scan. To support efficiently locat- ing a vertex’s delta-chain and edge-deltas block, GTX has a vertex index providing 𝑂 (1) lookup given vertex ID. Note that GTX sup- ports edge labels, and stores edge-deltas (edges) of the same label in an edge-deltas block, but for simplicity of this demonstration, we assume all edges have the same label. GTX manages a delta-chains index of each edge-deltas block. Each edge-deltas block determines its delta-chains number at its allocation time, and allocates a vector where each vector entry 𝑖 stores the offset of the 𝑖𝑡ℎ delta-chain within the block.\\n\\nGTX adopts multi-version concurrency control [11], and uses atomic operations to access and update vertex and edge versions, e.g. 𝑓 𝑒𝑡𝑐ℎ_𝑎𝑑𝑑, 𝑙𝑜𝑎𝑑, and 𝑐𝑜𝑚𝑝𝑎𝑟𝑒_𝑎𝑛𝑑_𝑠𝑤𝑎𝑝(𝐶𝐴𝑆). Vertex versions are updated by a 𝐶𝐴𝑆 on the vertex delta-chains pointer. Each edge- deltas block maintains a combined_offset that combines the offsets of the edge data region and edge-delta region (each 32 bits) into a 64 bits integer. Edge-delta allocation is done by executing a fetch_add to the combined_offset and write the edge-delta and its associated property data at the offsets returned by the operation.\\n\\n3.6 Transaction Example We present how GTX transactions lookup and update edges. We use 𝑡𝑠, 𝑡𝑠𝑐 , 𝑡𝑠𝑟 , 𝑡𝑠𝑐𝑟 , and 𝑡𝑠𝑖 to indicate timestamp, commit_timestamp, read_timestamp, creation_timestamp, and invalidation_timestamp, respectively.Figure 2 illustrates how transactions execute edge in- sert, delete, and update. Transactions and their modification to the edge-deltas block and delta-chains index are in the same color. The edge-deltas block for Vertex 1 originally stores the 3 edges (1, 12), (1, 3), and (1, 5) using 3 edge-deltas and 2 delta-chains. Delta-chains Index Entry 0 stores 𝑜 𝑓 𝑓 𝑠𝑒𝑡 = 64 that points to (1, 12), and Entry 1 stores 𝑜 𝑓 𝑓 𝑠𝑒𝑡 = 192 pointing to (1, 5). Transaction 𝑇1 inserts a new edge (1, 8) with 32 bytes of property data. 𝑇1 calculates that Edge (1, 8) belongs to Delta-Chain 0 = 8 mod 2 where 2 is the number of delta-chains, and uses the 0𝑡ℎ delta-chains index entry 64 to locate the edge-deltas chain, and searches for a previous version of (1, 8). Then, it allocates space in both the delta region and the data region by updating combined_offset atomically. Next, it writes the new in- sert edge-delta and its property into the allocated region. Note that the new edge-delta has previous_offset=64 and previous_version=0 because it needs to point to the previous edge-delta on the delta- chain, and Edge (1, 8) has no previous version. At commit phase, 𝑇1 updates delta-chains index entry to point to the new edge-delta, and the commit manager commits 𝑇1 at 𝑡𝑠𝑐 = 14. The edge-delta’s 𝑡𝑠𝑐𝑟 gets updated to 14. Next, Transaction 𝑇2 deletes Edge (1, 5). 𝑇2 uses the delta-chains index entry 1 to find the previous version of (1, 5) at offset 196. Then, it allocates the space for its edge-delta by updating the combined_offset. Since 𝑇2 is deleting (1, 5), it does not allocate space in the data region. 𝑇2 creates its delete edge-delta, and stores offset 192 as the new delta’s previous version. 𝑇2 com- mits in a similar way by updating the delta-chains index Entry 1 and registers with the commit manager.\\n\\n3.3 Edge Read Operation GTX supports two types of edge read operations: Single-edge lookup and adjacency list scan. Single-edge lookup uses the delta-chains index and is identical as checking whether an edge exists in an edge write operation. Adjacency list scan is implemented as scan- ning the whole edge-deltas block. After locating the edge-deltas block, 𝑡 loads the combined_offset. The combined_offset determines where in the edge-deltas block the scan starts. 𝑡 scans all edge-deltas starting from there, and can see all edge-deltas created before and invalidated after (or not invalidated at all) 𝑡’s 𝑟𝑡𝑠.\\n\\n3.4 Hybrid Commit GTX supports a hybrid group commit protocol. GTX manages a transaction table that provides 𝑂 (1) access time of looking up a transaction’s status. A transaction registers a commit request by in- forming the commit manager. The commit manager keeps a global write epoch number, and assigns its value as the 𝑤𝑡𝑠 to the commit group of transactions by updating their status in the transaction ta- ble. After committing each group, the commit manager increments the global read epoch and global write epoch by 1. The committing transactions eagerly find and update their deltas’ timestamps with their commit timestamp. Concurrent transactions may observe a delta’s timestamp is a committed transaction (ID) by querying the transaction table, and update the delta’s timestamp for the com- mitting transaction. We call this cooperative commit protocol hybrid commit. It reduces group commit latency, and saves cost in synchronizing committing transactions and commit manager.\\n\\n3.5 Block Consolidation and Garbage Collection Edge-deltas blocks may become overflowed and require allocating a new block for the most recent edge-deltas while recycling the old block with the outdated edge-deltas. GTX uses a latch-free state protection protocol inspired by [10] for each edge-deltas block that eliminates latches in shared memory and preserves cache coherence. Its core is analyzing the edge-deltas block, allocating a new block according to workload history, and migrating the latest version edge-deltas to the new block while concurrent transactions can still read the original edge-deltas block. After consolidating an edge- deltas block, the old deltas block is considered outdated and is placed in a queue. GTX tracks timestamps of current running transactions, and periodically has its worker threads free the memory blocks no transactions can access.\\n\\n3\\n\\nGTX Sortledton LiveGraph\\n\\nPR 4365903 3568179 640138\\n\\nSSSP 4265851 3490729 630250\\n\\nPR hotspot 4959926 1364880 418388\\n\\nSSSP hotspot 4265851 N/A 405232\\n\\nTable 3: Mixed-Workload Throughput(txns/s)\\n\\n[7] Alexandru Iosup and et al. 2016. LDBC Graphalytics: A Benchmark for Large- Scale Graph Analysis on Parallel and Distributed Platforms. 9, 13 (2016), 12. Jérôme Kunegis. [n.d.]. The KONECT Project. http://konect.cc/\\n\\n[8] [9] Changji Li and et al. 2022. ByteGraph: A High-Performance Distributed Graph\\n\\nDatabase in ByteDance. Proc. VLDB Endow. 15, 12 (2022).\\n\\n[10] Tianyu Li, Badrish Chandramouli, and Samuel Madden. 2022. Performant Almost- Latch-Free Data Structures Using Epoch Protection. In Data Management on New Hardware (DaMoN’22). Article 1, 10 pages.\\n\\nGTX Sortledton LiveGraph\\n\\nPR 15993070 10525814 13720317\\n\\nSSSP 22575448 15560585 21072157\\n\\nPR hotspot 14020837 10391524 10633277\\n\\nSSSP hotspot 18414454 N/A 18571507\\n\\n[11] Yingjun Wu, Joy Arulraj, Jiexi Lin, Ran Xian, and Andrew Pavlo. 2017. An Empirical Evaluation of In-Memory Multi-Version Concurrency Control. Proc. VLDB Endow. 10, 7 (2017). https://doi.org/10.14778/3067421.3067427\\n\\n[12] Xiaowei Zhu and et al. 2020. LiveGraph: A Transactional Graph Storage System with Purely Sequential Adjacency List Scans. 13, 7 (mar 2020), 1020–1034.\\n\\nTable 4: Mixed-Workload Latency(microsecond)\\n\\n4\\n\\nuse \"temporal localities\" to collectively refer to graph updates with both temporal localities and hotspots. State-of-the-art transactional graph systems use coarse-grained concurrency control (e.g., vertex- centric locking [6, 12]) and suffer from these update patterns [6]. To address these challenges and requirements, a graph system needs to support transactions, adapt to the temporal localities, and reduce the interference between concurrent transactions and analytics.\\n\\nThis demo presents GTX, a main memory latch-free transac- tional graph data system that is optimized for read-write transac- tions while maintaining competitive graph analytics. It targets the open problem of maintaining high throughput in workloads involv- ing temporal localities [6]. Many real world graphs are power-law graphs with hub vertices (vertices with large degree) [6, 12]. For example, one of the real world graph edit-wiki has vertices with millions of adjacent edges and the edges’ creation follows temporal localities and contains hotspots [6, 8]. Most of the existing transac- tional graph systems only support transactions, but are unware of temporal localities. Moreover, they suffer from concurrently run- ning updates and graph analytics. Two of the three state-of-the-art systems that we have evaluated have some bugs under such con- current workloads. Therefore, we design GTX with the following features to resolve these challenges:\\n\\n1. GTX has a latch-free graph storage that uses atomic operations to update vertices and edges. It eliminates latching overheads and reduces thread idling.\\n\\n2. GTX combines chain-based delta storage and linear delta storage to benefit from delta-chains’ efficient lookup and linear storage’s cache performance.\\n\\n3. GTX has an efficient transaction management and concurrency control protocol by managing concurrency control at delta-chain level that adopts to the workload history.\\n\\n4. GTX has a hybrid group commit protocol that improves trans-\\n\\naction commit throughput.\\n\\nIn this demo, we will present how GTX handles vertex and edge creation transactions of real world power-law graphs with and with- out temporal localities and how GTX executes read-write transac- tions while concurrently executing graphs analytics using synthetic graph logs in Graph Framework Evaluation(GFE) [5, 6].\\n\\n2 OVERVIEW OF GTX GTX addresses the problem of managing and analyzing dynamic la- beled property graphs. It consists of a latch-free adjacency list-based graph store and a transaction manager and a concurrency control protocol. GTX can manage both uniform and power-law graphs but is optimized for power-law graphs as many real-world graphs are power-law graphs [12]. In this demonstration, we focus on manag- ing power-law graphs. GTX’s graph store is a multi-version delta store, where each delta captures an operation (insert, update) of a vertex or (insert, delete, update) of an edge. An overview of GTX’s\\n\\nFigure 1: GTX Storage: 1.Vector-based vertex index (entry). 2.Vertex delta chain. 3.Edge-deltas block\\n\\nDataset yahoo-songs edit-wiki graph500-24\\n\\nCategory real-world real-world synthetic\\n\\nVertex Count Edge Count 256,804,235 572,591,272 260,379,520\\n\\n1,625,951 50,757,442 8,870,942\\n\\nTable 1: Dataset Statistics\\n\\n4 DEMONSTRATION AND EVALUATION We demonstrate GTX’s abilities to ingest real-world graphs with hotspots and to efficiently execute concurrent read-write transac- tions and graph analytics. We evaluate GTX’s performance against other state-of-the-art transactional graph systems, illustrating GTX’s high throughput in transactions, and robustness across workloads while maintaining competitive graph analytics performance.\\n\\n4.1 Demonstration Scenario We showcase GTX’s high transaction throughput for workloads with temporal localities, while maintaining competitive graph an- alytics performance. We compare GTX’s transaction throughput\\n\\nFigure 2: Examples\\n\\nand graph analytics latency against state-of-the-art transactional graph systems.We adopt the Graph Framework4 Evaluation (GFE) from Teseo [5] and Sortledton [6] for evaluation and demonstration. We use two real-world power-law graph datasets yahoo-songs(y-s) and edit-wiki(e-w) [8] and two sets of synthetic graph update logs (2.6 billion entries each) generated by [4] on graph500-24 [7]. One graph log shuffles edge updates and the other contains temporal localities. The statistics of the datasets can be found in Table 1.\\n\\nWe execute the demonstration on a dual-socket machine with Intel(R) Xeon(R) Platinum 8368 CPU @ 2.40GHz processors with 156 CPU and 192GB of DRAM. We run all evaluations in a single NUMA node if the memory size fits. All systems execute checked operations as transactions, checking an edge’s existence before performing the write. This demonstration only considers undirected graphs and each system treats an undirected edge as two directed edges. Therefore, for each edge 𝑒 (𝑢, 𝑣), each system creates a transaction that checks whether 𝑒 (𝑢, 𝑣) and 𝑒 (𝑣, 𝑢) exist, and inserts both edges.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nWhat kind of Edge Read operations GTX support?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\nSingle-edge lookupAdjacency list scan\\n\\nWhat kind of Edge Write operations GTX support?InsertUpdateDelete\\n\\nHow do GTX handle edge-deltas overflow?Block consolidationGarbage collection\\n\\nWhat is GTX\\'s concurrency control mechanism?Delta-chain based\\n\\nWhat is GTX\\'s commit protocol?Hybrid commit\\n\\nWhat is GTX\\'s transaction model?Snapshot isolation\\n\\nWhat is GTX\\'s storage structure?Multi-version delta storage\\n\\nWhat is GTX\\'s storage layout?Linear edge-delta storage\\n\\nWhat is GTX\\'s concurrency control granularity?Vertex and edge\\n\\nWhat is GTX\\'s concurrency control type?Optimistic\\n\\nWhat is GTX\\'s concurrency control algorithm?Multi-version concurrency control\\n\\nWhat is GTX\\'s concurrency control implementation?Atomic operations\\n\\nWhat is GTX\\'s storage structure?Multi-version delta storage\\n\\nWhat is GTX\\'s storage layout?Linear edge-delta storage\\n\\nWhat is GTX\\'s concurrency control granularity?Vertex and edge\\n\\nWhat is GTX\\'s concurrency control type?Optimistic\\n\\nWhat is GTX\\'s concurrency control algorithm?Multi-version concurrency control\\n\\nWhat is GTX\\'s concurrency control implementation?Atomic operations\\n\\nWhat is GTX\\'s storage structure?Multi-version delta storage\\n\\nWhat is GTX\\'s storage layout?Linear edge-delta storage\\n\\nWhat is GTX\\'s concurrency control granularity?Vertex and edge\\n\\nWhat is GTX\\'s concurrency control type?Optimistic\\n\\nWhat is GTX\\'s concurrency control algorithm?Multi-version concurrency control\\n\\nWhat is GTX\\'s concurrency control implementation?Atomic operations\\n\\nWhat is GTX\\'s storage structure?Multi-version delta storage\\n\\nWhat is GTX\\'s storage layout?Linear edge-delta storage\\n\\nWhat is GTX\\'s concurrency control granularity?Vertex and edge\\n\\nWhat is GTX\\'s concurrency control type?Optimistic\\n\\nWhat is GTX\\'s concurrency control algorithm?Multi-version concurrency control\\n\\nWhat is GTX\\'s concurrency control implementation?Atomic operations\\n\\nWhat is GTX\\'s storage structure?Multi-version delta storage\\n\\nWhat is GTX\\'s storage layout?Linear edge-delta storage\\n\\nWhat is GTX\\'s concurrency control granularity?Vertex and edge\\n\\nWhat is GTX\\'s concurrency control type?Optimistic\\n\\nWhat is GTX\\'s concurrency control algorithm?Multi-version concurrency control\\n\\nWhat is GTX\\'s concurrency control implementation?Atomic operations\\n\\nWhat is GTX\\'s storage structure?Multi-version delta storage\\n\\nWhat is GTX\\'s storage layout?Linear edge-delta storage\\n\\nWhat is GTX\\'s concurrency control granularity?Vertex and edge\\n\\nWhat is GTX\\'s concurrency control type?Optimistic\\n\\nWhat is GTX\\'s concurrency control algorithm?Multi-version concurrency control\\n\\nWhat is GTX\\'s concurrency control implementation?Atomic operations\\n\\nWhat is GTX\\'s storage'}"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"When is the generalized arithmetic progression homogeneous?\"\n",
        "result = Chain_web.invoke(query)\n",
        "result"
      ],
      "metadata": {
        "id": "zEUzruLFzfDS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56571592-29bd-4703-b3fb-aa2f5b721695"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/vectorstores.py:330: UserWarning: Relevance scores must be between 0 and 1, got [(Document(page_content='}m,k\\n\\n} × {\\n\\n{ be a set of positive integers, where A(n) >\\n\\nLet A =\\n\\na1 < a2 < . . .\\n\\n{\\n\\n}\\n\\nn1/2f (n), where f (n) tends to inﬁnite as n tends to inﬁnity.\\n\\nb1 < b2 < . . . am} × { F S(Y ) contains well-structured subsets.\\n\\nFurthermore let B =\\n\\n{\\n\\npositive integers. Let Y =\\n\\n{\\n\\nbe an inﬁnite sequence of\\n\\n} bk}m,k\\n\\nN. We will prove that ∈\\n\\nDeﬁnition (Finite generalized arithmetic progression). A ﬁnite gen-\\n\\neralized arithmetic progression or brieﬂy a GAP of dimension D is the\\n\\nset of the form\\n\\n{\\n\\nx0 + l1d1 + l2d2 +\\n\\n· ·\\n\\n+ lDdD : 0 < li ≤\\n\\nLi; i = 1, 2, . . . , D\\n\\n}\\n\\n,\\n\\nN (i = 1, 2, . . . , D). The GAP is homogeneous if\\n\\nwhere xi, li, di ∈ x0 = 0. The size of GAP is L1L2 · · · cardinality of the set above is equal of its size.\\n\\nLD, and GAP is proper if the\\n\\nTheorem 4.1. Let T > T0 be an integer.\\n\\n1. For every (L1, L2, . . . , LD) D homogeneous GAP with dimension D and size L1L2 · · · in F S(Y ). −\\n\\ntuples there exists a proper and\\n\\nLD containing\\n\\nON THE STRUCTURES OF SUBSET SETS IN HIGHER DIMENSION\\n\\n15\\n\\n2. There are inﬁnitely many rectangular R such that ∩\\n\\n2. There are inﬁnitely many rectangular R such that ∩\\n\\n2. There are inﬁnitely many rectangular R such that F S(Y ) | F S(Y ) |\\n\\nB(T ) T . In particular if lim supT\\n\\nB(T ) T > 0 then\\n\\nR | holds inﬁnitely many times.\\n\\n|\\n\\n∩\\n\\nR\\n\\n| ≫ R\\n\\n| ≫ |\\n\\n|\\n\\n(Recall that\\n\\nS |\\n\\n|\\n\\ndenotes the number of lattice points of S).\\n\\nProof of Theorem 4.1. First we prove 1.\\n\\nIn the ﬁrst step we prove the existence of an arithmetic progression\\n\\nof length L1. This will be GAP 1. Then successively if GAP i is deﬁned\\n\\nthen we show the existence of GAP (i + 1)\\n\\nGAP i. In order for the\\n\\n⊇\\n\\ni j=1 Ljdj. − AD,\\n\\n1\\n\\nith step GAP i to be proper, it is enough to prove that di > Consider the disjoint union of the sequence A = A1 ⊔ where Ai(n) > √nf (n)/D holds for i = 1, 2, . . . , D. GAP i will be\\n\\nP A2 ⊔ · · · ⊔\\n\\nconstructed on\\n\\n⊔\\n\\ni j=1Aj.\\n\\nWe now construct GAP 1 in A1. Let n be an integer for which f 2(n)/2D2 > L1. Write shortly L = L1. A1(n) > √nf (n)/D and let\\n\\nA′n =', metadata={'source': '/content/document1.pdf'}), 0.42260676216094506), (Document(page_content='Lemma 4.2 does not hold) and B(n) tends arbitrarily slowly to inﬁn-\\n\\nity. We just didn’t want to complicate our formulation.\\n\\nWe ensured arbitrary long arithmetic progression in F S(Y ). The\\n\\nquestion is, does it contain an inﬁnite arithmetic progression?\\n\\nON THE STRUCTURES OF SUBSET SETS IN HIGHER DIMENSION\\n\\n19\\n\\nAgain the answer depends on the ’slope’ of the arithmetic progres-\\n\\nsions (compare with Proposition 1.1). The following example shows\\n\\nthat a ”horizontal” arithmetic progression is possible.\\n\\nExample. Let S be the sequence of square numbers. It is known that n\\n\\nis expressible as a sum of 5 distinct non-zero squares provided n\\n\\n(see e.g. [B]). Let Z = S\\n\\n× {\\n\\n1\\n\\n}\\n\\n. Clearly\\n\\n{\\n\\n(n, 5) : n\\n\\n≥\\n\\n1024\\n\\n} ⊆\\n\\n1024\\n\\n≥ F S(Z).\\n\\nSo in the rest we exclude arithmetic progressions which are ’parallel’\\n\\nto the two axes. In this case, we can say no more:\\n\\nProposition 4.3. For every sequence of integers A, there exists an\\n\\ninﬁnite sequence of integers B such that F S(A\\n\\n×\\n\\nB) does not contain\\n\\nan inﬁnite arithmetic progression.\\n\\nProof. Let us enumerate the set of arithmetic progressions (they form\\n\\na countable set). Write them as AP1, AP2, . . . . For each APi we deﬁne a bi ∈ N 7→\\n\\nB. Let b1 any positive integers. We deﬁne a function α1 :\\n\\nN for which for all (n, y)\\n\\nF S(A\\n\\ny (note that\\n\\nB), α(n)\\n\\n×\\n\\n∈\\n\\n≥ B), when n is\\n\\nnot only for one y rather for each (n, y)\\n\\nF S(A\\n\\n∈\\n\\n×\\n\\nﬁxed). Represent n in the way when the term(n) is maximal. So\\n\\nn = ai1 + ai2 +\\n\\n· ·\\n\\n+ ait ≥\\n\\n1 + 2 + . . . t, hence max term(n)\\n\\n≤\\n\\n2√n.\\n\\nHence α1(n) = 2b1√n is an appropriate function. If AP1 lies on the line R, such that for every x > x0, g(x) = y = mx + b then there is an x0 ∈\\n\\n20\\n\\nON THE STRUCTURES OF SUBSET SETS IN HIGHER DIMENSION\\n\\ng(x) > 2b1√x . Furthermore, there exists an explicit computable x1 ∈ N such that between (x0, g(x0)) and (x1, g(x1)) the line contains at\\n\\nN } The further elements of B are deﬁned in a similar way; if bi has\\n\\nm > g(x1) : m\\n\\nleast ten elements from AP1. Now let b2 = min\\n\\n{\\n\\n∈\\n\\n.\\n\\nbeen deﬁned then for all (n, m)', metadata={'source': '/content/document1.pdf'}), 0.20781112284341974), (Document(page_content='2k\\n\\nbe split into sum of earlier elements, if i > i0. Par excellence\\n\\n{\\n\\nthe Fibonacci sequence e.t.c are splittable.\\n\\nIn section 4 we are going to study the structure of the subset sum am}\\n\\nof Z =\\n\\nbk}m,k\\n\\nam} × {\\n\\nN where ∈\\n\\nis denser. We show that un-\\n\\n{\\n\\n{ der some condition F S(Z) contains generalized arithmetic progressions\\n\\nand ’dense’ rectangles.\\n\\nThe diﬃculty of ﬁnding F S(A\\n\\nB) structures is to match equal\\n\\n× parts between A and B elements and to show the desired structures.\\n\\nIn the present work we can avoid this tool used in [BHP] and [GFH]\\n\\n(the so-called ”Matching Principle”).\\n\\n1.1. Thin subset sums. Denote by fi the ith axis of Nk, i.e. N }\\n\\nlet (recall that N denotes the positive\\n\\n(0, . . . , 0, αi, 0, . . . 0) : αi ∈ integers). In section 2 we prove\\n\\nfi =\\n\\n{\\n\\nON THE STRUCTURES OF SUBSET SETS IN HIGHER DIMENSION\\n\\nProposition 1.1. Let z\\n\\n∈\\n\\nNk and let R = z + Nk. Suppose that A is\\n\\ncomplete with respect to R. Then all projections of A onto fi are thick.\\n\\nNote that in the proposition above the boundary of the domain R\\n\\ncontains half-lines parallel to the axis. In the next theorem, we will\\n\\nshow that this was the only reason for a region to have thin subset\\n\\nsums.\\n\\nNk are integer vectors, and they Theorem 1.2. Suppose v1, . . . , vk ∈ are in general position in Rn. In addition, suppose none of vi is parallel\\n\\nto a coordinate axis. Let U be the cone generated by them, that is,\\n\\nRk\\n\\nU :=\\n\\n0\\n\\n+ akvk ∈\\n\\na1, . . . , ak ≥\\n\\n,\\n\\na1v1 +\\n\\n| Nn be the collection of all integer points in it. Let X =\\n\\n· ·\\n\\n{\\n\\n}\\n\\nand V = U\\n\\n∩ (X1∪· · ·∪\\n\\nS\\n\\nXk), where S is the set of all integer points in the simplex\\n\\n∪\\n\\ngenerated by kv1, . . . , kvk, and Xi =\\n\\n{\\n\\nvi, 2vi, 4vi, 8vi, . . . }\\n\\n. Then X is\\n\\nthin, and V = FS(X).\\n\\n2. Proof of Proposition 1.1 and Theorem 1.2\\n\\nProof of Proposition 1.1. Suppose that, contrary to the assumption,', metadata={'source': '/content/document1.pdf'}), 0.185230358697829), (Document(page_content='4 2 0 2\\n\\ny a M 3\\n\\n]\\n\\nO C . h t a m\\n\\n[\\n\\n1 v 9 6 2 2 0 . 5 0 4 2 : v i X r a\\n\\nON THE STRUCTURES OF SUBSET SETS IN HIGHER DIMENSION\\n\\nNORBERT HEGYV ´ARI, M ´AT´E P ´ALFY, AND ERFEI YUE\\n\\nAbstract. A given subset A of natural numbers is said to be complete if every element of N is the sum of distinct terms taken from A. This topic is strongly\\n\\nconnected to the knapsack problem which is known to be NP complete. The main\\n\\ngoal of the paper is to study the structure of subset sums in a higher dimension. We\\n\\nshow ’dense’ sets and generalized arithmetic progrssions in subset sums of certain\\n\\nsets.\\n\\nAMS 2010 Primary 11B30, 11B39, Secondary 11B75\\n\\nKeywords: Structure of subset sums, thin bases, Szemer´edi-Vu, Chen theorem\\n\\non complete sequences, generalized arithmetic progression, matching problems\\n\\n1. Introduction\\n\\nThe classical result of Lagrange’s theorem states that the squares\\n\\nform a base of order 4, i.e. every integer can be written as the sum of\\n\\nfour squares.\\n\\nLet us N denote the set of positive integers, i.e. let N =\\n\\n. } N is a basis, of order h if every suﬃciently large integer\\n\\n1, 2, . . .\\n\\n{\\n\\nThe set A\\n\\n⊆\\n\\ncan be represented as the sum of exactly h not necessarily distinct\\n\\nelements of A. An easy counting argument shows that if A is a bases\\n\\nof order h then lim infn\\n\\n→∞\\n\\nto be a thin basis if lim supn\\n\\nA(n) n1/h > 0. The basis A of order h is said 1, 4, 9, . . .\\n\\nA(n) n1/h <\\n\\n. Let S =\\n\\n∞\\n\\n{\\n\\n→∞\\n\\n1\\n\\n}\\n\\n2\\n\\nON THE STRUCTURES OF SUBSET SETS IN HIGHER DIMENSION\\n\\ndenote the squares. Wirsing showed in [W] that there exists an ’almost’\\n\\nthin bases of squares, proving that there exists an S′\\n\\n⊆\\n\\nS such that\\n\\n(x log x)1/4 and S′ is a bases of order four. (f\\n\\nS′ | the Vinogradov notation; it means that there is an absolute constant\\n\\n[1, x]\\n\\n∩\\n\\n| ≪\\n\\n≪\\n\\ng is\\n\\nC such that\\n\\nf |\\n\\n| ≤\\n\\nC\\n\\ng |\\n\\n. q |\\n\\n≫\\n\\nf means f\\n\\n≪\\n\\ng). Spencer also gave a\\n\\nquick proof of this statement (see [S]). For thin bases see Nathanson’s\\n\\npaper [4] and the references therein to other authors.\\n\\nFor any X\\n\\n⊆\\n\\nNk let\\n\\n(1)\\n\\nF S(X) :=\\n\\n{\\n\\n∞\\n\\nXi=1', metadata={'source': '/content/document1.pdf'}), 0.13190667172402804), (Document(page_content='GAP 1 has been deﬁned on\\n\\n⊔\\n\\ni+1 j=1Aj. First we use the sequence Ai+1. Actually we follow the previous argument. Our requirement now is that let f 2(n)/4D2 >\\n\\n1) on\\n\\n⊔\\n\\ni\\n\\nmax {\\n\\nj=1 djLj, Li+1}\\n\\n. As before we get an integer xi+1 for which the\\n\\nP\\n\\nnumber of representation of xi+1 in the form a + a′, a, a′ at least f 2(n)/2D2 > 2 max {\\n\\nAi+1 is\\n\\n∈\\n\\ni\\n\\nj=1 djLj, Li+1} x, x > 2r(x) we get that di+1 := xi+1 > max {\\n\\n. Since for all integer j=1 djLj, Li+1}\\n\\nP\\n\\ni\\n\\nand\\n\\nP\\n\\nwe can select an arithmetic progression of length Li+1. Denote this\\n\\narithmetic progression by AP (i + 1). Finally note that GAP (i + 1) =\\n\\ni j=1 djLj, we can avoid having two diﬀerent representations in the GAP (i + 1), which\\n\\nGAP i + AP (i + 1). Since for every i\\n\\n1 di+1 >\\n\\n≥\\n\\nP\\n\\nproves the ﬁrst part of the theorem.\\n\\nON THE STRUCTURES OF SUBSET SETS IN HIGHER DIMENSION\\n\\n17\\n\\nNote that in the present proof we use only that the set B is non-\\n\\nempty.\\n\\nNow we prove 2. For the proof, we need the following deep result of\\n\\nSzemer´edi and Vu, which Chen also proved independently. (see [SZV]\\n\\nand [C]):\\n\\nLemma 4.2. There is a positive constant c such that the following\\n\\nholds. Any increasing sequence X =\\n\\n{\\n\\nx1 < x2 < . . .\\n\\n}\\n\\nwith A(n) >\\n\\nc√n. We have that F S(X) contains an inﬁnite arithmetic progression.\\n\\nSplit A into two parts A = A1\\n\\nA2, where A1(n) > c√n for n > n0,\\n\\nand A2 = A\\n\\n\\\\\\n\\nF\\n\\nA1. By lemma 4.2 F S(A1) contains an inﬁnite arithmetic\\n\\nprogression, AP (d) with diﬀerence d. Choose the interval I = [1, H]\\n\\nsuch that the relative density of this arithmetic progression in I is at\\n\\nleast 1/2d, i.e.\\n\\nI |\\n\\n∩\\n\\nAP (d)\\n\\n|\\n\\n>\\n\\nI |\\n\\n/2d. For any x |\\n\\n∈\\n\\nI\\n\\n∩\\n\\nAP (d), denote\\n\\nby trm(x) the maximum number of members taken from A in the A1, εi ∈ AP (d) . } c)\\n\\nrepresentation F S(A1); (formally when x =\\n\\n∞i=1 εiai : ai ∈ I ∩\\n\\nP trm(x) : x\\n\\ni εi). Let Q = max {\\n\\nthen trm(x) =\\n\\n0, 1\\n\\n{ Choose AQ :=\\n\\n}\\n\\n∈\\n\\nP a1, a2, . . . , aQ} tuple exists. Let A′1 = A1 ∪\\n\\nfrom A2. Since A2(n) > √n(f (n)\\n\\n− Q aj + ≤ Q + 1.\\n\\n{\\n\\nAQ. Clearly I ′ :=\\n\\nthis Q\\n\\nj\\n\\n1 P I ′, trm(y)', metadata={'source': '/content/document1.pdf'}), 0.06789563978592561), (Document(page_content='187 pp. 233-249. , 17 p. (2018)\\n\\n[H1] Hegyv´ari, N. Complete sequences in N2 European Journal of Combinatorics (17) (1996) 741-\\n\\n749.\\n\\n[H2] Hegyv´ari, Thin complete subsequence. Ann. Univ. Sci. Budap. Rolando E¨otv¨os, Sect. Math.\\n\\n44, 151-156 (2001).\\n\\n[N] Nathanson, Melvyn B. Thin bases in additive number theory, Discrete Math. 312, No. 12-13,\\n\\n2069-2075 (2012)\\n\\n[S] Spencer, Joel (1996). ”Four Squares with Few Squares”. Number Theory: New York Seminar\\n\\n1991–1995. Springer US. pp. 295–297. doi:10.1007/978-1-4612-2418-1-22\\n\\n[SZV] E. Szemer´edi, V. Vu, Finite and inﬁnite arithmetic progressions in sumsets, Annals of Math-\\n\\nematics, 163 (2006), 1-35, https://doi.org/10.4007/annals.2006.163.1\\n\\n[W] Eduard Wirsing, Thin Subbases, Analysis 6 (1986), 285–30\\n\\nNorbert Hegyv´ari, ELTE TTK, E¨otv¨os University, Institute of Mathematics, H-\\n\\n1117 P´azm´any st. 1/c, Budapest, Hungary and Alfr´ed R´enyi Institute of Mathe-\\n\\nmatics, H-1364 Budapest, P.O.Box 127.\\n\\nEmail address: hegyvari@renyi.hu\\n\\nM´at´e P´alfy, ELTE TTK, E¨otv¨os University, Institute of Mathematics, H-1117\\n\\nP´azm´any st. 1/c, Budapest, Hungary\\n\\nEmail address: palfymateandras@gmail.com\\n\\nErfei Yue, ELTE TTK, E¨otv¨os University, Institute of Mathematics, H-1117\\n\\nP´azm´any st. 1/c, Budapest, Hungary\\n\\nEmail address: yef9262@mail.bnu.edu.cn\\n\\n21', metadata={'source': '/content/document1.pdf'}), 0.05377501254192674), (Document(page_content='· ·\\n\\nreplace vl + 2vl + there is no repeated elements in it. Hence Vi+1 ⊆ X(n) Finally, notice that |\\n\\n· ·\\n\\nFS(X).\\n\\nk i=1 |\\n\\nS\\n\\nXi(n)\\n\\n+\\n\\nc + k log2 n,\\n\\n|\\n\\n| ≤ | c′nk, for some constant c, c′ and as n\\n\\n| ≤\\n\\nP\\n\\nand\\n\\nV (n) |\\n\\n. Hence X is\\n\\n| ∼\\n\\n→ ∞\\n\\n(cid:3)\\n\\nthin and we ﬁnished the proof.\\n\\n3. On structure of reducible Subset sums of splittable sets in N2\\n\\nRecall a sequence A is splittable if any element ai can be split into\\n\\nsum of earlier elements, if i > i0. The simplest example for splittable\\n\\nset is the set of powers of 2.\\n\\nIn this section, then, we consider the subset sum structure of X\\n\\n2m\\n\\n2k\\n\\nwhere X :=\\n\\n∞k=0. Our argument can also be applied to other divisible sequences too. This study is motivated by a result from\\n\\n∞m=0 × {\\n\\n{\\n\\n}\\n\\n}\\n\\n[BHP].\\n\\n10\\n\\nON THE STRUCTURES OF SUBSET SETS IN HIGHER DIMENSION\\n\\nWe are going to show that there is a structured set which is contained\\n\\nin F S(X), but surprisingly there is also an exceptional set E. We will\\n\\nshow that the exceptional set contains an arbitrary large ’empty’ part\\n\\n(Proposition 3.2), and sometimes a ’dense’ part as well (Proposition\\n\\n3.3).\\n\\nProposition 3.1. Let E :=\\n\\na\\n\\n≤\\n\\nlog2 b\\n\\n}\\n\\n. Then (N2\\n\\n\\\\\\n\\nE)\\n\\n(a, b)\\n\\n∈ { F S(X).\\n\\n⊆\\n\\nN2 : b\\n\\n≤\\n\\nlog2 a\\n\\n} ∪ {\\n\\n(a, b)\\n\\n∈\\n\\nN2 :\\n\\nProof. Since the set X is reﬂected to the line y = x, it is enough to\\n\\nshow that every (a, b)\\n\\n∈\\n\\nN2\\n\\n\\\\\\n\\nE with b\\n\\n≤\\n\\na is an element of F S(X).\\n\\nDenote by n and m the number of terms in the dyadic expansion of a\\n\\nand b respectively. First assume that n\\n\\n≤\\n\\nm. Let a = 2c1 + ... + 2cn be\\n\\nthe dyadic expansion of a and b = 2d1 + ... + 2dm of b. Let us note that\\n\\nif ci > 0, then we can write\\n\\na = 2c1 + ... + 2ci−\\n\\n1 + 2ci−\\n\\n1 + ... + 2cn,\\n\\nwhich shows that a can be written as the sum of n + 1 powers of\\n\\ntwo. Repeating this process, we can write a as the sum of n + 1, n +\\n\\n2, ..., a many sums of powers of two (since a\\n\\n≥\\n\\nb this process is indeed\\n\\nterminates). Since n\\n\\n≤\\n\\nm we have that n\\n\\n≤\\n\\nm\\n\\n≤\\n\\na and hence we\\n\\ncan write a as the sum of powers of two such that we use exactly m', metadata={'source': '/content/document1.pdf'}), 0.023692446536185074), (Document(page_content='When p = q + 1, then it is easy to see that Rq(q + 1) = 0.\\n\\nTheorem 1.6.\\n\\n(a) For q ≥ 2,\\n\\nRq(q + 2) =\\n\\n(cid:18) 1 q\\n\\n(cid:19)q (cid:18) 1 2\\n\\n2) (cid:19)(q\\n\\nand Rq(q + 3) =\\n\\n(cid:18)1 q\\n\\n(cid:19)q (cid:18) 1 2\\n\\n(cid:19)(⌊q/2⌋\\n\\n2 ) 2 )+(⌈q/2⌉\\n\\n.\\n\\n(b) For q ≥ 3,\\n\\nRq(q + 4) =\\n\\n(cid:18) 1 q\\n\\n(cid:19)q (cid:18) 1 2\\n\\n(cid:19)(⌊q/3⌋\\n\\n2 )+(⌊(q+1)/3⌋\\n\\n2\\n\\n) )+(⌊(q+2)/3⌋\\n\\n2\\n\\n.\\n\\nObserve Theorems 1.2, 1.3, 1.4, and 1.5 suggest a “periodic behavior” based on the parity of p for Rq(p) when p is large enough compared to q. In [5], the authors conjectured that this behavior should occur for all p ≥ 2q + 1. In Section 3 we show that this conjecture does not hold in general, but we can prove that it holds for p ≥ 5q.\\n\\nTheorem 1.7. Let q ≥ 5 and p ≥ 5q. If p = 2h ≥ 5q is even, then\\n\\nRq(2h) = max 0≤x≤1\\n\\n1 2\\n\\n(cid:16) x 2\\n\\n(cid:17)2 (cid:18)h − 2\\n\\n3\\n\\n(cid:19) (cid:18) 1 − x h − 2\\n\\n(cid:19)q−2\\n\\n+x\\n\\n(cid:18)h − 2 4\\n\\n(cid:19) (cid:18) 1 − x h − 2\\n\\n(cid:19)q−1\\n\\n+\\n\\n(cid:18)h − 2 5\\n\\n(cid:19) (cid:18) 1 − x h − 2\\n\\nIf p = 2h + 1 ≥ 5q is odd, then\\n\\nRq(2h + 1) =\\n\\n(cid:18) 1 h\\n\\n(cid:19)q (cid:18)h 5\\n\\n(cid:19)\\n\\n.\\n\\nIn Section 2 we describe a general construction that will be used to prove the lower bounds in the stated theorems. In Section 3 we discuss counterexamples to a conjecture stated in [5] on the behavior of Rq(p). In Section 4 we introduce our main tool: a weighted version of Zykov’s theorem. In Section 5 we establish the upper bounds in Theorems 1.2, 1.3, 1.4, 1.5, 1.6, and 1.7.\\n\\nOrganization.', metadata={'source': '/content/document2.pdf'}), 0.0028445079202713552), (Document(page_content='20\\n\\nON THE STRUCTURES OF SUBSET SETS IN HIGHER DIMENSION\\n\\ng(x) > 2b1√x . Furthermore, there exists an explicit computable x1 ∈ N such that between (x0, g(x0)) and (x1, g(x1)) the line contains at\\n\\nN } The further elements of B are deﬁned in a similar way; if bi has\\n\\nm > g(x1) : m\\n\\nleast ten elements from AP1. Now let b2 = min\\n\\n{\\n\\n∈\\n\\n.\\n\\nbeen deﬁned then for all (n, m)\\n\\nF S(A\\n\\n< bi}\\n\\nb1 <\\n\\n) we have\\n\\n∈\\n\\n· ·\\n\\n× { 1 bj)√n and then the deﬁnition of bi+1 is similar as\\n\\n(\\n\\nm\\n\\ni\\n\\n1 P\\n\\nj\\n\\n≤ before.\\n\\n≤\\n\\n≤\\n\\n−\\n\\nAssume that the sequence B has been deﬁned. Consider say an\\n\\nB), x0 ≤ ∈ of the representation of m then m ≥\\n\\nelement (n, m)\\n\\nF S(A\\n\\n×\\n\\nn\\n\\nx1. If any bi, i\\n\\n≤ b2. If just b1, then m\\n\\n≥\\n\\nThe given ten point in the AP1 are not elements of F S(A\\n\\n2 is a term\\n\\n×\\n\\n2b1√n.\\n\\n≤ B).\\n\\nThe further argument is the same for any APi.\\n\\n(cid:3)\\n\\nAcknowledgment\\n\\nThe ﬁrst named author is supported by the National Research, De-\\n\\nvelopment and Innovation Oﬃce NKFIH Grant No K-146387.\\n\\nReferences\\n\\n[B] P.T.Bateman, A.J.Hildebrand, and G.B.Purdy: Sums of distinct squares, Acta Arith. 67:4\\n\\n(1994), pp. 349–380.\\n\\n[BHP] Bakos B., Hegyv´ari N., P´alfy M. On a communication complexity problem in combinatorial\\n\\nnumber theory Moscow Journal of Combinatorics and Number Theory 10 : 4 pp. 297-302. , 6\\n\\np. (2022)\\n\\nON THE STRUCTURES OF SUBSET SETS IN HIGHER DIMENSION\\n\\n[C] Yong-Gao Chen, On subset sums of a ﬁxed set Acta Arithmetica 106 (2003), 207-211 DOI:\\n\\n10.4064/aa106-3-1\\n\\n[GFH] Chen, YG ; Fang, JH ; Hegyv´ari, N Erd˝os-Birch type question in Nr J. of Number Theory\\n\\n187 pp. 233-249. , 17 p. (2018)\\n\\n[H1] Hegyv´ari, N. Complete sequences in N2 European Journal of Combinatorics (17) (1996) 741-\\n\\n749.\\n\\n[H2] Hegyv´ari, Thin complete subsequence. Ann. Univ. Sci. Budap. Rolando E¨otv¨os, Sect. Math.\\n\\n44, 151-156 (2001).\\n\\n[N] Nathanson, Melvyn B. Thin bases in additive number theory, Discrete Math. 312, No. 12-13,\\n\\n2069-2075 (2012)\\n\\n[S] Spencer, Joel (1996). ”Four Squares with Few Squares”. Number Theory: New York Seminar', metadata={'source': '/content/document1.pdf'}), -0.001527721850551833), (Document(page_content='The instances of G(n; s, t) that appear in Table 1 can be checked that they match the corresponding values of Rq(p) in Theorems 1.2, 1.3, 1.4, and 1.5 thus providing lower bounds in those proofs.\\n\\n6\\n\\np\\n\\nq\\n\\n2\\n\\n3\\n\\n4\\n\\n5\\n\\n4\\n\\n5\\n\\n6\\n\\n7\\n\\n8\\n\\n9\\n\\n10\\n\\n11\\n\\n12\\n\\n2s + 1 ≥ 13\\n\\nG(n; s, s)\\n\\nG(n; s, s)\\n\\nG(n; s, s)\\n\\nG(n; s, s)\\n\\n2s ≥ 14\\n\\nG(n; s, s − 1) G(n; s, s − 1) G(n; s, s − 1) G(n; s, s − 1)\\n\\nTable 1: Constructions for Rq(p). Gray indicates density 1/2 and green indicates density 1. 7\\n\\n3 Counterexamples\\n\\nAssume a graph in G(n; s, t) (asymptotically) achieves the maximum of RT(n, #Kq, Kp, αn) with p ≥ q + 2: it must have at least q cells, or else the number of Kq will be o(nq). In such a graph, the number of cells must always be at least the number of parts. One might expect that a Kp-free graph with the most copies of Kq would contain copies of Kp−1, and so the sum of the number of parts and cells should be p − 1. Subject to all these constraints, the authors of [5] conjectured that to maximize the number of Kq, we should take the number of parts t as large as possible. We restate their conjecture in our notation.\\n\\nConjecture 3.1. Given integers p > q ≥ 3, one of the asymptotically maximal graphs for RT(n, #Kq, Kp, αn) lies in G(n; q, p−q −1) when p ≤ 2q −1 and lies in G(n; ⌈(p−1)/2⌉, ⌊(p− 1)/2⌋) when p ≥ 2q.', metadata={'source': '/content/document2.pdf'}), -0.011307055110534536)]\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'query': 'When is the generalized arithmetic progression homogeneous?',\n",
              " 'result': \"\\n<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nUse the following context to answer the question at the end. Do not use any other information. If you can't find the relevant information in the context, just say you don't have enough information to answer the question. Don't try to make up an answer.\\n\\n}m,k\\n\\n} × {\\n\\n{ be a set of positive integers, where A(n) >\\n\\nLet A =\\n\\na1 < a2 < . . .\\n\\n{\\n\\n}\\n\\nn1/2f (n), where f (n) tends to inﬁnite as n tends to inﬁnity.\\n\\nb1 < b2 < . . . am} × { F S(Y ) contains well-structured subsets.\\n\\nFurthermore let B =\\n\\n{\\n\\npositive integers. Let Y =\\n\\n{\\n\\nbe an inﬁnite sequence of\\n\\n} bk}m,k\\n\\nN. We will prove that ∈\\n\\nDeﬁnition (Finite generalized arithmetic progression). A ﬁnite gen-\\n\\neralized arithmetic progression or brieﬂy a GAP of dimension D is the\\n\\nset of the form\\n\\n{\\n\\nx0 + l1d1 + l2d2 +\\n\\n· ·\\n\\n+ lDdD : 0 < li ≤\\n\\nLi; i = 1, 2, . . . , D\\n\\n}\\n\\n,\\n\\nN (i = 1, 2, . . . , D). The GAP is homogeneous if\\n\\nwhere xi, li, di ∈ x0 = 0. The size of GAP is L1L2 · · · cardinality of the set above is equal of its size.\\n\\nLD, and GAP is proper if the\\n\\nTheorem 4.1. Let T > T0 be an integer.\\n\\n1. For every (L1, L2, . . . , LD) D homogeneous GAP with dimension D and size L1L2 · · · in F S(Y ). −\\n\\ntuples there exists a proper and\\n\\nLD containing\\n\\nON THE STRUCTURES OF SUBSET SETS IN HIGHER DIMENSION\\n\\n15\\n\\n2. There are inﬁnitely many rectangular R such that ∩\\n\\n2. There are inﬁnitely many rectangular R such that ∩\\n\\n2. There are inﬁnitely many rectangular R such that F S(Y ) | F S(Y ) |\\n\\nB(T ) T . In particular if lim supT\\n\\nB(T ) T > 0 then\\n\\nR | holds inﬁnitely many times.\\n\\n|\\n\\n∩\\n\\nR\\n\\n| ≫ R\\n\\n| ≫ |\\n\\n|\\n\\n(Recall that\\n\\nS |\\n\\n|\\n\\ndenotes the number of lattice points of S).\\n\\nProof of Theorem 4.1. First we prove 1.\\n\\nIn the ﬁrst step we prove the existence of an arithmetic progression\\n\\nof length L1. This will be GAP 1. Then successively if GAP i is deﬁned\\n\\nthen we show the existence of GAP (i + 1)\\n\\nGAP i. In order for the\\n\\n⊇\\n\\ni j=1 Ljdj. − AD,\\n\\n1\\n\\nith step GAP i to be proper, it is enough to prove that di > Consider the disjoint union of the sequence A = A1 ⊔ where Ai(n) > √nf (n)/D holds for i = 1, 2, . . . , D. GAP i will be\\n\\nP A2 ⊔ · · · ⊔\\n\\nconstructed on\\n\\n⊔\\n\\ni j=1Aj.\\n\\nWe now construct GAP 1 in A1. Let n be an integer for which f 2(n)/2D2 > L1. Write shortly L = L1. A1(n) > √nf (n)/D and let\\n\\nA′n =\\n\\nLemma 4.2 does not hold) and B(n) tends arbitrarily slowly to inﬁn-\\n\\nity. We just didn’t want to complicate our formulation.\\n\\nWe ensured arbitrary long arithmetic progression in F S(Y ). The\\n\\nquestion is, does it contain an inﬁnite arithmetic progression?\\n\\nON THE STRUCTURES OF SUBSET SETS IN HIGHER DIMENSION\\n\\n19\\n\\nAgain the answer depends on the ’slope’ of the arithmetic progres-\\n\\nsions (compare with Proposition 1.1). The following example shows\\n\\nthat a ”horizontal” arithmetic progression is possible.\\n\\nExample. Let S be the sequence of square numbers. It is known that n\\n\\nis expressible as a sum of 5 distinct non-zero squares provided n\\n\\n(see e.g. [B]). Let Z = S\\n\\n× {\\n\\n1\\n\\n}\\n\\n. Clearly\\n\\n{\\n\\n(n, 5) : n\\n\\n≥\\n\\n1024\\n\\n} ⊆\\n\\n1024\\n\\n≥ F S(Z).\\n\\nSo in the rest we exclude arithmetic progressions which are ’parallel’\\n\\nto the two axes. In this case, we can say no more:\\n\\nProposition 4.3. For every sequence of integers A, there exists an\\n\\ninﬁnite sequence of integers B such that F S(A\\n\\n×\\n\\nB) does not contain\\n\\nan inﬁnite arithmetic progression.\\n\\nProof. Let us enumerate the set of arithmetic progressions (they form\\n\\na countable set). Write them as AP1, AP2, . . . . For each APi we deﬁne a bi ∈ N 7→\\n\\nB. Let b1 any positive integers. We deﬁne a function α1 :\\n\\nN for which for all (n, y)\\n\\nF S(A\\n\\ny (note that\\n\\nB), α(n)\\n\\n×\\n\\n∈\\n\\n≥ B), when n is\\n\\nnot only for one y rather for each (n, y)\\n\\nF S(A\\n\\n∈\\n\\n×\\n\\nﬁxed). Represent n in the way when the term(n) is maximal. So\\n\\nn = ai1 + ai2 +\\n\\n· ·\\n\\n+ ait ≥\\n\\n1 + 2 + . . . t, hence max term(n)\\n\\n≤\\n\\n2√n.\\n\\nHence α1(n) = 2b1√n is an appropriate function. If AP1 lies on the line R, such that for every x > x0, g(x) = y = mx + b then there is an x0 ∈\\n\\n20\\n\\nON THE STRUCTURES OF SUBSET SETS IN HIGHER DIMENSION\\n\\ng(x) > 2b1√x . Furthermore, there exists an explicit computable x1 ∈ N such that between (x0, g(x0)) and (x1, g(x1)) the line contains at\\n\\nN } The further elements of B are deﬁned in a similar way; if bi has\\n\\nm > g(x1) : m\\n\\nleast ten elements from AP1. Now let b2 = min\\n\\n{\\n\\n∈\\n\\n.\\n\\nbeen deﬁned then for all (n, m)\\n\\n2k\\n\\nbe split into sum of earlier elements, if i > i0. Par excellence\\n\\n{\\n\\nthe Fibonacci sequence e.t.c are splittable.\\n\\nIn section 4 we are going to study the structure of the subset sum am}\\n\\nof Z =\\n\\nbk}m,k\\n\\nam} × {\\n\\nN where ∈\\n\\nis denser. We show that un-\\n\\n{\\n\\n{ der some condition F S(Z) contains generalized arithmetic progressions\\n\\nand ’dense’ rectangles.\\n\\nThe diﬃculty of ﬁnding F S(A\\n\\nB) structures is to match equal\\n\\n× parts between A and B elements and to show the desired structures.\\n\\nIn the present work we can avoid this tool used in [BHP] and [GFH]\\n\\n(the so-called ”Matching Principle”).\\n\\n1.1. Thin subset sums. Denote by fi the ith axis of Nk, i.e. N }\\n\\nlet (recall that N denotes the positive\\n\\n(0, . . . , 0, αi, 0, . . . 0) : αi ∈ integers). In section 2 we prove\\n\\nfi =\\n\\n{\\n\\nON THE STRUCTURES OF SUBSET SETS IN HIGHER DIMENSION\\n\\nProposition 1.1. Let z\\n\\n∈\\n\\nNk and let R = z + Nk. Suppose that A is\\n\\ncomplete with respect to R. Then all projections of A onto fi are thick.\\n\\nNote that in the proposition above the boundary of the domain R\\n\\ncontains half-lines parallel to the axis. In the next theorem, we will\\n\\nshow that this was the only reason for a region to have thin subset\\n\\nsums.\\n\\nNk are integer vectors, and they Theorem 1.2. Suppose v1, . . . , vk ∈ are in general position in Rn. In addition, suppose none of vi is parallel\\n\\nto a coordinate axis. Let U be the cone generated by them, that is,\\n\\nRk\\n\\nU :=\\n\\n0\\n\\n+ akvk ∈\\n\\na1, . . . , ak ≥\\n\\n,\\n\\na1v1 +\\n\\n| Nn be the collection of all integer points in it. Let X =\\n\\n· ·\\n\\n{\\n\\n}\\n\\nand V = U\\n\\n∩ (X1∪· · ·∪\\n\\nS\\n\\nXk), where S is the set of all integer points in the simplex\\n\\n∪\\n\\ngenerated by kv1, . . . , kvk, and Xi =\\n\\n{\\n\\nvi, 2vi, 4vi, 8vi, . . . }\\n\\n. Then X is\\n\\nthin, and V = FS(X).\\n\\n2. Proof of Proposition 1.1 and Theorem 1.2\\n\\nProof of Proposition 1.1. Suppose that, contrary to the assumption,\\n\\n4 2 0 2\\n\\ny a M 3\\n\\n]\\n\\nO C . h t a m\\n\\n[\\n\\n1 v 9 6 2 2 0 . 5 0 4 2 : v i X r a\\n\\nON THE STRUCTURES OF SUBSET SETS IN HIGHER DIMENSION\\n\\nNORBERT HEGYV ´ARI, M ´AT´E P ´ALFY, AND ERFEI YUE\\n\\nAbstract. A given subset A of natural numbers is said to be complete if every element of N is the sum of distinct terms taken from A. This topic is strongly\\n\\nconnected to the knapsack problem which is known to be NP complete. The main\\n\\ngoal of the paper is to study the structure of subset sums in a higher dimension. We\\n\\nshow ’dense’ sets and generalized arithmetic progrssions in subset sums of certain\\n\\nsets.\\n\\nAMS 2010 Primary 11B30, 11B39, Secondary 11B75\\n\\nKeywords: Structure of subset sums, thin bases, Szemer´edi-Vu, Chen theorem\\n\\non complete sequences, generalized arithmetic progression, matching problems\\n\\n1. Introduction\\n\\nThe classical result of Lagrange’s theorem states that the squares\\n\\nform a base of order 4, i.e. every integer can be written as the sum of\\n\\nfour squares.\\n\\nLet us N denote the set of positive integers, i.e. let N =\\n\\n. } N is a basis, of order h if every suﬃciently large integer\\n\\n1, 2, . . .\\n\\n{\\n\\nThe set A\\n\\n⊆\\n\\ncan be represented as the sum of exactly h not necessarily distinct\\n\\nelements of A. An easy counting argument shows that if A is a bases\\n\\nof order h then lim infn\\n\\n→∞\\n\\nto be a thin basis if lim supn\\n\\nA(n) n1/h > 0. The basis A of order h is said 1, 4, 9, . . .\\n\\nA(n) n1/h <\\n\\n. Let S =\\n\\n∞\\n\\n{\\n\\n→∞\\n\\n1\\n\\n}\\n\\n2\\n\\nON THE STRUCTURES OF SUBSET SETS IN HIGHER DIMENSION\\n\\ndenote the squares. Wirsing showed in [W] that there exists an ’almost’\\n\\nthin bases of squares, proving that there exists an S′\\n\\n⊆\\n\\nS such that\\n\\n(x log x)1/4 and S′ is a bases of order four. (f\\n\\nS′ | the Vinogradov notation; it means that there is an absolute constant\\n\\n[1, x]\\n\\n∩\\n\\n| ≪\\n\\n≪\\n\\ng is\\n\\nC such that\\n\\nf |\\n\\n| ≤\\n\\nC\\n\\ng |\\n\\n. q |\\n\\n≫\\n\\nf means f\\n\\n≪\\n\\ng). Spencer also gave a\\n\\nquick proof of this statement (see [S]). For thin bases see Nathanson’s\\n\\npaper [4] and the references therein to other authors.\\n\\nFor any X\\n\\n⊆\\n\\nNk let\\n\\n(1)\\n\\nF S(X) :=\\n\\n{\\n\\n∞\\n\\nXi=1<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nWhen is the generalized arithmetic progression homogeneous?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\"}"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is Planning in case of Private SPN Construction?\"\n",
        "result = Chain_web.invoke(query)\n",
        "result"
      ],
      "metadata": {
        "id": "e15evr_tzfFy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca6fad4d-a94e-417c-828e-c0040eb9a440"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/vectorstores.py:330: UserWarning: Relevance scores must be between 0 and 1, got [(Document(page_content='Algorithm 2: Private SPN Construction PrivSPN(𝑇 , 𝜖) : table 𝑇 , total privacy budget 𝜖. Input Output : A tree of SPN 𝑡 = (parent, left, right)\\n\\n1 op, 𝜖op, ¯𝜖 ← Planning(𝑇 , 𝜖); 2 parent, ( (cid:101)S𝐿, (cid:101)S𝑅) ← ParentGen(𝑇 , op, 𝜖op); 3 left, right ← ChildrenGen(𝑇 , op, (cid:101)S𝐿, (cid:101)S𝑅, ¯𝜖); 4 return (parent, left, right) 5 procedure ParentGen(𝑇 , op, 𝜖op) if op = OP.LEAF then 6 Δ(hist) (cid:101)𝐻 ← hist(𝑇 ) + Lap( 𝜖op else if op = OP.SUM then\\n\\n); parent ← LeafNode( (cid:101)𝐻 );\\n\\n7\\n\\n8\\n\\n9\\n\\n10\\n\\n11\\n\\n(cid:101)S𝐿, (cid:101)S𝑅 ← RowSplit(𝑇 , 𝜖op ); parent ← SumNode( (cid:101)S𝐿, (cid:101)S𝑅 ); else if op = OP.PRODUCT then (cid:101)S𝐿, (cid:101)S𝑅 ← ColSplit(𝑇 , 𝜖op ); parent ← ProdNode( (cid:101)S𝐿, (cid:101)S𝑅 );\\n\\n12\\n\\nreturn parent, ( (cid:101)S𝐿, (cid:101)S𝑅)\\n\\n13 procedure ChildrenGen(𝑇 , op, (cid:101)S𝐿, (cid:101)S𝑅, ¯𝜖)\\n\\n14\\n\\n15\\n\\nif op = OP.SUM then 𝜖𝑙 ← ¯𝜖, 𝜖𝑟 ← ¯𝜖\\n\\n16\\n\\nelse if op = OP.PRODUCT then\\n\\n17\\n\\n𝜖𝑙 ← ¯𝜖 ·\\n\\n𝜎 (𝑇 [ (cid:101)S𝐿 ] ) 𝜎 (𝑇 [ (cid:101)S𝐿 ] )+𝜎 (𝑇 [ (cid:101)S𝑅 ] )\\n\\n, 𝜖𝑟 ← ¯𝜖 − 𝜖𝑙\\n\\n18\\n\\nif op = OP.LEAF then\\n\\n19\\n\\nleft ← null; right ← null;\\n\\n20\\n\\nelse\\n\\n21\\n\\nleft ← PrivSPN(𝑇 [ (cid:101)S𝐿 ], 𝜖𝑙 ); right ← PrivSPN(𝑇 [ (cid:101)S𝑅 ], 𝜖𝑟 );\\n\\n22\\n\\nreturn left, right\\n\\n4.2 Private SPN Construction 4.2.1 Overview of PrivSPN. Algorithm 2 outlines our process for constructing an SPN on a single table. Following DeepDB [13], we employ a tree-structured SPN to model a given table 𝑇 . The PrivSPN procedure recursively generates a binary tree 𝑡 = (parent, left, right), where parent should be a parent node, and left, right should be two subtrees. We divide PrivSPN into the following three procedures. • Planning (Operation Planning): We decide an operation op for the parent node and the privacy budget 𝜖op allocated to op. • ParentGen (Parent Generation): According to the operation op\\n\\nand the privacy budget 𝜖op, we generate the parent node.\\n\\nChildrenGen (Children generation): We further generate the chil- dren left, right through recursive calls to PrivSPN.', metadata={'source': '/content/document4.pdf'}), 0.10285122441649686), (Document(page_content='5.1 Privacy Analysis on PrivSPN In this subsection, we first prove that the Planning, ParentGen, and ChildrenGen procedures satisfy DP, and then prove that PrivSPN therefore achieves DP.\\n\\n5.1.1 Analysis of Planning. The Planning procedure consists of the CorrTrial, DecideOP, and AllocBudgetOP subprocedures. If the subprocedures satisfy DP, we can conclude that Planning ensures DP according to the sequential composition theorem of DP [9].\\n\\nCorrTrial. (1) When |𝑇 | ≥ 2𝛽 and |𝑇 .Attr| > 1, the CorrTrial proce- dure employs the 𝜖eval · 𝛾2-DP procedure ColSplit(𝑇 , 𝜖eval · 𝛾2) to (cid:98)S𝐿, (cid:98)S𝑅 (cid:1). Then, it calculates the NMI compute a perturbed partition (cid:0) Δ(NMI𝑐𝑜𝑙 ) 𝜖eval · (1−𝛾2 ) ) into it, of ( (cid:98)S𝐿, (cid:98)S𝑅) and injects the Laplace noise Lap( which ensures (cid:0)𝜖eval · (1 − 𝛾2)(cid:1)-DP. Consequently, according to the sequential composition theorem of DP [9], CorrTrial(𝑇 , 𝜖) satisfies 𝜖eval-DP. (2) When |𝑇 | < 2𝛽 or |𝑇 .Attr| = 1, since varying the value of any row in 𝑇 does not have any impact on the output (𝜎𝑐𝑜𝑙 , 𝜖eval), CorrTrial(𝑇 , 𝜖) satisfies 0-DP according to Definition 1. Note that we adopt the bounded DP interpretation; in the case of unbounded DP, adding/removing a row from 𝑇 causes the procedure to take the other conditional branch (i.e., Lines 11–14), thereby breaking the DP guarantee.\\n\\nDecideOP and AllocBudgetOP. Similar to the second case of CorrTrial, for both procedures DecideOP and AllocBudgetOP, because chang- ing the value of any row in 𝑇 does not affect the table size |𝑇 | and the table dimension |𝑇 .Attr|, it also does not impact their outputs. Therefore, they ensure 0-DP.\\n\\nPlanning. When |𝑇 | < 2𝛽 or |𝑇 .Attr| = 1, Planning satisfies 0-DP since any change to 𝑇 does not impact the output (Lines 1–2); otherwise, since it sequentially combines CorrTrial, DecideOP, and AllocBudgetOP, Planning(𝑇 , 𝜖) satisfies the 𝜖eval-DP according to the sequential composition theorem of DP [9], where the value of 𝜖eval differs in different cases (see Lines 12 and 16).', metadata={'source': '/content/document4.pdf'}), 0.08027467495575735), (Document(page_content='This work is licensed under the Creative Commons BY-NC-ND 4.0 International License. Visit https://creativecommons.org/licenses/by-nc-nd/4.0/ to view a copy of this license. For any use beyond those covered by this license, obtain permission by emailing info@vldb.org. Copyright is held by the owner/author(s). Publication rights\\n\\nlicensed to the VLDB Endowment. Proceedings of the VLDB Endowment, Vol. 17, No. 1 ISSN 2150-8097. doi:XX.XX/XXX.XX\\n\\n1\\n\\ncount: 1600Size765Price45.435.210.5\\n\\nID135…\\n\\nOriginal Table 𝑇\\n\\nSize358Price15.010.025.0……count: 160𝑇[ሚ𝒮𝐿]\\n\\nID245…\\n\\n×\\n\\n040.0\\n\\n×\\n\\nID245…count: 340Size469Price40.030.032.0\\n\\n110\\n\\n+\\n\\nID136…\\n\\n𝑝𝑒𝑟𝑡𝑢𝑟𝑏(ሚ𝒮𝐿,ሚ𝒮𝑅)Size486935…Price40.025.030.032.015.010.0…ID123456…count: 500\\n\\n×\\n\\n𝐻\\n\\n……\\n\\nSize765…count: 1600\\n\\n…Size271count: 3400…Price94.122.633.4\\n\\n+\\n\\n…(𝒮𝐿,𝒮𝑅)\\n\\nID136…\\n\\n1\\n\\ncount: 5000(b) Learning an SPN and Generating Datacount: 160count: 160count: 340count: 340\\n\\n040.0count: 1600Price45.435.210.5\\n\\n……𝑇[ሚ𝒮𝑅]\\n\\nPrivate SPN ConstructionSPN-Based Database Synthesis\\n\\ncount: 3400Size765271…Price45.435.210.594.122.633.4…\\n\\n110110\\n\\n×\\n\\nSize358…Price15.010.025.0…Price40.030.032.0…Size469…\\n\\ncount: 3400\\n\\n𝑝𝑒𝑟𝑡𝑢𝑟𝑏෩𝐻\\n\\nID246…\\n\\nSize271…Price94.122.633.4…\\n\\nData SamplingSynthetic Table \\u0de0𝑇\\n\\nFigure 1: Learning an SPN and generating data based on single-table.', metadata={'source': '/content/document4.pdf'}), 0.04052421195749567), (Document(page_content='and the privacy budget 𝜖op, we generate the parent node.\\n\\nChildrenGen (Children generation): We further generate the chil- dren left, right through recursive calls to PrivSPN.\\n\\n4.2.2 Planning. The Planning procedure takes the role of deter- mining an operation op for the parent node, which can be creating a leaf/sum/product node. A sum (resp. product) node indicates par- titioning the rows (resp. columns) of the given table, while a leaf node means generating the table’s histogram. Since all these types of operations should be performed in a differentially private man- ner, Planning also allocates a privacy budget 𝜖op for the parent\\n\\n5\\n\\nAlgorithm 3: Operation Planning Planning(𝑇 , 𝜖) Input Output : next operation op, privacy budget 𝜖op for op,\\n\\n: table 𝑇 , privacy budget 𝜖\\n\\nremaining privacy budget ¯𝜖\\n\\nParam: threshold for column splitting 𝛼, minimum table\\n\\nsize 𝛽, budget ratios 𝛾1, 𝛾2 1 if |𝑇 | < 2𝛽 and |𝑇 .Attr| = 1 then 2\\n\\nop ← OP.LEAF; 𝜖op ← 𝜖; ¯𝜖 ← 0;\\n\\n3 else 4\\n\\n5\\n\\n6\\n\\n𝛿𝑐𝑜𝑙 , 𝜖eval ← CorrTrial(𝑇 , 𝜖); op ← DecideOP(𝑇 , 𝛿𝑐𝑜𝑙 ); 𝜖op, ¯𝜖 ← AllocBudgetOP(𝑇 , op, 𝜖, 𝜖eval);\\n\\n7 return op, 𝜖op, ¯𝜖; 8 procedure CorrTrial(𝑇 , 𝜖) 9\\n\\nif |𝑇 | ≥ 2𝛽 and |𝑇 .Attr| > 1 then\\n\\n10\\n\\n11\\n\\n12\\n\\n𝜖eval ← 𝜖 · 𝛾1; ( (cid:98)S𝐿, (cid:98)S𝑅) ← ColSplit(𝑇 , 𝜖eval · 𝛾2); 𝛿𝑐𝑜𝑙 ← NMI(𝑇 [ (cid:98)S𝐿],𝑇 [ (cid:98)S𝑅]) + Lap(\\n\\nΔ(NMI) 𝜖eval · (1−𝛾2 ) );\\n\\n13\\n\\nelse\\n\\n14\\n\\n𝜖eval ← 0; 𝛿𝑐𝑜𝑙 ← 0;\\n\\nreturn 𝛿𝑐𝑜𝑙 , 𝜖eval 15 16 procedure DecideOP(𝑇 , 𝛿𝑐𝑜𝑙 ) 17\\n\\nif |𝑇 | ≥ 2𝛽 and |𝑇 .Attr| > 1 then\\n\\n18\\n\\nif 𝛿𝑐𝑜𝑙 ≤ 𝛼 then\\n\\n19\\n\\nop ← OP.PRODUCT\\n\\n20\\n\\nelse\\n\\n21\\n\\nop ← OP.SUM\\n\\n22\\n\\nelse if |𝑇 | ≥ 2𝛽 and |𝑇 .Attr| = 1 then\\n\\n23\\n\\nop ← OP.SUM\\n\\n24\\n\\nelse if |𝑇 | < 2𝛽 and |𝑇 .Attr| > 1 then\\n\\n25\\n\\nop ← OP.PRODUCT\\n\\n26\\n\\nreturn op\\n\\n27 procedure AllocBudgetOP(𝑇 , op, 𝜖, 𝜖eval)\\n\\n28\\n\\nif op = OP.PRODUCT and |𝑇 .Attr| = 2 then\\n\\n29\\n\\n𝜖op ← 0;\\n\\n30\\n\\nelse\\n\\n31\\n\\n32\\n\\n𝜖op ← 𝜖/2 − 𝜖eval; return 𝜖op, 𝜖 − 𝜖eval − 𝜖op', metadata={'source': '/content/document4.pdf'}), 0.03013484521216625), (Document(page_content='Example 2. Figure 3 shows an example of how PrivBench com- plements the original SPN 𝑡𝑖 with the leaf nodes of the foreign key Type-ID. First, we identify that attribute size attribute possesses the most leaf nodes in 𝑡𝑖 . For each leaf node of attribute size, we find the corresponding subcolumn of attribute Type − ID using the row indices of the subcolumn of attribute size and calculate its fanout table. Then, we perturb the fanout table and store it in a new leaf node. Finally, the new leaf node is added into the SPN as a sibling to the leaf node of attribute size and as the child of a newly added product node, resulting in a modified SPN 𝑡 ′ 𝑖 .\\n\\n4.4 SPN-Based Database Synthesis Algorithm 5 shows how we synthesize a table (cid:98)𝑇𝑖 given an SPN 𝑡𝑖 . If the root node of 𝑡𝑖 is a leaf node, we sample a table (cid:98)𝑇𝑖 ac- cording to the histogram or fanout table stored in the node (Lines 1–2). If it is a sum (product, resp.) node, we recursively apply the SampleDataFromSPN procedure to the left and right children of 𝑡𝑖 and vertically (horizontally, resp.) concatenate the returned tables (cid:98)𝑇𝐿 and (cid:98)𝑇𝑅 (Lines 4–9). Finally, we obtain a table (cid:98)𝑇𝑖 that has a data distribution similar to that of the private table 𝑇𝑖 . By synthesizing tables for all modified SPNs, we obtain a synthetic database (cid:98)𝐷.\\n\\n7\\n\\n5 PRIVACY ANALYSIS In this section, we rigoriously analyze how PrivBench satisfies DP and how we should allocate privacy budgets to some key steps in PrivBench.1\\n\\n5.1 Privacy Analysis on PrivSPN In this subsection, we first prove that the Planning, ParentGen, and ChildrenGen procedures satisfy DP, and then prove that PrivSPN therefore achieves DP.\\n\\n5.1.1 Analysis of Planning. The Planning procedure consists of the CorrTrial, DecideOP, and AllocBudgetOP subprocedures. If the subprocedures satisfy DP, we can conclude that Planning ensures DP according to the sequential composition theorem of DP [9].', metadata={'source': '/content/document4.pdf'}), -0.01292229093286612), (Document(page_content='Planning. When |𝑇 | < 2𝛽 or |𝑇 .Attr| = 1, Planning satisfies 0-DP since any change to 𝑇 does not impact the output (Lines 1–2); otherwise, since it sequentially combines CorrTrial, DecideOP, and AllocBudgetOP, Planning(𝑇 , 𝜖) satisfies the 𝜖eval-DP according to the sequential composition theorem of DP [9], where the value of 𝜖eval differs in different cases (see Lines 12 and 16).\\n\\nLemma 1. For any 𝜖 ≥ 0 and any 𝛾1 ≥ 0, if |𝑇 | ≥ 2𝛽 and |𝑇 .Attr| > 1, Planning(𝑇 , 𝜖) satisfies (𝜖 ·𝛾1)-DP; otherwise, it satisfies 0-DP at the table level.\\n\\n5.1.2 Analysis of ParentGen. Then, we show that in any cases of the given operation op, ParentGen achieves DP. (1) When op = OP.LEAF, we ensure DP using the Laplace mechanism. That is, Δ(hist) we inject Laplacian noise Lap( ) into the histogram hist(𝑇 ), 𝜖op which satisfies 𝜖op-DP. (2) When op = OP.SUM, we employ a DP row splitting mechanism RowSplit(𝑇 , 𝜖op) to generate a per- turbed row partition (cid:101)𝑆𝐿, (cid:101)𝑆𝑅, which achieves 𝜖op-DP. (3) When op = OP.PRODUCT, our column splitting mechanism ColSplit(𝑇 , 𝜖op) is essentially an instance of the Exponential mechanism, thereby guaranteeing 𝜖op-DP.\\n\\n1All missing proofs can be found in our online appendix: https://drive.google.com/file/ d/1mjhKOajBNvN5RbX7k_pvEoFi9DRCWuza/view?usp=sharing.\\n\\nLemma 2. For any op and any 𝜖op ≥ 0, ParentGen(𝑇 , op, 𝜖op)\\n\\nTheorem 4. Let T denote the set of all possibles subtables of 𝑇 with |𝑡 | < 2𝛽 and 𝑡 .Attr = 1, ∀𝑡 ∈ T . If for any 𝑡 ∈ T , PrivSPN(𝑇 , 𝜖) calls PrivSPN(𝑡, 𝜖leaf), PrivSPN(𝑇 , 𝜖) satisfies table-level 𝜖-DP, where 𝜎 (𝑇 ) = 2|𝑇 .Attr|+ |𝑇 |', metadata={'source': '/content/document4.pdf'}), -0.0489715019885979), (Document(page_content='14\\n\\n𝜖eval ← 0; 𝛿𝑐𝑜𝑙 ← 0;\\n\\nreturn 𝛿𝑐𝑜𝑙 , 𝜖eval 15 16 procedure DecideOP(𝑇 , 𝛿𝑐𝑜𝑙 ) 17\\n\\nif |𝑇 | ≥ 2𝛽 and |𝑇 .Attr| > 1 then\\n\\n18\\n\\nif 𝛿𝑐𝑜𝑙 ≤ 𝛼 then\\n\\n19\\n\\nop ← OP.PRODUCT\\n\\n20\\n\\nelse\\n\\n21\\n\\nop ← OP.SUM\\n\\n22\\n\\nelse if |𝑇 | ≥ 2𝛽 and |𝑇 .Attr| = 1 then\\n\\n23\\n\\nop ← OP.SUM\\n\\n24\\n\\nelse if |𝑇 | < 2𝛽 and |𝑇 .Attr| > 1 then\\n\\n25\\n\\nop ← OP.PRODUCT\\n\\n26\\n\\nreturn op\\n\\n27 procedure AllocBudgetOP(𝑇 , op, 𝜖, 𝜖eval)\\n\\n28\\n\\nif op = OP.PRODUCT and |𝑇 .Attr| = 2 then\\n\\n29\\n\\n𝜖op ← 0;\\n\\n30\\n\\nelse\\n\\n31\\n\\n32\\n\\n𝜖op ← 𝜖/2 − 𝜖eval; return 𝜖op, 𝜖 − 𝜖eval − 𝜖op\\n\\nnode and calculates the remaining privacy budget ¯𝜖 for the children. Concretely, Planning proceeds as follows. 1. Leaf node validation (Lines 1–2). We validate if we should only generate a leaf node for the given table. When both the size and the dimension of the given table are insufficient to split, i.e., |𝑇 | < 2𝛽 and |𝑇 .Attr| = 1, the operation must be creating a leaf node, i.e., ← OP.LEAF, and all the given privacy budget 𝜖 should be allocated for op, i.e., 𝜖op = 𝜖. Here, we require that the size of a split subtable should not be smaller than the parameter 𝛽 for the sake of utility, which is ensured by the row splitting algorithm RowSplit.\\n\\n2. Correlation trial (procedure CorrTrial). We preliminarily trial the performance of column splitting, which will be used to decide the operation op by the DecideOP procedure. Specifically, we consume a slight privacy budget 𝜖eval to generate a trial column partition ( (cid:98)S𝐿, (cid:98)S𝑅) (Line 11) and compute the perturbed correlation ),\\n\\nwhere NMI calculates the normalized mutual information of two subtables, Δ returns the global sensitvity of the given function, and Lap generates the Laplace noise (Line 12). Intuitively, the smaller the correlation between the two subtables, the smaller the impact of column splitting on the data distribution, hence the better the performance of column splitting [13]. Note that when either col- umn or row splitting is infeasible, we do not need to evaluate the correlation since the operation must be the feasible one (Line 14).', metadata={'source': '/content/document4.pdf'}), -0.05562210609622298), (Document(page_content='7.5 Case Study In this subsection, we conduct two case studies to demonstrate the performance of PrivBench in real-world scenarios.\\n\\nCase study 1: Real estate database. In this study, we select a real es- tate database [19] collected from Australia, which provides a rich and diverse array of property-related information, including details about the properties themselves, addresses, surrounding features, school district information, and more. The database comprises 12\\n\\n12\\n\\ntables and various common data types. We construct 10 complex queries and evaluate the performance of PrivBench in terms of Q- error, KL divergence, and execution time error. As shown in Table 6, PrivBench performs excellently across all metrics, meeting practi- cal needs. The query with the highest Q-error, targeting properties near the top 50 primary schools and their details, is: \"SELECT pb.*, pa.Formated Address, ps.name, sr.Ranking FROM Property_Basic pb JOIN Property_Address pa ON pb.proID = pa.proID JOIN Mapping_School ms ON pb.proID = ms.pro_ID JOIN School ps ON ms.sec1= ps.school_ID JOIN School_Ranking sr ON ps.school_ID = sr.school_ID WHERE sr.Ranking <= 50 and ps.type = \\'Primary\\';\".\\n\\nFor this query, the cardinality on the original data was 2466, while on the generated data, it was 3129. The reason for this discrepancy is the noise perturbation on the Ranking and type attributes, with the error further amplified through the join of five tables, leading to the final cardinality difference. The execution time error for this query was 0.89 seconds, indicating that the query logic on the generated data did not undergo significant changes. The main cost still lies in the join of the five tables, and the changes in the two attributes did not significantly impact the table scan.', metadata={'source': '/content/document4.pdf'}), -0.058974550720476415), (Document(page_content='for PrivBench.\\n\\nsatisfies table-level 0-DP; otherwise, it satisfies table-level ¯𝜖-DP.\\n\\n5.1.4 Analysis of PrivSPN. Based on Lemmas 1, 2, and 3, we con- clude the DP gurantee of PrivSPN through sequential composition.\\n\\nCorollary 1 (of Theorem 5). PrivBench satisfies ((cid:205)𝑖 ∈ [𝑛] 𝜏𝑖 · 𝑖 )-DP at the database level.\\n\\nLemma 4. For any 𝜖 ≥ 0, PrivSPN(𝑇 , 𝜖) satisfies table-level 𝜖-DP.\\n\\n𝜏𝑖 · 𝜖 𝑓\\n\\n𝜖𝑠 𝑖 + (cid:205)𝑇𝑖 refers to 𝑇𝑗\\n\\nIn addition, since PrivSPN allocates the privacy budget 𝜖 from the root to the leaf nodes in a top-down manner, it may result in insufficient budgets allocated to the leaf nodes. Consequently, an interesting question arises: If we allocate a privacy budget 𝜖leaf to all leaf nodes, how large the total privacy budget 𝜖 should be? Theorem 4 answers the question and instructs the design of the scale metirc. That is, if we guarantee a privacy budget 𝜖leaf for each leaf node, then constructing the left and right subtrees should satisfy 𝜎 (𝑇 [(cid:101)𝑆𝐿])-DP and 𝜎 (𝑇 [(cid:101)𝑆𝑅])-DP, respectively. Therefore, the privacy budgets 𝜖𝑙 , 𝜖𝑟 should be proportional to the corresponding scales (see Line 17 of Algorithm 2). In our experiments, we set the scale metric as defined in Theorem 4 for PrivSPN.\\n\\n6 DISCUSSION In this section, we discuss several interesting functionalities of PrivBench.\\n\\nSupported data types. While most works on privacy-preserving database synthesis only support categorical data, PrivBench ad- ditionally supports numeric data. This is because the leaf nodes of the SPN can classify numeric data by range and generate his- tograms for these ranges. Synthetic numeric data is then sampled from these ranges using the histograms. Theoretically, this data synthesis method could also support more complex data types, such\\n\\n8\\n\\nTable 2: Summary of datasets, workloads, and baselines.\\n\\n#Tables\\n\\nDataset\\n\\n#Attributes Workload\\n\\n#Rows\\n\\nBaselines SAM (non-DP), VGAN (non-DP), AIM, DPSynthesizer, CGAN, PrivSyn, DataSyn, PreFair, MST, PrivMRF\\n\\n48842\\n\\nAdult\\n\\nSAM-1000\\n\\n13\\n\\n1\\n\\n×', metadata={'source': '/content/document4.pdf'}), -0.0597358070982803), (Document(page_content='𝑄 − 𝑒𝑟𝑟𝑜𝑟 (𝑞, 𝐷, (cid:98)𝐷) = max\\n\\n(cid:32)𝐶𝑎𝑟𝑑 (𝑞, 𝐷) 𝐶𝑎𝑟𝑑 (𝑞, (cid:98)𝐷)\\n\\n,\\n\\n𝐶𝑎𝑟𝑑 (𝑞, (cid:98)𝐷) 𝐶𝑎𝑟𝑑 (𝑞, 𝐷)\\n\\n(cid:33)\\n\\nwhere 𝑞 denotes a query in the workload, and 𝐶𝑎𝑟𝑑 (𝑞, 𝐷) and 𝐶𝑎𝑟𝑑 (𝑞, (cid:98)𝐷) denote the cardinalities of executing 𝑞 on 𝐷 and (cid:98)𝐷, respectively. Then, we compute the mean Q-error of the queries in the workload.\\n\\n4 PRIVBENCH: DATABASE SYNTHESIS WITH\\n\\nPRIVATE SPNS\\n\\nIn this section, we propose PrivBench, an SPN-based differentially private database synthesis method.\\n\\n4.1 Overview of PrivBench PrivBench utilizes SPNs to address the two concerns outlined in Section 3. SPNs, due to their excellent performance in representing complex dependencies and data distributions at multiple granulari- ties, have been employed in cardinality estimation [13], a prevalent approach intrinsically connected to estimating the runtime cost of queries. Drawing inspiration from this, we adapt the output in [13] to produce a synthesized database, designed to closely resemble the input database in terms of data distribution and cardinality upon query execution. As shown in Algorithm 1, PrivBench involves a three-phase process for database synthesis.\\n\\nPrivate SPN Construction (Lines 1–2). Firstly, we construct an SPN 𝑡𝑖 for each table 𝑇𝑖 in the input database 𝐷. Each SPN 𝑡𝑖 is created by a differentially private algorithm PrivSPN, where each leaf node contains a histogram of a column of a subset of rows in 𝑇𝑖 . • Private Fanout Construction (Lines 3–4). Secondly, we complement each SPN 𝑡𝑖 with some leaf nodes that model primary-foreign key references to obtain a modified SPN 𝑡 ′ 𝑖 . We use a differentially private algorithm PrivFanout to calculate fanout frequencies for each foreign key and store them in the complemented leaf nodes. • SPN-Based Database Synthesis (Lines 5–6) Thirdly, we sample a 𝑖 to synthesize a database (cid:98)𝐷. table (cid:98)𝑇𝑖 from each modified SPN 𝑡 ′', metadata={'source': '/content/document4.pdf'}), -0.06342871396480487)]\n",
            "  warnings.warn(\n",
            "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'query': 'What is Planning in case of Private SPN Construction?',\n",
              " 'result': \"\\n<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nUse the following context to answer the question at the end. Do not use any other information. If you can't find the relevant information in the context, just say you don't have enough information to answer the question. Don't try to make up an answer.\\n\\nAlgorithm 2: Private SPN Construction PrivSPN(𝑇 , 𝜖) : table 𝑇 , total privacy budget 𝜖. Input Output : A tree of SPN 𝑡 = (parent, left, right)\\n\\n1 op, 𝜖op, ¯𝜖 ← Planning(𝑇 , 𝜖); 2 parent, ( (cid:101)S𝐿, (cid:101)S𝑅) ← ParentGen(𝑇 , op, 𝜖op); 3 left, right ← ChildrenGen(𝑇 , op, (cid:101)S𝐿, (cid:101)S𝑅, ¯𝜖); 4 return (parent, left, right) 5 procedure ParentGen(𝑇 , op, 𝜖op) if op = OP.LEAF then 6 Δ(hist) (cid:101)𝐻 ← hist(𝑇 ) + Lap( 𝜖op else if op = OP.SUM then\\n\\n); parent ← LeafNode( (cid:101)𝐻 );\\n\\n7\\n\\n8\\n\\n9\\n\\n10\\n\\n11\\n\\n(cid:101)S𝐿, (cid:101)S𝑅 ← RowSplit(𝑇 , 𝜖op ); parent ← SumNode( (cid:101)S𝐿, (cid:101)S𝑅 ); else if op = OP.PRODUCT then (cid:101)S𝐿, (cid:101)S𝑅 ← ColSplit(𝑇 , 𝜖op ); parent ← ProdNode( (cid:101)S𝐿, (cid:101)S𝑅 );\\n\\n12\\n\\nreturn parent, ( (cid:101)S𝐿, (cid:101)S𝑅)\\n\\n13 procedure ChildrenGen(𝑇 , op, (cid:101)S𝐿, (cid:101)S𝑅, ¯𝜖)\\n\\n14\\n\\n15\\n\\nif op = OP.SUM then 𝜖𝑙 ← ¯𝜖, 𝜖𝑟 ← ¯𝜖\\n\\n16\\n\\nelse if op = OP.PRODUCT then\\n\\n17\\n\\n𝜖𝑙 ← ¯𝜖 ·\\n\\n𝜎 (𝑇 [ (cid:101)S𝐿 ] ) 𝜎 (𝑇 [ (cid:101)S𝐿 ] )+𝜎 (𝑇 [ (cid:101)S𝑅 ] )\\n\\n, 𝜖𝑟 ← ¯𝜖 − 𝜖𝑙\\n\\n18\\n\\nif op = OP.LEAF then\\n\\n19\\n\\nleft ← null; right ← null;\\n\\n20\\n\\nelse\\n\\n21\\n\\nleft ← PrivSPN(𝑇 [ (cid:101)S𝐿 ], 𝜖𝑙 ); right ← PrivSPN(𝑇 [ (cid:101)S𝑅 ], 𝜖𝑟 );\\n\\n22\\n\\nreturn left, right\\n\\n4.2 Private SPN Construction 4.2.1 Overview of PrivSPN. Algorithm 2 outlines our process for constructing an SPN on a single table. Following DeepDB [13], we employ a tree-structured SPN to model a given table 𝑇 . The PrivSPN procedure recursively generates a binary tree 𝑡 = (parent, left, right), where parent should be a parent node, and left, right should be two subtrees. We divide PrivSPN into the following three procedures. • Planning (Operation Planning): We decide an operation op for the parent node and the privacy budget 𝜖op allocated to op. • ParentGen (Parent Generation): According to the operation op\\n\\nand the privacy budget 𝜖op, we generate the parent node.\\n\\nChildrenGen (Children generation): We further generate the chil- dren left, right through recursive calls to PrivSPN.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nWhat is Planning in case of Private SPN Construction?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n4.2.2 Operation Planning. In this section, we describe how to plan operations and allocate privacy budgets for each operation. For simplicity, we assume that the input table 𝑇 has no duplicate rows. We first define the notion of sensitivity of an operation. Definition 4.1 (Sensitivity). Let 𝑆 be a set of tables with the same schema as 𝑇. The sensitivity of an operation op ∈ {OP.SUM, OP.PROD} over 𝑆 is defined as follows:\\n\\n• For OP.SUM, 𝛾(op, 𝑆) = max{𝜎 (𝑇 ) | 𝑇 ∈ 𝑆}. • For OP.PROD, 𝛾(op, 𝑆) = max{𝜎 (𝑇 ) | 𝑇 ∈ 𝑆}, where 𝜎 (𝑇 ) =\\n\\n(cid:102)𝑥∈𝑇 𝑥(cid:103).\\n\\nWe also define the sensitivity of OP.LEAF as 𝛾(OP.LEAF, 𝑆) = 0. Note that the sensitivity of an operation depends on the set of tables it operates on. To simplify the discussion, we will omit the dependency on 𝑆 when there is no ambiguity. Given a table 𝑇, we want to construct an SPN that minimizes the overall error while satisfying the privacy constraint. However, since the exact error bound of an SPN is hard to compute, we instead aim to minimize the upper bound of the error. Specifically, we consider the following objective function:\\n\\n(cid:104)𝑂𝑝𝑐𝑜𝑠𝑡(𝑇 ) = 𝜖op + 𝛾(op, 𝑆)𝜀𝑚𝑎𝑥(𝑇 ), (4.1)\\n\\nwhere 𝜖op is the privacy budget allocated to op, and 𝜀𝑚𝑎𝑥(𝑇 ) is the maximum error of the SPN constructed from 𝑇. Intuitively, 𝜖op is the amount of noise added by op, and 𝛾(op, 𝑆)𝜀𝑚𝑎𝑥(𝑇 ) is the maximum error introduced by all children nodes under op. Therefore, 𝑂𝑝𝑐𝑜𝑠𝑡(\"}"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What are the risk factors of LG company?\"\n",
        "result = Chain_web.invoke(query)\n",
        "result"
      ],
      "metadata": {
        "id": "3wsGS0B4zfIS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fde09220-e45e-4301-93eb-abec07d05007"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/vectorstores.py:330: UserWarning: Relevance scores must be between 0 and 1, got [(Document(page_content='[7] Donald B. Rubin. “INFERENCE AND MISSING DATA”. In: ETS Research Bulletin Series 1975 (1 June 1975), pp. i–19. ISSN: 2333-8504. DOI: 10.1002/J.2333- 8504.1975.TB01053.X. URL: https://onlinelibrary.wiley. com/doi/full/10.1002/j.2333-8504.1975.tb01053.x. [8] Andrew Briggs et al. “Missing.... presumed at random: cost-analysis of incomplete data”. In: Health Econ- comics 12 (5 2003), pp. 377–392. DOI: 10.1002/hec. 766. URL: https://onlinelibrary.wiley.com/doi/10.1002/ hec.766.\\n\\n[9] Daniel A. Newman. “Missing Data: Five Practical Guidelines”. In: Organizational Research Methods 17 (4 Oct. 2014), pp. 372–411. ISSN: 15527425. DOI: 10. 1177/1094428114548590. Joost R. van Ginkel et al. “Rebutting Existing Mis- conceptions About Multiple Imputation as a Method for Handling Missing Data”. In: Journal of Personality Assessment 102 (3 May 2020), pp. 297–308. ISSN: 00223891. DOI: 10.1080/00223891.2018.1530680. [11] Sharlee Climer, Alan R. Templeton, and Weixiong Zhang. “Allele-Specific Network Reveals Combinato- rial Interaction That Transcends Small Effects in Pso- riasis GWAS”. In: PLoS Computational Biology 10 (9 2014). ISSN: 15537358. DOI: 10 . 1371 / journal . pcbi . 1003766.\\n\\n[10]\\n\\n[12] Sharlee Climer. “Connecting the dots: The boons and banes of network modeling”. In: Patterns 2 (12 Dec. 2021). ISSN: 26663899. DOI: 10 . 1016 / j . patter. 2021 . 100374.\\n\\n[13] Michael Chan. auto-miss. 2019. URL: https : / / github.\\n\\ncom/ClimerLab/auto-miss..\\n\\n[14] Kenneth Smith and Sharlee Climer. “Mr. Clean: An Ensemble of Data Cleaning Algorithms for Increased Data Retention”. In: ().\\n\\n[15] Hyunsoo Kim, Gene H. Golub, and Haesun Park. “Missing value estimation for DNA microarray gene expression data: least squares imputation”. In: Bioinformatics 21 (2 Jan. 2005), pp. 187–198. ISSN: 1367-4811. DOI: 10.1093/bioinformatics/bth499. [16] Olga Troyanskaya et al. “Missing value estimation methods for DNA microarrays”. In: Bioinformatics 17 (6 June 2001), pp. 520–525. ISSN: 1367-4811. DOI: 10. 1093/bioinformatics/17.6.520.\\n\\nlocal', metadata={'source': '/content/document5.pdf'}), -0.010105617046882953), (Document(page_content='Model\\n\\nSAM DPSynthesizer PrivBench (sf=1) PrivBench (sf=5) PrivBench (sf=10)\\n\\nKL Div.\\n\\n18.71 16.30 10.92 16.35 25.81\\n\\nQ-error\\n\\nExec. time error\\n\\nMean Median 75th 90th MAX Mean Median 75th 90th MAX 17.97 2355 4.50 4.61 4.65\\n\\n1.77 1.02 1.03 1.03 1.03\\n\\n3.58 1.598 1.23 1.24 1.24\\n\\n8.60 14.39 1.99 2.01 2.03\\n\\n5040 2 × 105 380 393 407\\n\\n1.19 1.11 1.02 1.69 1.85\\n\\n0.75 0.66 0.45 1.27 1.52\\n\\n1.07 1.01 0.96 1.82 2.08\\n\\n1.48 1.43 1.36 2.94 3.34\\n\\n2.16 2.21 2.13 3.75 3.89\\n\\nTable 5: KL Divergence, Q-error and execution time error of test queries (JOB-70) on JOB-light. The privacy budget is set to 3.2. The sf parameter indicates the scale factor.\\n\\nModel\\n\\nSAM DPSynthesizer PrivBench (sf=1) PrivBench (sf=5) PrivBench (sf=10)\\n\\nKL Div.\\n\\n18.71 16.30 10.92 16.35 25.81\\n\\nQ-error\\n\\nExec. time error\\n\\nMean Median 75th 90th MAX Mean Median 75th 90th MAX 6280 2776 5368 5963 636 15.85 1261 16.68 2658 16.94\\n\\n2 × 105 8 × 105 923 1103 1169\\n\\n5.39 46.91 2.61 2.78 2.82\\n\\n2.29 3.35 1.30 1.32 1.32\\n\\n27.78 478.7 5.77 5.92 6.11\\n\\n110.7 106.2 13.17 21.70 28.54\\n\\n1.03 0.83 0.52 1.26 1.48\\n\\n5.81 4.57 2.85 6.83 8.79\\n\\n86.47 59.92 5.49 13.74 20.32\\n\\nthe performance of PrivBench is still significantly better. In ad- dition, we observe that PrivBench is sensitive to the scale factor sf, but even in the case of sf = 10, its KL divergence is still much lower than those of DP-based baselines. Therefore, we conclude that PrivBench synthesizes statistically accurate databases in single- relation database synthesis. It is observed that there is no significant difference in this metric between GAN-based and Marginal-based approaches, highlighting the superiority of the intermediate struc- ture, SPNs utilized in our work.', metadata={'source': '/content/document4.pdf'}), -0.035679734667414253), (Document(page_content='[22] Adrienne D Woods et al. “Missing Data and Multiple Imputation Decision Tree”. In: PsyArXiv (2021). DOI: 10.31234/osf.io/mdw5r. URL: https://doi.org/10.31234/ osf.io/mdw5r.\\n\\n10\\n\\n[23]\\n\\nJungyeon Choi, Olaf M. Dekkers, and Saskia le Cessie. “A comparison of different methods to handle missing data in the context of propensity score analysis”. In: European Journal of Epidemiology 34 (1 Jan. 2019), pp. 23–36. ISSN: 15737284. DOI: 10.1007/s10654-018- 0447-z.\\n\\n[24] Thomas L. Saaty. Optimization in Integers and Re- lated Extremal Problems. Ed. by Donald K. Prentiss. McGraw-Hill, Inc, 1970, pp. 1–295. DOI: 10 . 2307 / 2316512.\\n\\n[25] Kelly A. Frazer et al. “A second generation human haplotype map of over 3.1 million SNPs”. In: Nature 449 (7164 Oct. 2007), pp. 851–861. ISSN: 00280836. DOI: 10.1038/nature06258.\\n\\n[26] Kevin Dunn. Kamyr digester-OpenMV.net Datasets. Oct. 2011. URL: https : / / openmv . net / info / kamyr - digester.\\n\\n[27] Bhupinder S Dayal et al. “Application of feedforward: least squares regression neural networks and partial for modelling kappa number in a continuous Kamyr digester: how multivariate data analysis might help pulping”. In: 1994.\\n\\n[28] Bingshan Li et al. “Transcriptome analysis of psoria- sis in a large case-control sample: RNA-seq provides insights into disease mechanisms”. In: Journal of In- vestigative Dermatology 134 (7 2014), pp. 1828–1838. ISSN: 15231747. DOI: 10.1038/jid.2014.28. URL: http: //dx.doi.org/10.1038/jid.2014.28.\\n\\n[29] Peter M Szabo et al. “Cancer-associated fibroblasts are the main contributors to epithelial-to-mesenchymal sig- natures in the tumor microenvironment”. In: Scientific Reports 13 (3051 2023). DOI: 10 . 1038 / s41598 - 023 - 28480- 9. URL: https://doi.org/10.1038/s41598- 023- 28480-9.', metadata={'source': '/content/document5.pdf'}), -0.12746832146619758), (Document(page_content='Model\\n\\nSAM DPSynthesizer VGAN CGAN PrivSyn DataSyn PreFair MST PrivMRF AIM PrivBench (sf=1) PrivBench (sf=5) PrivBench (sf=10)\\n\\nKL Div.\\n\\n6.48 41.2 40.6 38.3 40.7 39.5 33.5 35.1 37.6 34.8 4.79 16.72 24.51\\n\\nQ-error\\n\\nExec. time error\\n\\nMean Median 75th 90th MAX Mean Median 75th 90th MAX 1.68 1.97 1.67 41.56 1.65 199.2 1.67 50.03 1.75 1519 1.69 426.9 1.75 381.4 1.74 833.5 1.81 184.2 1.66 766.4 1.64 1.73 2.40 1.75 2.97 1.81\\n\\n1.31 1.45 3.49 8.78 28.39 13.81 13.47 20.22 7.51 16.54 1.12 1.13 1.13\\n\\n1.76 2.61 19.22 22.67 576.3 95.74 103.8 246.3 24.11 161.2 1.45 1.46 1.46\\n\\n2.70 13.42 61.43 60.82 4369 568.0 518.6 1448 61.39 882.1 2.05 2.10 2.14\\n\\n0.62 0.60 0.65 0.63 0.62 0.63 0.67 0.65 0.71 0.68 0.64 0.84 0.95\\n\\n0.81 0.85 0.79 0.81 0.80 0.82 0.86 0.88 0.93 0.82 0.80 1.26 1.44\\n\\n0.52 0.57 0.61 0.56 0.58 0.55 0.63 0.59 0.65 0.59 0.53 0.76 0.83\\n\\n168 3 × 104 1 × 105 2 × 104 3 × 106 1 × 106 8 × 105 2 × 106 8 × 104 4 × 106 241 263 259\\n\\n0.38 0.42 0.44 0.40 0.41 0.44 0.48 0.42 0.53 0.47 0.42 0.59 0.64\\n\\nSAM\\n\\n3.2Privacy Budget\\n\\n0.2\\n\\n0.4\\n\\n0.8\\n\\nPrivBench\\n\\n5\\n\\n1.6\\n\\nPrivLava\\n\\nNaN\\n\\n20\\n\\n0\\n\\n25KL Divergence\\n\\n15\\n\\n10\\n\\n0.1\\n\\n0.8\\n\\n1.75Mean Q-error\\n\\n0.4\\n\\nPrivLava\\n\\n0.25\\n\\n1.50\\n\\n3.2Privacy Budget\\n\\n0.75\\n\\nPrivBench\\n\\n1.00\\n\\nSAM\\n\\n0.2\\n\\n0.50\\n\\n0.1\\n\\n0.00\\n\\nNaN\\n\\n1.6\\n\\n1.25\\n\\n0\\n\\n0.2\\n\\n0.4\\n\\n10Mean Exection Time Error (%)\\n\\n3.2Privacy Budget\\n\\n4\\n\\n1.6\\n\\nSAM\\n\\n0.8\\n\\nNaN\\n\\n0.1\\n\\nPrivLava\\n\\n8\\n\\nPrivBench\\n\\n2\\n\\n6\\n\\n(a) KL divergence\\n\\n(b) Q-error\\n\\n(c) Exec. time error.\\n\\nFigure 5: KL Divergence, Q-error and execution time error of test queries on California. The privacy budget is varied.\\n\\nThe JOB-light dataset [37] includes 6 tables related to movies, which is extracted from the Internet Movie Database (IMDB). In addition to the MSCN-400 workload with up to 2 joins, we also use the JOB-70 workload [37], which focuses on joins of multiple tables and provides 70 queries with up to 5 joins. The baselines includes SAM [37] and DPSynthesizer [18]. Note that we do not test DPSynthesizer on California and PrivLava on JOB-light because their open-source code has not been adapted to these datasets.', metadata={'source': '/content/document4.pdf'}), -0.14650748593141794), (Document(page_content='Missing data is generally handled either by deletion or imputation. Deletion techniques remove samples or features from a data set in order to delete missing elements. The simplest version of deletion, and the default in many software packages at one time [8], is list-wise deletion, where a sample is deleted if it contains any missing data. Testing power is reduced using this technique, as the number of samples is decreased. Additionally, if the data is not MCAR, list-wise deletion results in bias [9]. Similarly, feature-wise deletion removes any feature with missing data. This technique keeps the original testing power of the experiment, but risks deleting strongly predictive features and introducing biases of its own [10]. Another common deletion techniques is called pair- wise deletion, where samples are deleted if they contain any missing data in a subset of the features. Consider the scenario where a regression model is to be created using three features. When cleaning with pair-wise deletion, samples are removed only if missing data occurs in those three features, regardless of missing data in the remaining features. While pair-wise deletion often retains more samples than list-wise, it can result in a covariance matrix that is not positive definite, causing issues for some techniques [9].\\n\\nIn his seminal work, Rubin outlined three mechanisms behind missing data [7]. These mechanisms are differentiated by whether or not the probability of an element being missing is independent, dependent on observed data, or dependent on the missing value itself. If the probability of a given element of data being missing is independent of both the value of the data and the other data elements, then it is considered Missing Completely at Random (MCAR). When the probability of an element being missing is independent of the missing value, but', metadata={'source': '/content/document5.pdf'}), -0.17844181587333408), (Document(page_content='Index Terms—data cleaning, data deletion, mixed integer\\n\\nprogramming, imputation\\n\\nI. INTRODUCTION\\n\\nT HE first step in most data processing pipelines is data\\n\\ncleaning. This preparatory procedure involves removing duplicate data, along with the handling of erroneous and missing data. The widespread occurrence of missing data across different domains (gene expression analysis [1, 2, 3], psychology [4], epidemiology [5], genetics [6]), combined with the inability of many analysis algorithms to function correctly with missing data, makes data cleaning a critical area of research.\\n\\ndependent at least in part on other observed values, it is called Missing at Random (MAR). This is occasionally observed in psychology studies when the probability of an individual answering a question is related to their gender. The third mechanism, called Missing Not at Random (MNAR), occurs when the probability of an element being missing is based on the value of the missing data. Missing data can occur in gene expression studies, when values below the minimum detection threshold are considered missing. In this case, the actual value of the data impacts its probability to be considered missing. Despite work in understanding the mechanisms behind missing data, it is not always possible to determine which mechanism or combination of mechanisms is causing missing data in a given experiment. However, it is important to understand the biases introduced to a data set by various cleaning algorithms, based on the missingness mechanisms.', metadata={'source': '/content/document5.pdf'}), -0.1927263100643435), (Document(page_content='Regression imputation tries to capitalize on any relation- ships between missing values and observed features, by train- ing models to predict missing values using the observed data as predictors. Some error may be introduced if the model does not accurately capture the dependence between features. Additionally, the variance for imputed features is biased down- ward, while the covariance is biased upwards [10]. Random error terms are added to regression imputation to counter act the variance and covariance biases. This advanced regression imputation, called stochastic regression, suffers from a bias in the p-values and confidence intervals [10]. The most robust variant of imputation is Multiple Imputation (MI) [10]. In MI, multiple stochastic regression models, typically 5 - 20 [21], are generated on the target data set. Each of these imputed data sets are independently analyzed and the results are combined by various pooling techniques. When an accurate imputation model is selected, the MI estimates are unbiased for MCAR and MAR data. The major drawback of MI is the increased computational requirements, which may not always outweigh the benefits [22]. When the data is MNAR, MI can still suffer biases, but these can be reduced by using the missing indicator method [23]. Imputation methods, in general, can reinforce patterns in observed data and inflate correlation structures [12]. Thus, despite the wide range of available deletion and imputation techniques, no consensus has been reached on the best way to handle missing data.\\n\\nWe continue this manuscript with an overview of the Mr.', metadata={'source': '/content/document5.pdf'}), -0.20448165616991099), (Document(page_content='report the same runtime performance on (cid:98)𝐷.\\n\\nFor the first concern, one evaluation measure is KL divergence. Given a table 𝑇 ∈ 𝐷 and its counterpart (cid:98)𝑇 ∈ (cid:98)𝐷, the KL divergence of 𝑇 from (cid:98)𝑇 is defined as\\n\\n𝐾𝐿(𝑇 ∥ (cid:98)𝑇 ) = −\\n\\n∑︁\\n\\n𝑥 ∈𝑇\\n\\nPr[𝑥,𝑇 ] log\\n\\n(cid:32)\\n\\nPr[𝑥,𝑇 ] Pr[𝑥, (cid:98)𝑇 ]\\n\\n(cid:33)\\n\\nwhere Pr[𝑥,𝑇 ] denotes the probability of tuple 𝑥 in 𝑇 . Then, the KL divergence of 𝐷 from (cid:98)𝐷 is defined as the average KL divergence over all the tables of 𝐷. For the second concern, given a query workload, we can compare the execution times on 𝐷 and (cid:98)𝐷. Since the execution time may vary across DBMSs, we can also compare the cardinality, i.e., the number of tuples in the query result, which is often used in a query optimizer for estimating query performance [17]. In\\n\\n(1)\\n\\n(2)\\n\\n4\\n\\nAlgorithm 1: PrivBench : Database 𝐷 Input Output : Synthetic database (cid:98)𝐷 Param: Privacy budgets 𝜖𝑠\\n\\n𝑖 (for SPN construction) and 𝜖 𝑓\\n\\n(for fanout construction), ∀𝑖 ∈ [𝑛]\\n\\n𝑖\\n\\n1 for each private table 𝑇𝑖 of 𝐷 do 𝑡𝑖 ← PrivSPN(𝑇𝑖, 𝜖𝑠 𝑖 ); 2 3 for each pair of tables (𝑇𝑖,𝑇𝑗 ) where 𝑇𝑖 refers to 𝑇𝑗 do\\n\\n𝑖 ← PrivFanout(𝑇𝑖, 𝑡𝑖, 𝑃𝐾𝑗 , 𝜖 𝑓 𝑡 ′\\n\\n4 5 for each modified SPN 𝑡 ′\\n\\n𝑖 do\\n\\n𝑖 );\\n\\n6\\n\\n(cid:98)𝑇𝑖 ← SampleDataFromSPN(𝑡 ′\\n\\n𝑖 );\\n\\n7 return (cid:98)𝐷 = { (cid:98)𝑇1, . . . , (cid:98)𝑇𝑛 };\\n\\nparticular, Q-error [26] is a widely used measure for comparing cardinalities:\\n\\n𝑄 − 𝑒𝑟𝑟𝑜𝑟 (𝑞, 𝐷, (cid:98)𝐷) = max\\n\\n(cid:32)𝐶𝑎𝑟𝑑 (𝑞, 𝐷) 𝐶𝑎𝑟𝑑 (𝑞, (cid:98)𝐷)\\n\\n,\\n\\n𝐶𝑎𝑟𝑑 (𝑞, (cid:98)𝐷) 𝐶𝑎𝑟𝑑 (𝑞, 𝐷)\\n\\n(cid:33)\\n\\nwhere 𝑞 denotes a query in the workload, and 𝐶𝑎𝑟𝑑 (𝑞, 𝐷) and 𝐶𝑎𝑟𝑑 (𝑞, (cid:98)𝐷) denote the cardinalities of executing 𝑞 on 𝐷 and (cid:98)𝐷, respectively. Then, we compute the mean Q-error of the queries in the workload.\\n\\n4 PRIVBENCH: DATABASE SYNTHESIS WITH\\n\\nPRIVATE SPNS\\n\\nIn this section, we propose PrivBench, an SPN-based differentially private database synthesis method.', metadata={'source': '/content/document4.pdf'}), -0.20661774274613998), (Document(page_content='Adult. The middle five columns of Table 3 present the results of experiments comparing our approach with other baselines on a single-table dataset in terms of Q-error-related metrics. The results\\n\\n11\\n\\nTable 6: Performance of PrivBench in two case studies.\\n\\nCase study\\n\\nEstate database Business database (sf=1) Business database (sf=100)\\n\\nKL Div. Q-error\\n\\n8.52 16.68 34.90\\n\\n1.1 1.07 1.14\\n\\nExec. time error 1.24 1.78 30.51\\n\\nSimilarly to the Q-error, in terms of execution time error, our work outperforms PrivLava and SAM. Additionally, both PrivLava and PrivBench exhibit a negative correlation between execution time error and privacy budget 𝜖, indicating better runtime perfor- mance under larger privacy budget.\\n\\nJOB-light. Tables 4 and 5 document the performance of baselines and PrivBench in terms of Q-error on the multi-table dataset JOB- light. We can observe that PrivBench consistently outperforms others, including SAM and DPSynthesizer, as evidenced in Tables 4 and 5. A comparison among Tables 3, 4, and 5, which respectively feature workloads with no joins, up to two joins, and up to five joins, illustrates that while the Q-error performance of SAM and DPSynthesizer deteriorates significantly with increasing joins, the performance of PrivBench remains stable. Consequently, it can be concluded that PrivBench not only surpasses DPSynthesizer in terms of Q-error but also demonstrates robust performance against increases in the number of joins.\\n\\nIn the multi-table experiment on execution time error, our ap- proach significantly outperforms both competitors. As previously mentioned, our advantage increases significantly with the increase in dataset size and the number of joins between multiple tables, un- affected by the increase in scale factor. In conclusion, our approach excels over baselines in large multi-relation databases and remains robust against increases in scale factor.', metadata={'source': '/content/document4.pdf'}), -0.23291990502784854), (Document(page_content='The JOB-light dataset [37] includes 6 tables related to movies, which is extracted from the Internet Movie Database (IMDB). In addition to the MSCN-400 workload with up to 2 joins, we also use the JOB-70 workload [37], which focuses on joins of multiple tables and provides 70 queries with up to 5 joins. The baselines includes SAM [37] and DPSynthesizer [18]. Note that we do not test DPSynthesizer on California and PrivLava on JOB-light because their open-source code has not been adapted to these datasets.\\n\\nSettings of parameters. In our experiments, the privacy budget 𝜖 for PrivBench varies from 0.1 to 3.2, which aligns with prior work [5]. The settings of the scale metric 𝜎 and the privacy budgets 𝜖𝑠 𝑖 for SPN construction and 𝜖 𝑓 for fanout construction follow those dis- 𝑖 cussed in Section 5. For all datasets, we set the threshold for column splitting 𝛼 = 0.001, and the budget ratios 𝛾1 = 0.001, 𝛾2 = 0.3 for correlation trial. The minimum table size 𝛽 is set to 800 for Adult and to 1000 for the other datasets considering that the table size of Adult in much smaller than other tables. The budget ratio 𝛾 for fanout construction is set to 0.2 for California and to 0.8 for JOB-light given that JOB-light has more referential pairs of tables to be connected by fanout tables.\\n\\nThese datasets are respectively used for single-relation, two-relation, and multi-relation database synthesis, thereby allowing for a com- prehensive assessment of PrivBench’s performance.', metadata={'source': '/content/document4.pdf'}), -0.23470769004798364)]\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/langchain_core/vectorstores.py:342: UserWarning: No relevant docs were retrieved using the relevance score threshold 0.1\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'query': 'What are the risk factors of LG company?',\n",
              " 'result': \"\\n<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nUse the following context to answer the question at the end. Do not use any other information. If you can't find the relevant information in the context, just say you don't have enough information to answer the question. Don't try to make up an answer.\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nWhat are the risk factors of LG company?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\"}"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    }
  ]
}